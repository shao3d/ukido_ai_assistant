# app.py
"""
‚úÖ –§–ò–ù–ê–õ–¨–ù–ê–Ø –í–ï–†–°–ò–Ø: –ß–∏—Å—Ç—ã–π RAG —Ç–µ—Å—Ç —Å LlamaIndex
- 2 –≤—ã–∑–æ–≤–∞ GPT-4o mini: –æ–±–æ–≥–∞—â–µ–Ω–∏–µ + —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç
- –£–º–Ω–∞—è –∏—Å—Ç–æ—Ä–∏—è: –º–∞–∫—Å–∏–º—É–º 4 —Å–æ–æ–±—â–µ–Ω–∏—è (2 –ø–∞—Ä—ã)
- –û—Ç–∫–ª—é—á–µ–Ω—ã: –º–∞—à–∏–Ω–∞ —Å–æ—Å—Ç–æ—è–Ω–∏–π, —é–º–æ—Ä –ñ–≤–∞–Ω–µ—Ü–∫–æ–≥–æ, —Å—Ç–∞—Ä—ã–π –æ–±–æ–≥–∞—Ç–∏—Ç–µ–ª—å
- –ü—Ä–æ—Å—Ç–∞—è, –Ω–∞–¥–µ–∂–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è
"""

import logging
import time
import threading
import atexit
from concurrent.futures import ThreadPoolExecutor
from flask import Flask, request
from typing import Dict, Any, Tuple, Optional, List
import requests

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –Ω–∞—à–∏ –º–æ–¥—É–ª–∏
from config import config
from telegram_bot import telegram_bot
from conversation import conversation_manager
from rag_system import rag_system
from hubspot_client import hubspot_client
from intelligent_analyzer import intelligent_analyzer

# --- LLAMAINDEX INTEGRATION ---
from llamaindex_rag import llama_index_rag
USE_LLAMAINDEX = True  # –ì–ª–∞–≤–Ω—ã–π –ø–µ—Ä–µ–∫–ª—é—á–∞—Ç–µ–ª—å
# --- END LLAMAINDEX ---

# --- RAG DEBUG LOGGER ---
try:
    from rag_debug_logger import rag_debug
    DEBUG_LOGGING_ENABLED = True
except ImportError:
    DEBUG_LOGGING_ENABLED = False
    print("Debug –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–∫–ª—é—á–µ–Ω–æ - rag_debug_logger –Ω–µ –Ω–∞–π–¥–µ–Ω")
# --- END DEBUG ---


class ProductionConnectionPool:
    """Production-ready HTTP Connection pooling"""
    
    def __init__(self):
        self.session = requests.Session()
        self.logger = logging.getLogger(f"{__name__}.ConnectionPool")
        
        adapter = requests.adapters.HTTPAdapter(
            pool_connections=10,
            pool_maxsize=20,
            max_retries=3
        )
        self.session.mount('http://', adapter)
        self.session.mount('https://', adapter)
        self.session.timeout = (5, 15)
        
        atexit.register(self.cleanup)
        self.logger.info("üîó Production connection pool –≥–æ—Ç–æ–≤")
    
    def post(self, *args, **kwargs):
        return self.session.post(*args, **kwargs)
    
    def get(self, *args, **kwargs):
        return self.session.get(*args, **kwargs)
    
    def cleanup(self):
        try:
            self.session.close()
            self.logger.info("üîó Connection pool –∑–∞–∫—Ä—ã—Ç")
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–∫—Ä—ã—Ç–∏—è connection pool: {e}")


class ProductionFastResponseCache:
    """Fast response cache –¥–ª—è –ø—Ä–æ—Å—Ç—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤"""
    
    def __init__(self):
        self.fast_responses = {
            '—Ü–µ–Ω–∞': "–°—Ç–æ–∏–º–æ—Å—Ç—å –∫—É—Ä—Å–æ–≤ –æ—Ç 6000 –¥–æ 8000 –≥—Ä–Ω –≤ –º–µ—Å—è—Ü. –ü–µ—Ä–≤—ã–π —É—Ä–æ–∫ –±–µ—Å–ø–ª–∞—Ç–Ω—ã–π!",
            '—Å—Ç–æ–∏–º–æ—Å—Ç—å': "–°—Ç–æ–∏–º–æ—Å—Ç—å –∫—É—Ä—Å–æ–≤ –æ—Ç 6000 –¥–æ 8000 –≥—Ä–Ω –≤ –º–µ—Å—è—Ü. –ü–µ—Ä–≤—ã–π —É—Ä–æ–∫ –±–µ—Å–ø–ª–∞—Ç–Ω—ã–π!",
            '—Å–∫–æ–ª—å–∫–æ —Å—Ç–æ–∏—Ç': "–°—Ç–æ–∏–º–æ—Å—Ç—å –∫—É—Ä—Å–æ–≤ –æ—Ç 6000 –¥–æ 8000 –≥—Ä–Ω –≤ –º–µ—Å—è—Ü. –ü–µ—Ä–≤—ã–π —É—Ä–æ–∫ –±–µ—Å–ø–ª–∞—Ç–Ω—ã–π!",
            '–ø—Ä–æ–±–Ω—ã–π': "–û—Ç–ª–∏—á–Ω–æ! –ü–µ—Ä–≤—ã–π —É—Ä–æ–∫ —É –Ω–∞—Å –±–µ—Å–ø–ª–∞—Ç–Ω—ã–π.",
            '—É—Ä–æ–∫': "–£ –Ω–∞—Å –µ—Å—Ç—å –∫—É—Ä—Å—ã soft-skills –¥–ª—è –¥–µ—Ç–µ–π 7-17 –ª–µ—Ç. –ü–µ—Ä–≤—ã–π —É—Ä–æ–∫ –±–µ—Å–ø–ª–∞—Ç–Ω—ã–π!",
            '–≤–æ–∑—Ä–∞—Å—Ç': "–ö—É—Ä—Å—ã –¥–ª—è –¥–µ—Ç–µ–π 7-17 –ª–µ—Ç, –≥—Ä—É–ø–ø—ã: 7-9, 10-12, 13-17 –ª–µ—Ç.",
            '–≤—Ä–µ–º—è': "–†–∞—Å–ø–∏—Å–∞–Ω–∏–µ –≥–∏–±–∫–æ–µ, –ø–æ–¥—Å—Ç—Ä–∞–∏–≤–∞–µ–º—Å—è –ø–æ–¥ —É–¥–æ–±–Ω–æ–µ –≤—Ä–µ–º—è.",
            '–∑–∞–ø–∏—Å–∞—Ç—å—Å—è': "–ó–∞–º–µ—á–∞—Ç–µ–ª—å–Ω–æ! –î–∞–≤–∞–π—Ç–µ –∑–∞–ø–∏—à–µ–º –Ω–∞ –±–µ—Å–ø–ª–∞—Ç–Ω—ã–π –ø—Ä–æ–±–Ω—ã–π —É—Ä–æ–∫.",
        }
        
        self.logger = logging.getLogger(f"{__name__}.FastCache")
        self.logger.info("üí® Fast response cache –≥–æ—Ç–æ–≤")

    def get_fast_response(self, message: str, chat_id: str) -> Optional[str]:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –º–æ–∂–µ–º –ª–∏ –¥–∞—Ç—å –±—ã—Å—Ç—Ä—ã–π –æ—Ç–≤–µ—Ç"""
        message_lower = message.lower().strip()
        
        for keyword, response in self.fast_responses.items():
            if keyword in message_lower:
                # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è –ø—Ä–æ–±–Ω–æ–≥–æ —É—Ä–æ–∫–∞
                if keyword in ['–ø—Ä–æ–±–Ω—ã–π', '–∑–∞–ø–∏—Å–∞—Ç—å—Å—è']:
                    lesson_link = f"https://ukidoaiassistant-production.up.railway.app/lesson?user_id={chat_id}"
                    return f"{response}\n\nüîó –ó–∞–ø–∏—Å–∞—Ç—å—Å—è: {lesson_link}"
                
                return response
        
        return None


class ProductionAIService:
    """Production-ready AI —Å–µ—Ä–≤–∏—Å –¥–ª—è —á–∏—Å—Ç–æ–≥–æ RAG —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        self.connection_pool = ProductionConnectionPool()
        self.fast_response_cache = ProductionFastResponseCache()
        
        self.executor = ThreadPoolExecutor(max_workers=4, thread_name_prefix="UkidoAI")
        
        # Thread-safe —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.performance_stats = {
            'total_requests': 0,
            'fast_responses': 0,
            'avg_response_time': 0,
        }
        self.stats_lock = threading.Lock()
        
        # AI –º–æ–¥–µ–ª—å
        self.ai_model_available = self._initialize_ai_model()
        
        self.logger.info("üöÄ ProductionAIService –≥–æ—Ç–æ–≤ –∫ RAG —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—é")
    
    def _initialize_ai_model(self) -> bool:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Gemini –º–æ–¥–µ–ª–∏"""
        try:
            import google.generativeai as genai
            genai.configure(api_key=config.GEMINI_API_KEY)
            
            generation_config = {
                "temperature": 0.7,
                "top_p": 0.95,
                "top_k": 40,
                "max_output_tokens": 1024,
            }
            
            safety_settings = [
                {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
            ]
            
            self.model = genai.GenerativeModel(
                model_name="gemini-1.5-pro",
                generation_config=generation_config,
                safety_settings=safety_settings
            )
            
            self.logger.info("‚úÖ Gemini 1.5 Pro –≥–æ—Ç–æ–≤")
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ AI: {e}")
            return False

    def process_user_message(self, user_message: str, chat_id: str) -> str:
        """
        ‚úÖ –ì–õ–ê–í–ù–´–ô –ú–ï–¢–û–î: –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏–π —Å —á–∏—Å—Ç—ã–º RAG —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º
        """
        start_time = time.time()
        
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º fast response
            fast_response = self.fast_response_cache.get_fast_response(user_message, chat_id)
            if fast_response:
                with self.stats_lock:
                    self.performance_stats['fast_responses'] += 1
                return fast_response
            
            # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
            current_state = conversation_manager.get_dialogue_state(chat_id)
            
            # ‚úÖ –£–ú–ù–ê–Ø –ò–°–¢–û–†–ò–Ø - –ª–æ–∫–∞–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã
            def get_smart_conversation_history():
                """–ü–æ–ª—É—á–∞–µ–º —É–º–Ω—É—é –∏—Å—Ç–æ—Ä–∏—é: –º–∞–∫—Å–∏–º—É–º 4 —Å–æ–æ–±—â–µ–Ω–∏—è (2 –ø–∞—Ä—ã)"""
                full_history = conversation_manager.get_conversation_history(chat_id)
                
                if not full_history or len(full_history) == 0:
                    return []  # –ü–µ—Ä–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
                
                # –ú–∞–∫—Å–∏–º—É–º 4 —Å–æ–æ–±—â–µ–Ω–∏—è (2 –ø–∞—Ä—ã –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç)
                return full_history[-4:]
            
            def get_rag_context():
                """‚úÖ –ß–ò–°–¢–´–ô RAG –ø–æ–∏—Å–∫ –±–µ–∑ —Å—Ç–∞—Ä–æ–≥–æ –æ–±–æ–≥–∞—â–µ–Ω–∏—è"""
                # –ü–æ–ª—É—á–∞–µ–º —É–º–Ω—É—é –∏—Å—Ç–æ—Ä–∏—é
                smart_history = get_smart_conversation_history()
                
                # ‚úÖ –ü–ï–†–ï–î–ê–ï–ú –û–†–ò–ì–ò–ù–ê–õ–¨–ù–´–ô –∑–∞–ø—Ä–æ—Å (—Å—Ç–∞—Ä—ã–π –æ–±–æ–≥–∞—Ç–∏—Ç–µ–ª—å –æ—Ç–∫–ª—é—á–µ–Ω)
                
                if USE_LLAMAINDEX and llama_index_rag:
                    self.logger.info("üöÄ LlamaIndex RAG (—á–∏—Å—Ç—ã–π —Ç–µ—Å—Ç)")
                    
                    # Debug –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
                    if DEBUG_LOGGING_ENABLED:
                        rag_debug.log_enricher_input(user_message, smart_history)
                    
                    # ‚úÖ –û–†–ò–ì–ò–ù–ê–õ–¨–ù–´–ô –∑–∞–ø—Ä–æ—Å + —É–º–Ω–∞—è –∏—Å—Ç–æ—Ä–∏—è
                    context, metrics = llama_index_rag.search_knowledge_base(
                        query=user_message,  # –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –∑–∞–ø—Ä–æ—Å!
                        conversation_history=smart_history
                    )
                else:
                    self.logger.info("Legacy RAG fallback")
                    context, metrics = rag_system.search_knowledge_base(user_message, smart_history)

                return context, metrics

            # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ
            try:
                smart_history_future = self.executor.submit(get_smart_conversation_history)
                conversation_history = smart_history_future.result(timeout=3)
                
                rag_future = self.executor.submit(get_rag_context)
                facts_context, rag_metrics = rag_future.result(timeout=5)
            except Exception as e:
                self.logger.error(f"Parallel processing error: {e}")
                facts_context = "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞."
                rag_metrics = {'best_score': 0.0, 'chunks_found': 0}
                conversation_history = []
            
            # ‚úÖ –ß–ò–°–¢–´–ô RAG –¢–ï–°–¢: –±–µ–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏–∏ –∏ –º–∞—à–∏–Ω—ã —Å–æ—Å—Ç–æ—è–Ω–∏–π
            rag_score = rag_metrics.get('max_score', 0.0)

            # ‚úÖ –ü–†–û–°–¢–û–ô RAG –ü–†–û–ú–ü–¢ (–±–µ–∑ —é–º–æ—Ä–∞ –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏–∏)
            llm_start = time.time()
            combined_prompt = self._build_simple_rag_prompt(
                user_message=user_message,
                facts_context=facts_context,
                rag_score=rag_metrics.get('average_score', 0.5),
                conversation_history=conversation_history
            )

            # Debug –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
            if DEBUG_LOGGING_ENABLED:
                rag_debug.log_final_prompt(combined_prompt, len(conversation_history))

            # –õ–æ–≥–∏—Ä—É–µ–º —Ä–µ—à–µ–Ω–∏–µ
            self.logger.info(f"üìä [–ß–ò–°–¢–´–ô RAG] Score: {rag_score:.2f}, –ò—Å—Ç–æ—Ä–∏—è: {len(conversation_history)} msgs")

            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            combined_response = self._call_ai_model(combined_prompt)
            llm_time = time.time() - llm_start

            # Debug —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
            if DEBUG_LOGGING_ENABLED:
                rag_debug.log_final_response(combined_response, llm_time)

            # –ü–∞—Ä—Å–∏–º –æ—Ç–≤–µ—Ç
            main_response, analysis_data = self._parse_combined_response(combined_response)

            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–æ–∫–µ–Ω—ã –¥–µ–π—Å—Ç–≤–∏–π
            final_response = self._process_action_tokens(main_response, chat_id, current_state)

            # –û–±–Ω–æ–≤–ª—è–µ–º –∏—Å—Ç–æ—Ä–∏—é
            conversation_manager.update_conversation_history(chat_id, user_message, final_response)
            conversation_manager.set_dialogue_state(chat_id, analysis_data.get('state', current_state))

            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            total_time = time.time() - start_time
            self._update_performance_stats(total_time, llm_time)

            self.logger.info(f"‚úÖ –û—Ç–≤–µ—Ç –≥–æ—Ç–æ–≤ –¥–ª—è {chat_id} –∑–∞ {total_time:.3f}s")
            return final_response
            
        except Exception as e:
            self.logger.error(f"üí• –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}")
            error_response = "–ò–∑–≤–∏–Ω–∏—Ç–µ, –≤—Ä–µ–º–µ–Ω–Ω–∞—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–±–ª–µ–º–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑."
            
            if DEBUG_LOGGING_ENABLED:
                rag_debug.log_final_response(f"ERROR: {error_response}", 0.0)
            return error_response
    
    def _build_simple_rag_prompt(self, user_message: str, facts_context: str, 
                                rag_score: float, conversation_history: list = None) -> str:
        """
        ‚úÖ –ü–†–û–°–¢–û–ô RAG –ü–†–û–ú–ü–¢ –¥–ª—è —á–∏—Å—Ç–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        """
        # –ò—Å—Ç–æ—Ä–∏—è —É–∂–µ –æ–±—Ä–µ–∑–∞–Ω–∞ –≤ get_smart_conversation_history() –¥–æ 4 —Å–æ–æ–±—â–µ–Ω–∏–π
        if not conversation_history or len(conversation_history) == 0:
            history_text = "–ù–∞—á–∞–ª–æ –¥–∏–∞–ª–æ–≥–∞"
            greeting_instruction = "üìù –ù–ê–ß–ê–õ–û: –ù–∞—á–Ω–∏ —Å –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏—è ('–î–æ–±—Ä—ã–π –¥–µ–Ω—å!')."
        else:
            history_text = "\n".join(conversation_history)
            greeting_instruction = "üìù –ü–†–û–î–û–õ–ñ–ï–ù–ò–ï: –ë–ï–ó –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–π. –£—á–∏—Ç—ã–≤–∞–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –±–µ—Å–µ–¥—ã."
        
        return f"""–¢—ã AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –æ–Ω–ª–∞–π–Ω-—à–∫–æ–ª—ã Ukido –¥–ª—è —Ä–∞–∑–≤–∏—Ç–∏—è soft skills —É –¥–µ—Ç–µ–π.

üìö –ò–ù–§–û–†–ú–ê–¶–ò–Ø –ò–ó –ë–ê–ó–´ –ó–ù–ê–ù–ò–ô (RAG Score: {rag_score:.2f}):
{facts_context}

üí¨ –ò–°–¢–û–†–ò–Ø –î–ò–ê–õ–û–ì–ê ({len(conversation_history) if conversation_history else 0} —Å–æ–æ–±—â–µ–Ω–∏–π, –º–∞–∫—Å. 4):
{history_text}

‚ùì –í–û–ü–†–û–°: {user_message}

üìã –ò–ù–°–¢–†–£–ö–¶–ò–ò:
1. –ï—Å–ª–∏ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π –ï–°–¢–¨ –æ—Ç–≤–µ—Ç - –∏—Å–ø–æ–ª—å–∑—É–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ñ–∞–∫—Ç—ã
2. –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ù–ï–¢ - —Å–∫–∞–∂–∏: "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, –≤ –º–æ–µ–π –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø–æ —ç—Ç–æ–º—É –≤–æ–ø—Ä–æ—Å—É"
3. –ù–ï –≤—ã–¥—É–º—ã–≤–∞–π —Ñ–∞–∫—Ç—ã
4. –£–ß–ò–¢–´–í–ê–ô –∏—Å—Ç–æ—Ä–∏—é –¥–∏–∞–ª–æ–≥–∞ - –Ω–µ –ø–æ–≤—Ç–æ—Ä—è–π—Å—è
5. {greeting_instruction}
6. –û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ —Å—É—â–µ—Å—Ç–≤—É

–û—Ç–≤–µ—Ç:"""

    def _call_ai_model(self, prompt: str) -> str:
        """–í—ã–∑–æ–≤ Gemini –º–æ–¥–µ–ª–∏"""
        try:
            if not self.ai_model_available:
                return "AI –º–æ–¥–µ–ª—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."
            
            response = self.model.generate_content(prompt)
            return response.text if response.text else "–ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç."
            
        except Exception as e:
            self.logger.error(f"AI model error: {e}")
            return "–í—Ä–µ–º–µ–Ω–Ω–∞—è –ø—Ä–æ–±–ª–µ–º–∞ —Å AI. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑."

    def _parse_combined_response(self, response_text: str) -> Tuple[str, Dict[str, Any]]:
        """–ü—Ä–æ—Å—Ç–æ–π –ø–∞—Ä—Å–∏–Ω–≥ –æ—Ç–≤–µ—Ç–∞"""
        try:
            lines = response_text.strip().split('\n')
            main_response = []
            analysis_data = {}
            
            for line in lines:
                if line.startswith('[') and ']' in line:
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º metadata
                    try:
                        key_value = line.strip('[]').split(':')
                        if len(key_value) == 2:
                            key, value = key_value
                            analysis_data[key.strip()] = value.strip()
                    except:
                        main_response.append(line)
                else:
                    main_response.append(line)
            
            return '\n'.join(main_response).strip(), analysis_data
            
        except Exception as e:
            self.logger.error(f"Response parsing error: {e}")
            return response_text, {}

    def _process_action_tokens(self, response: str, chat_id: str, current_state: str) -> str:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤"""
        try:
            # [LESSON_LINK] —Ç–æ–∫–µ–Ω
            if "[LESSON_LINK]" in response:
                lesson_link = f"https://ukidoaiassistant-production.up.railway.app/lesson?user_id={chat_id}"
                response = response.replace("[LESSON_LINK]", lesson_link)
            
            # [CONTACT_MANAGER] —Ç–æ–∫–µ–Ω
            if "[CONTACT_MANAGER]" in response:
                contact_info = "üìû –°–≤—è–∑–∞—Ç—å—Å—è: @ukido_manager –∏–ª–∏ +38 (093) 123-45-67"
                response = response.replace("[CONTACT_MANAGER]", contact_info)
            
            return response
            
        except Exception as e:
            self.logger.error(f"Action token error: {e}")
            return response

    def _update_performance_stats(self, total_time: float, llm_time: float):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        try:
            with self.stats_lock:
                self.performance_stats['total_requests'] += 1
                
                current_avg = self.performance_stats['avg_response_time']
                total_requests = self.performance_stats['total_requests']
                
                new_avg = ((current_avg * (total_requests - 1)) + total_time) / total_requests
                self.performance_stats['avg_response_time'] = new_avg
                
                if total_requests % 10 == 0:
                    self.logger.info(f"üìä Stats: {total_requests} requests, avg: {new_avg:.2f}s")
                    
        except Exception as e:
            self.logger.error(f"Stats error: {e}")


# === –ì–õ–û–ë–ê–õ–¨–ù–ê–Ø –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø ===
production_ai_service = ProductionAIService()

# === FLASK –ü–†–ò–õ–û–ñ–ï–ù–ò–ï ===
app = Flask(__name__)

@app.route('/', methods=['POST', 'GET'])
def handle_telegram_webhook():
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ Telegram webhook"""
    try:
        if request.method == 'GET':
            return "Ukido AI Assistant is running! üöÄ", 200
        
        update = request.get_json()
        if not update:
            return "No data received", 400
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ
        if 'message' in update:
            message = update['message']
            chat_id = str(message['chat']['id'])
            
            if 'text' in message:
                user_message = message['text']
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ
                bot_response = production_ai_service.process_user_message(user_message, chat_id)
                
                # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –æ—Ç–≤–µ—Ç
                telegram_bot.send_message(chat_id, bot_response)
                
        return "OK", 200
        
    except Exception as e:
        logging.error(f"Webhook error: {e}")
        return "Error", 500

@app.route('/test-message', methods=['POST'])
def test_message_endpoint():
    """‚úÖ –¢–µ—Å—Ç–æ–≤—ã–π —ç–Ω–¥–ø–æ–∏–Ω—Ç –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏"""
    try:
        data = request.get_json()
        user_message = data.get('message', '')
        user_id = data.get('user_id', 'test_user')
        
        start_time = time.time()
        bot_response = production_ai_service.process_user_message(user_message, user_id)
        response_time = time.time() - start_time
        
        return {
            'bot_response': bot_response,
            'response_time': response_time,
            'status': 'success'
        }, 200
        
    except Exception as e:
        return {'error': str(e)}, 500

@app.route('/clear-memory', methods=['POST'])
def clear_memory():
    """‚úÖ –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    try:
        conversation_manager.clear_all_conversations()
        return {"status": "success", "message": "Memory cleared"}, 200
    except Exception as e:
        return {"error": str(e)}, 500

if __name__ == '__main__':
    import os
    port = int(os.environ.get('PORT', 5000))
    app.run(host='0.0.0.0', port=port, debug=False)