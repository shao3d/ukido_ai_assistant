"""
DETERMINISTIC BUSINESS-CRITICAL CHUNKER FOR UKIDO RAG SYSTEM
============================================================

–î–µ—Ç–µ—Ä–º–∏–Ω–∏—Å—Ç–∏—á–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞ —á–∞–Ω–∫–æ–≤–∞–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ –≤—Å–µ—Ö 9 –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.
–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å –¥–ª—è business-critical Telegram –±–æ—Ç–∞.
"""

import os
import time
import google.generativeai as genai
from pinecone import Pinecone
from dotenv import load_dotenv
from typing import List, Dict
from datetime import datetime
import re

# --- –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ---
load_dotenv()

GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")
PINECONE_HOST_FACTS = os.getenv("PINECONE_HOST_FACTS")

if not all([GEMINI_API_KEY, PINECONE_API_KEY, PINECONE_HOST_FACTS]):
    raise ValueError("–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è")

genai.configure(api_key=GEMINI_API_KEY)
embedding_model = 'models/text-embedding-004'

class DeterministicBusinessChunker:
    """
    –î–µ—Ç–µ—Ä–º–∏–Ω–∏—Å—Ç–∏—á–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞ —á–∞–Ω–∫–æ–≤–∞–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
    """
    
    def __init__(self):
        print("üéØ DETERMINISTIC BUSINESS-CRITICAL CHUNKER")
        print("üìã –î–µ—Ç–µ—Ä–º–∏–Ω–∏—Å—Ç–∏—á–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞")
        print("=" * 55)

    def create_strategic_overview_chunks(self) -> List[Dict]:
        """
        –°—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–∏–µ —á–∞–Ω–∫–∏ –¥–ª—è –∫–ª—é—á–µ–≤—ã—Ö –±–∏–∑–Ω–µ—Å-–≤–æ–ø—Ä–æ—Å–æ–≤
        """
        chunks = []
        
        # 1. –ì–õ–ê–í–ù–´–ô –ß–ê–ù–ö - –û–±–∑–æ—Ä –≤—Å–µ—Ö –∫—É—Ä—Å–æ–≤ –¥–ª—è "–ö–∞–∫–∏–µ –∫—É—Ä—Å—ã?"
        courses_main = """–û–°–ù–û–í–ù–´–ï –ö–£–†–°–´ –®–ö–û–õ–´ UKIDO

–®–∫–æ–ª–∞ Ukido –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç —Ç—Ä–∏ –æ—Å–Ω–æ–≤–Ω—ã—Ö –∫—É—Ä—Å–∞ –¥–ª—è —Ä–∞–∑–≤–∏—Ç–∏—è soft skills —É –¥–µ—Ç–µ–π —Ä–∞–∑–Ω–æ–≥–æ –≤–æ–∑—Ä–∞—Å—Ç–∞:

1. –ö–£–†–° "–Æ–ù–´–ô –û–†–ê–¢–û–†" (7-10 –ª–µ—Ç)
–¶–µ–ª—å: –£–±—Ä–∞—Ç—å —Å—Ç—Ä–∞—Ö –ø—É–±–ª–∏—á–Ω—ã—Ö –≤—ã—Å—Ç—É–ø–ª–µ–Ω–∏–π, –Ω–∞—É—á–∏—Ç—å —á–µ—Ç–∫–æ –∏–∑–ª–∞–≥–∞—Ç—å –º—ã—Å–ª–∏
–î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: 3 –º–µ—Å—è—Ü–∞ (24 –∑–∞–Ω—è—Ç–∏—è), 2 —Ä–∞–∑–∞ –≤ –Ω–µ–¥–µ–ª—é –ø–æ 90 –º–∏–Ω—É—Ç
–ì—Ä—É–ø–ø—ã: –¥–æ 8 –¥–µ—Ç–µ–π
–°—Ç–æ–∏–º–æ—Å—Ç—å: 6000 –≥—Ä–Ω –≤ –º–µ—Å—è—Ü
–ü—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª—å: –ê–Ω–Ω–∞ –ö–æ–≤–∞–ª–µ–Ω–∫–æ (8 –ª–µ—Ç –æ–ø—ã—Ç–∞, –∞–≤—Ç–æ—Ä –º–µ—Ç–æ–¥–∏–∫–∏ "–ë–µ—Å—Å—Ç—Ä–∞—à–Ω—ã–π –æ—Ä–∞—Ç–æ—Ä")
–†–µ–∑—É–ª—å—Ç–∞—Ç: 94% –¥–µ—Ç–µ–π –∏–∑–±–∞–≤–ª—è—é—Ç—Å—è –æ—Ç —Å—Ç—Ä–∞—Ö–∞ –ø—É–±–ª–∏—á–Ω—ã—Ö –≤—ã—Å—Ç—É–ø–ª–µ–Ω–∏–π

2. –ö–£–†–° "–≠–ú–û–¶–ò–û–ù–ê–õ–¨–ù–´–ô –ö–û–ú–ü–ê–°" (9-12 –ª–µ—Ç)
–¶–µ–ª—å: –ù–∞—É—á–∏—Ç—å —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç—å, –ø–æ–Ω–∏–º–∞—Ç—å –∏ —É–ø—Ä–∞–≤–ª—è—Ç—å —ç–º–æ—Ü–∏—è–º–∏
–î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: 4 –º–µ—Å—è—Ü–∞ (32 –∑–∞–Ω—è—Ç–∏—è), 2 —Ä–∞–∑–∞ –≤ –Ω–µ–¥–µ–ª—é –ø–æ 90 –º–∏–Ω—É—Ç  
–ì—Ä—É–ø–ø—ã: –¥–æ 6 –¥–µ—Ç–µ–π
–°—Ç–æ–∏–º–æ—Å—Ç—å: 7500 –≥—Ä–Ω –≤ –º–µ—Å—è—Ü
–ü—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª—å: –î–º–∏—Ç—Ä–∏–π –ü–µ—Ç—Ä–æ–≤ (PhD, –∞–≤—Ç–æ—Ä –∫–Ω–∏–≥–∏ "EQ –¥–ª—è –¥–µ—Ç–µ–π")
–†–µ–∑—É–ª—å—Ç–∞—Ç: –°–Ω–∏–∂–µ–Ω–∏–µ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–Ω–æ—Å—Ç–∏ –Ω–∞ 76%, –ø–æ–≤—ã—à–µ–Ω–∏–µ —ç–º–ø–∞—Ç–∏–∏ –Ω–∞ 82%

3. –ö–£–†–° "–ö–ê–ü–ò–¢–ê–ù –ü–†–û–ï–ö–¢–û–í" (11-14 –ª–µ—Ç)
–¶–µ–ª—å: –†–∞–∑–≤–∏—Ç–∏–µ –ª–∏–¥–µ—Ä—Å–∫–∏—Ö –∫–∞—á–µ—Å—Ç–≤ –∏ —É–º–µ–Ω–∏—è —Ä–∞–±–æ—Ç–∞—Ç—å –≤ –∫–æ–º–∞–Ω–¥–µ
–î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: 5 –º–µ—Å—è—Ü–µ–≤ (40 –∑–∞–Ω—è—Ç–∏–π), 2 —Ä–∞–∑–∞ –≤ –Ω–µ–¥–µ–ª—é –ø–æ 90 –º–∏–Ω—É—Ç
–ì—Ä—É–ø–ø—ã: –ø—Ä–æ–µ–∫—Ç–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã 4-5 —á–µ–ª–æ–≤–µ–∫  
–°—Ç–æ–∏–º–æ—Å—Ç—å: 8000 –≥—Ä–Ω –≤ –º–µ—Å—è—Ü
–ü—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª—å: –ï–ª–µ–Ω–∞ –°–∏–¥–æ—Ä–æ–≤–∞ (MBA, —Ç–æ–ø-10 –±–∏–∑–Ω–µ—Å-—Ç—Ä–µ–Ω–µ—Ä–æ–≤ –£–∫—Ä–∞–∏–Ω—ã)
–†–µ–∑—É–ª—å—Ç–∞—Ç: 85% –≤—ã–ø—É—Å–∫–Ω–∏–∫–æ–≤ —Å—Ç–∞–Ω–æ–≤—è—Ç—Å—è –ª–∏–¥–µ—Ä–∞–º–∏ –≤ –∫–ª–∞—Å—Å–∞—Ö

–ü–µ—Ä–≤—ã–π —É—Ä–æ–∫ –ª—é–±–æ–≥–æ –∫—É—Ä—Å–∞ –±–µ—Å–ø–ª–∞—Ç–Ω—ã–π –∏ –ø—Ä–æ–±–Ω—ã–π. –í—Å–µ –∑–∞–Ω—è—Ç–∏—è –ø—Ä–æ—Ö–æ–¥—è—Ç –æ–Ω–ª–∞–π–Ω."""
        
        chunks.append({"text": courses_main, "type": "courses_overview", "priority": "critical"})
        
        # 2. –¶–ï–ù–û–û–ë–†–ê–ó–û–í–ê–ù–ò–ï
        pricing_main = """–°–¢–û–ò–ú–û–°–¢–¨ –ö–£–†–°–û–í –ò –°–ö–ò–î–ö–ò UKIDO

–ë–ê–ó–û–í–´–ï –¶–ï–ù–´ –ü–û –ö–£–†–°–ê–ú:
‚Ä¢ "–Æ–Ω—ã–π –û—Ä–∞—Ç–æ—Ä" (7-10 –ª–µ—Ç): 6000 –≥—Ä–Ω/–º–µ—Å—è—Ü √ó 3 –º–µ—Å—è—Ü–∞ = 18000 –≥—Ä–Ω
‚Ä¢ "–≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –ö–æ–º–ø–∞—Å" (9-12 –ª–µ—Ç): 7500 –≥—Ä–Ω/–º–µ—Å—è—Ü √ó 4 –º–µ—Å—è—Ü–∞ = 30000 –≥—Ä–Ω
‚Ä¢ "–ö–∞–ø–∏—Ç–∞–Ω –ü—Ä–æ–µ–∫—Ç–æ–≤" (11-14 –ª–µ—Ç): 8000 –≥—Ä–Ω/–º–µ—Å—è—Ü √ó 5 –º–µ—Å—è—Ü–µ–≤ = 40000 –≥—Ä–Ω

–í–ê–†–ò–ê–ù–¢–´ –û–ü–õ–ê–¢–´ –ò –°–ö–ò–î–ö–ò:
‚Ä¢ –ü–æ–º–µ—Å—è—á–Ω–∞—è –æ–ø–ª–∞—Ç–∞: —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è —Ü–µ–Ω–∞, –±–µ–∑ –∫–æ–º–∏—Å—Å–∏–π
‚Ä¢ –ü–æ–∫–≤–∞—Ä—Ç–∞–ª—å–Ω–∞—è –æ–ø–ª–∞—Ç–∞: —Å–∫–∏–¥–∫–∞ 5% (–¥–ª—è "–Æ–Ω—ã–π –û—Ä–∞—Ç–æ—Ä" = 17100 –≥—Ä–Ω –≤–º–µ—Å—Ç–æ 18000 –≥—Ä–Ω)
‚Ä¢ –û–ø–ª–∞—Ç–∞ –ø–æ–ª–Ω–æ–≥–æ –∫—É—Ä—Å–∞: —Å–∫–∏–¥–∫–∞ 10% (–¥–ª—è "–Æ–Ω—ã–π –û—Ä–∞—Ç–æ—Ä" = 16200 –≥—Ä–Ω –≤–º–µ—Å—Ç–æ 18000 –≥—Ä–Ω)
‚Ä¢ –°–µ–º–µ–π–Ω–∞—è —Å–∫–∏–¥–∫–∞: 15% –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ 2+ –¥–µ—Ç–µ–π –∏–∑ –æ–¥–Ω–æ–π —Å–µ–º—å–∏
‚Ä¢ –°–∫–∏–¥–∫–∞ –∑–∞ –¥—Ä—É–≥–∞: 1000 –≥—Ä–Ω –ø—Ä–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
‚Ä¢ –†–∞—Å—Å—Ä–æ—á–∫–∞: –Ω–∞ 3 –º–µ—Å—è—Ü–∞ –±–µ–∑ –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤

–°–ï–ó–û–ù–ù–´–ï –ê–ö–¶–ò–ò:
‚Ä¢ –õ–µ—Ç–Ω—è—è —Å–∫–∏–¥–∫–∞: 20% (–∏—é–Ω—å-–∞–≤–≥—É—Å—Ç)
‚Ä¢ –ù–æ–≤–æ–≥–æ–¥–Ω—è—è: 15% (–¥–µ–∫–∞–±—Ä—å-—è–Ω–≤–∞—Ä—å)  
‚Ä¢ –î–µ–Ω—å –∑–Ω–∞–Ω–∏–π: +1 –º–µ—Å—è—Ü –±–µ—Å–ø–ª–∞—Ç–Ω–æ (—Å–µ–Ω—Ç—è–±—Ä—å)

–ì–ê–†–ê–ù–¢–ò–ò: 100% –≤–æ–∑–≤—Ä–∞—Ç –∑–∞ 7 –¥–Ω–µ–π, 50% –∑–∞ –º–µ—Å—è—Ü. –ü—Ä–æ–±–Ω—ã–π —É—Ä–æ–∫ –±–µ—Å–ø–ª–∞—Ç–Ω–æ."""
        
        chunks.append({"text": pricing_main, "type": "pricing_overview", "priority": "critical"})
        
        return chunks

    def chunk_courses_detailed(self, content: str) -> List[Dict]:
        """
        –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞ –¥–ª—è courses_detailed.txt
        """
        chunks = []
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –ø–æ –∫—É—Ä—Å–∞–º
        if "–ö–£–†–° \"–Æ–ù–´–ô –û–†–ê–¢–û–†\"" in content:
            orator_match = re.search(r'–ö–£–†–° "–Æ–ù–´–ô –û–†–ê–¢–û–†".*?(?=–ö–£–†–° "|$)', content, re.DOTALL)
            if orator_match:
                chunks.append({
                    "text": orator_match.group(0).strip(),
                    "type": "course_detail",
                    "course": "young_orator"
                })
        
        if "–ö–£–†–° \"–≠–ú–û–¶–ò–û–ù–ê–õ–¨–ù–´–ô –ö–û–ú–ü–ê–°\"" in content:
            compass_match = re.search(r'–ö–£–†–° "–≠–ú–û–¶–ò–û–ù–ê–õ–¨–ù–´–ô –ö–û–ú–ü–ê–°".*?(?=–ö–£–†–° "|$)', content, re.DOTALL)
            if compass_match:
                chunks.append({
                    "text": compass_match.group(0).strip(), 
                    "type": "course_detail",
                    "course": "emotional_compass"
                })
        
        if "–ö–£–†–° \"–ö–ê–ü–ò–¢–ê–ù –ü–†–û–ï–ö–¢–û–í\"" in content:
            captain_match = re.search(r'–ö–£–†–° "–ö–ê–ü–ò–¢–ê–ù –ü–†–û–ï–ö–¢–û–í".*?$', content, re.DOTALL)
            if captain_match:
                chunks.append({
                    "text": captain_match.group(0).strip(),
                    "type": "course_detail", 
                    "course": "project_captain"
                })
        
        return chunks

    def chunk_teachers_team(self, content: str) -> List[Dict]:
        """
        –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞ –¥–ª—è teachers_team.txt - —Å–≤—è–∑—å –ø—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª—å+–∫—É—Ä—Å
        """
        chunks = []
        
        # –ê–Ω–Ω–∞ –ö–æ–≤–∞–ª–µ–Ω–∫–æ + –Æ–Ω—ã–π –û—Ä–∞—Ç–æ—Ä
        anna_match = re.search(r'–ê–ù–ù–ê –ö–û–í–ê–õ–ï–ù–ö–û.*?(?=–î–ú–ò–¢–†–ò–ô –ü–ï–¢–†–û–í|$)', content, re.DOTALL)
        if anna_match:
            anna_text = anna_match.group(0).strip()
            enhanced_anna = f"""–ü–†–ï–ü–û–î–ê–í–ê–¢–ï–õ–¨ –ö–£–†–°–ê "–Æ–ù–´–ô –û–†–ê–¢–û–†"

{anna_text}

–ö—É—Ä—Å "–Æ–Ω—ã–π –û—Ä–∞—Ç–æ—Ä" (7-10 –ª–µ—Ç) - 6000 –≥—Ä–Ω/–º–µ—Å—è—Ü, 3 –º–µ—Å—è—Ü–∞, –≥—Ä—É–ø–ø—ã –¥–æ 8 –¥–µ—Ç–µ–π.
–†–µ–∑—É–ª—å—Ç–∞—Ç: 94% –¥–µ—Ç–µ–π –∏–∑–±–∞–≤–ª—è—é—Ç—Å—è –æ—Ç —Å—Ç—Ä–∞—Ö–∞ –ø—É–±–ª–∏—á–Ω—ã—Ö –≤—ã—Å—Ç—É–ø–ª–µ–Ω–∏–π."""
            
            chunks.append({
                "text": enhanced_anna,
                "type": "teacher_course_link",
                "teacher": "anna_kovalenko",
                "course": "young_orator"
            })
        
        # –î–º–∏—Ç—Ä–∏–π –ü–µ—Ç—Ä–æ–≤ + –≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –ö–æ–º–ø–∞—Å  
        dmitry_match = re.search(r'–î–ú–ò–¢–†–ò–ô –ü–ï–¢–†–û–í.*?(?=–ï–õ–ï–ù–ê –°–ò–î–û–†–û–í–ê|$)', content, re.DOTALL)
        if dmitry_match:
            dmitry_text = dmitry_match.group(0).strip()
            enhanced_dmitry = f"""–ü–†–ï–ü–û–î–ê–í–ê–¢–ï–õ–¨ –ö–£–†–°–ê "–≠–ú–û–¶–ò–û–ù–ê–õ–¨–ù–´–ô –ö–û–ú–ü–ê–°"

{dmitry_text}

–ö—É—Ä—Å "–≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –ö–æ–º–ø–∞—Å" (9-12 –ª–µ—Ç) - 7500 –≥—Ä–Ω/–º–µ—Å—è—Ü, 4 –º–µ—Å—è—Ü–∞, –≥—Ä—É–ø–ø—ã –¥–æ 6 –¥–µ—Ç–µ–π.
–†–µ–∑—É–ª—å—Ç–∞—Ç: –°–Ω–∏–∂–µ–Ω–∏–µ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–Ω–æ—Å—Ç–∏ –Ω–∞ 76%, –ø–æ–≤—ã—à–µ–Ω–∏–µ —ç–º–ø–∞—Ç–∏–∏ –Ω–∞ 82%."""
            
            chunks.append({
                "text": enhanced_dmitry,
                "type": "teacher_course_link", 
                "teacher": "dmitry_petrov",
                "course": "emotional_compass"
            })
        
        # –ï–ª–µ–Ω–∞ –°–∏–¥–æ—Ä–æ–≤–∞ + –ö–∞–ø–∏—Ç–∞–Ω –ü—Ä–æ–µ–∫—Ç–æ–≤
        elena_match = re.search(r'–ï–õ–ï–ù–ê –°–ò–î–û–†–û–í–ê.*?(?=–û–õ–¨–ì–ê –ú–ò–†–ù–ê–Ø|–ü–†–ò–ù–¶–ò–ü–´ –†–ê–ë–û–¢–´|$)', content, re.DOTALL)
        if elena_match:
            elena_text = elena_match.group(0).strip()
            enhanced_elena = f"""–ü–†–ï–ü–û–î–ê–í–ê–¢–ï–õ–¨ –ö–£–†–°–ê "–ö–ê–ü–ò–¢–ê–ù –ü–†–û–ï–ö–¢–û–í"

{elena_text}

–ö—É—Ä—Å "–ö–∞–ø–∏—Ç–∞–Ω –ü—Ä–æ–µ–∫—Ç–æ–≤" (11-14 –ª–µ—Ç) - 8000 –≥—Ä–Ω/–º–µ—Å—è—Ü, 5 –º–µ—Å—è—Ü–µ–≤, –ø—Ä–æ–µ–∫—Ç–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã 4-5 —á–µ–ª–æ–≤–µ–∫.
–†–µ–∑—É–ª—å—Ç–∞—Ç: 85% –≤—ã–ø—É—Å–∫–Ω–∏–∫–æ–≤ —Å—Ç–∞–Ω–æ–≤—è—Ç—Å—è –ª–∏–¥–µ—Ä–∞–º–∏ –≤ –∫–ª–∞—Å—Å–∞—Ö."""
            
            chunks.append({
                "text": enhanced_elena,
                "type": "teacher_course_link",
                "teacher": "elena_sidorova", 
                "course": "project_captain"
            })
        
        return chunks

    def chunk_faq_detailed(self, content: str) -> List[Dict]:
        """
        –ü—Ä–∞–≤–∏–ª–∞ –¥–ª—è faq_detailed.txt - –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ —Ç–µ–º–∞–º
        """
        chunks = []
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ –æ—Å–Ω–æ–≤–Ω—ã–º —Ä–∞–∑–¥–µ–ª–∞–º FAQ
        sections = re.split(r'\n---\n', content)
        
        for section in sections:
            if len(section.strip()) < 200:
                continue
                
            section = section.strip()
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —Ä–∞–∑–¥–µ–ª–∞ –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫—É
            if "–û–ë–©–ò–ï –í–û–ü–†–û–°–´" in section:
                chunks.append({"text": section, "type": "faq_general"})
            elif "–†–ï–ó–£–õ–¨–¢–ê–¢–ê–• –ò –≠–§–§–ï–ö–¢–ò–í–ù–û–°–¢–ò" in section:
                chunks.append({"text": section, "type": "faq_results"})
            elif "–¢–ï–•–ù–ò–ß–ï–°–ö–ò–ï –ò –û–†–ì–ê–ù–ò–ó–ê–¶–ò–û–ù–ù–´–ï" in section:
                chunks.append({"text": section, "type": "faq_technical"})
            elif "–°–ï–†–¢–ò–§–ò–ö–ê–¶–ò–ò –ò –î–û–ö–£–ú–ï–ù–¢–ê–•" in section:
                chunks.append({"text": section, "type": "faq_certificates"})
            elif "–§–ò–ù–ê–ù–°–û–í–´–ï –í–û–ü–†–û–°–´" in section:
                chunks.append({"text": section, "type": "faq_financial"})
            elif "–ü–†–ï–ü–û–î–ê–í–ê–¢–ï–õ–Ø–• –ò –ú–ï–¢–û–î–ò–ö–ê–•" in section:
                chunks.append({"text": section, "type": "faq_methodology"})
            else:
                chunks.append({"text": section, "type": "faq_other"})
        
        return chunks

    def chunk_methodology_approach(self, content: str) -> List[Dict]:
        """
        –ü—Ä–∞–≤–∏–ª–∞ –¥–ª—è methodology_approach.txt
        """
        chunks = []
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ –∫–ª—é—á–µ–≤—ã–º —Ä–∞–∑–¥–µ–ª–∞–º
        sections = re.split(r'\n---\n', content)
        
        for section in sections:
            section = section.strip()
            if len(section) < 300:
                continue
                
            if "–ü–†–ê–ö–¢–ò–ö–ê + –ò–ì–†–ê + –†–ï–§–õ–ï–ö–°–ò–Ø" in section:
                chunks.append({"text": section, "type": "methodology_core"})
            elif "–ò–ù–î–ò–í–ò–î–£–ê–õ–¨–ù–´–ô –ü–û–î–•–û–î" in section:
                chunks.append({"text": section, "type": "methodology_individual"})
            elif "–¢–ï–•–ù–û–õ–û–ì–ò–ß–ï–°–ö–ê–Ø –ü–û–î–î–ï–†–ñ–ö–ê" in section:
                chunks.append({"text": section, "type": "methodology_tech"})
            elif "–ì–ï–ô–ú–ò–§–ò–ö–ê–¶–ò–Ø" in section:
                chunks.append({"text": section, "type": "methodology_gamification"})
            elif "–ù–ê–£–ß–ù–ê–Ø –û–°–ù–û–í–ê" in section:
                chunks.append({"text": section, "type": "methodology_science"})
            elif "–í–û–ó–†–ê–°–¢–ù–´–ï –û–°–û–ë–ï–ù–ù–û–°–¢–ò" in section:
                chunks.append({"text": section, "type": "methodology_age"})
            else:
                chunks.append({"text": section, "type": "methodology_other"})
        
        return chunks

    def chunk_standard_document(self, content: str, doc_type: str) -> List[Dict]:
        """
        –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ —á–∞–Ω–∫–æ–≤–∞–Ω–∏–µ –¥–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        """
        chunks = []
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ —Ä–∞–∑–¥–µ–ª–∞–º —Å ---
        sections = re.split(r'\n---\n', content)
        
        for section in sections:
            section = section.strip()
            if len(section) < 400:
                continue
                
            if len(section) > 1200:
                # –†–∞–∑–±–∏–≤–∞–µ–º –¥–ª–∏–Ω–Ω—É—é —Å–µ–∫—Ü–∏—é –ø–æ –∞–±–∑–∞—Ü–∞–º
                paragraphs = re.split(r'\n\n', section)
                current_chunk = ""
                
                for paragraph in paragraphs:
                    potential = current_chunk + "\n\n" + paragraph if current_chunk else paragraph
                    
                    if len(potential) > 1000 and current_chunk:
                        chunks.append({"text": current_chunk.strip(), "type": doc_type})
                        current_chunk = paragraph
                    else:
                        current_chunk = potential
                
                if current_chunk:
                    chunks.append({"text": current_chunk.strip(), "type": doc_type})
            else:
                chunks.append({"text": section, "type": doc_type})
        
        return chunks

    def process_all_documents(self, directory_path: str) -> List[Dict]:
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–º–∏ –ø—Ä–∞–≤–∏–ª–∞–º–∏
        """
        print(f"\nüìÇ –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑: {directory_path}")
        
        all_chunks = []
        chunk_id = 0
        
        # 1. –°—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–∏–µ —á–∞–Ω–∫–∏
        print("üéØ –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–∏—Ö —á–∞–Ω–∫–æ–≤...")
        strategic_chunks = self.create_strategic_overview_chunks()
        
        for chunk in strategic_chunks:
            all_chunks.append({
                "id": f"ukido-strategic-{chunk_id}",
                "text": chunk["text"],
                "metadata": {
                    "source": "strategic_overview",
                    "chunk_type": chunk["type"],
                    "priority": chunk.get("priority", "normal"),
                    "chunk_length": len(chunk["text"])
                }
            })
            chunk_id += 1
        
        # 2. –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        print("üìö –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–º–∏ –ø—Ä–∞–≤–∏–ª–∞–º–∏...")
        
        document_rules = {
            "courses_detailed.txt": self.chunk_courses_detailed,
            "teachers_team.txt": self.chunk_teachers_team, 
            "faq_detailed.txt": self.chunk_faq_detailed,
            "methodology_approach.txt": self.chunk_methodology_approach
        }
        
        files = [f for f in os.listdir(directory_path) if f.endswith('.txt')]
        
        for filename in files:
            print(f"üìÑ {filename}")
            
            with open(os.path.join(directory_path, filename), 'r', encoding='utf-8') as f:
                content = f.read().strip()
            
            if not content:
                continue
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞ –∏–ª–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ —á–∞–Ω–∫–æ–≤–∞–Ω–∏–µ
            if filename in document_rules:
                doc_chunks = document_rules[filename](content)
            else:
                doc_type = filename.replace('.txt', '').replace('_', '-')
                doc_chunks = self.chunk_standard_document(content, doc_type)
            
            # –î–æ–±–∞–≤–ª—è–µ–º —á–∞–Ω–∫–∏
            for chunk in doc_chunks:
                all_chunks.append({
                    "id": f"ukido-{chunk_id}",
                    "text": chunk["text"],
                    "metadata": {
                        "source": filename,
                        "chunk_type": chunk["type"],
                        "chunk_length": len(chunk["text"]),
                        **{k: v for k, v in chunk.items() if k not in ["text", "type"]}
                    }
                })
                chunk_id += 1
            
            print(f"   ‚úÖ {len(doc_chunks)} —á–∞–Ω–∫–æ–≤")
        
        print(f"\nüìä –ò–¢–û–ì–û: {len(all_chunks)} —á–∞–Ω–∫–æ–≤")
        return all_chunks

    def vectorize_and_upload(self, chunks: List[Dict]) -> bool:
        """
        –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –∏ –∑–∞–≥—Ä—É–∑–∫–∞ –≤ Pinecone
        """
        print(f"\nüîÑ –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è {len(chunks)} —á–∞–Ω–∫–æ–≤...")
        
        vectors = []
        
        for i, chunk_data in enumerate(chunks):
            try:
                embedding = genai.embed_content(
                    model=embedding_model,
                    content=chunk_data['text'],
                    task_type="RETRIEVAL_DOCUMENT"
                )
                
                vectors.append({
                    "id": chunk_data['id'],
                    "values": embedding['embedding'],
                    "metadata": {
                        "text": chunk_data['text'],
                        **chunk_data['metadata']
                    }
                })
                
                if (i + 1) % 10 == 0:
                    print(f"   üìä {i + 1}/{len(chunks)}")
                
                time.sleep(0.1)
                
            except Exception as e:
                print(f"   ‚ùå –û—à–∏–±–∫–∞ {chunk_data['id']}: {e}")
                continue
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –≤ Pinecone
        print(f"\n‚òÅÔ∏è –ó–∞–≥—Ä—É–∑–∫–∞ {len(vectors)} –≤–µ–∫—Ç–æ—Ä–æ–≤ –≤ Pinecone...")
        
        try:
            pc = Pinecone(api_key=PINECONE_API_KEY)
            index = pc.Index(host=PINECONE_HOST_FACTS)
            
            # –û—á–∏—Å—Ç–∫–∞
            print("   üóëÔ∏è –û—á–∏—Å—Ç–∫–∞ –∏–Ω–¥–µ–∫—Å–∞...")
            index.delete(delete_all=True)
            time.sleep(5)
            
            # –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞—Ç—á–∞–º–∏
            batch_size = 100
            for i in range(0, len(vectors), batch_size):
                batch = vectors[i:i+batch_size]
                index.upsert(vectors=batch)
                print(f"   üì¶ –ë–∞—Ç—á {i//batch_size + 1}/{(len(vectors) + batch_size - 1)//batch_size}")
                time.sleep(1)
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞
            time.sleep(3)
            stats = index.describe_index_stats()
            print(f"   ‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {stats.total_vector_count} –≤–µ–∫—Ç–æ—Ä–æ–≤")
            
            return True
            
        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞ Pinecone: {e}")
            return False

def main():
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è
    """
    print("üöÄ DETERMINISTIC BUSINESS-CRITICAL RECHUNKING")
    print("üéØ –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å –¥–ª—è Telegram –±–æ—Ç–∞")
    print("=" * 50)
    
    chunker = DeterministicBusinessChunker()
    
    try:
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        chunks = chunker.process_all_documents("data_facts")
        
        if not chunks:
            print("‚ùå –ù–µ—Ç —á–∞–Ω–∫–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
            return False
        
        # –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –∏ –∑–∞–≥—Ä—É–∑–∫–∞
        success = chunker.vectorize_and_upload(chunks)
        
        if success:
            print("\nüéâ RECHUNKING –ó–ê–í–ï–†–®–ï–ù!")
            print("üß™ –¢–µ—Å—Ç–∏—Ä—É–π: https://ukidoaiassistant-production.up.railway.app/test-rag")
            return True
        else:
            print("\n‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏")
            return False
            
    except Exception as e:
        print(f"\nüí• –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        return False

if __name__ == "__main__":
    success = main()
    if success:
        print("\n‚ú® RAG –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –∫–æ–Ω–≤–µ—Ä—Å–∏–∏!")
    else:
        print("\n‚ö†Ô∏è –¢—Ä–µ–±—É–µ—Ç—Å—è —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –æ—à–∏–±–æ–∫")
