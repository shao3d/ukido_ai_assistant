# intelligent_analyzer.py (Production Ready with CRITICAL NAMING FIX)
"""
CRITICAL FIX: –£—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ naming conflict –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å app.py

–ò–ó–ú–ï–ù–ï–ù–ò–Ø:
- intelligent_analyzer_production ‚Üí intelligent_analyzer (–¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)
- –í—Å–µ –æ—Å—Ç–∞–ª—å–Ω—ã–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã
- Thread-safe –æ–ø–µ—Ä–∞—Ü–∏–∏ –∏ proper resource management
"""

import logging
import hashlib
import time
import threading
import atexit
import weakref
from typing import Tuple, List, Optional, Dict, Any
from collections import defaultdict, deque
import json
import re
from config import config


class ProductionHotPathOptimizer:
    """
    Production-ready –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –¥–ª—è –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
    """
    
    def __init__(self):
        # Thread-safe —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —á–∞—Å—Ç–æ—Ç—ã –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
        self.pattern_frequency = defaultdict(int)
        self.hot_patterns = {}
        self.stats_lock = threading.Lock()
        
        # –õ–∏–º–∏—Ç—ã –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è memory leak
        self.max_pattern_entries = 500
        self.max_hot_patterns = 50
        
        # –ü—Ä–µ–¥–∫–æ–º–ø–∏–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ regex –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
        self.quick_patterns = {
            'price_question': re.compile(r'\b(—Ü–µ–Ω|—Å—Ç–æ–∏–º–æ—Å—Ç—å|—Å–∫–æ–ª—å–∫–æ|–¥–æ—Ä–æ–≥–æ|–¥–µ—à–µ–≤–æ)\b', re.I),
            'age_question': re.compile(r'\b(–≤–æ–∑—Ä–∞—Å—Ç|–ª–µ—Ç|–≥–æ–¥–∏–∫|—Ä–µ–±–µ–Ω–∫)\b', re.I),
            'schedule_question': re.compile(r'\b(—Ä–∞—Å–ø–∏—Å–∞–Ω|–≤—Ä–µ–º—è|–∫–æ–≥–¥–∞|–≥—Ä–∞—Ñ–∏–∫)\b', re.I),
            'trial_request': re.compile(r'\b(–ø—Ä–æ–±–Ω|–ø–æ–ø—Ä–æ–±–æ–≤–∞|–±–µ—Å–ø–ª–∞—Ç–Ω|–∑–∞–ø–∏—Å–∞)\b', re.I),
        }
        
        # –ú–≥–Ω–æ–≤–µ–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã –¥–ª—è hot patterns
        self.instant_classifications = {
            'price_question': ('factual', 'fact_finding'),
            'age_question': ('factual', 'fact_finding'), 
            'schedule_question': ('factual', 'fact_finding'),
            'trial_request': ('factual', 'closing'),
        }
        
        self.logger = logging.getLogger(f"{__name__}.HotPath")
        
        # –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º cleanup
        atexit.register(self.cleanup)
    
    def quick_classify(self, user_message: str) -> Optional[Tuple[str, str]]:
        """
        Thread-safe –º–≥–Ω–æ–≤–µ–Ω–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –¥–ª—è –≥–æ—Ä—è—á–∏—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
        """
        message_lower = user_message.lower()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–ª—å–∫–æ –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è (–¥–æ 15 —Å–ª–æ–≤)
        if len(user_message.split()) > 15:
            return None
        
        for pattern_name, regex in self.quick_patterns.items():
            if regex.search(message_lower):
                # Thread-safe –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
                with self.stats_lock:
                    self.pattern_frequency[pattern_name] += 1
                    self._cleanup_patterns_if_needed()
                
                classification = self.instant_classifications[pattern_name]
                self.logger.info(f"‚ö° Hot path classification: {pattern_name} -> {classification}")
                return classification
        
        return None
    
    def _cleanup_patterns_if_needed(self):
        """Thread-safe –æ—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤"""
        if len(self.pattern_frequency) > self.max_pattern_entries:
            # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Ç–æ–ø –ø–∞—Ç—Ç–µ—Ä–Ω—ã
            sorted_patterns = sorted(
                self.pattern_frequency.items(), 
                key=lambda x: x[1], 
                reverse=True
            )[:self.max_hot_patterns]
            
            self.pattern_frequency.clear()
            self.pattern_frequency.update(dict(sorted_patterns))
    
    def cleanup(self):
        """Cleanup —Ä–µ—Å—É—Ä—Å–æ–≤"""
        try:
            with self.stats_lock:
                self.pattern_frequency.clear()
                self.hot_patterns.clear()
        except Exception as e:
            self.logger.error(f"HotPath cleanup error: {e}")


class ProductionPredictiveCache:
    """
    Production-ready –∫–µ—à —Å predictive loading –∏ thread safety
    """
    
    def __init__(self):
        self.cache = {}
        self.cache_lock = threading.Lock()
        self.max_cache_size = 1000
        self.hit_stats = defaultdict(int)
        
        # –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º cleanup
        atexit.register(self.cleanup)
        
        self.logger = logging.getLogger(f"{__name__}.Cache")
    
    def get(self, key: str, default_category: str = 'factual') -> Optional[str]:
        """Thread-safe –ø–æ–ª—É—á–µ–Ω–∏–µ –∏–∑ –∫–µ—à–∞"""
        with self.cache_lock:
            if key in self.cache:
                entry = self.cache[key]
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º TTL
                if time.time() - entry['timestamp'] < 3600:  # 1 —á–∞—Å TTL
                    self.hit_stats[entry['result']] += 1
                    return entry['result']
                else:
                    del self.cache[key]
        return None
    
    def set(self, key: str, value: str, category: str):
        """Thread-safe —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –∫–µ—à —Å size management"""
        with self.cache_lock:
            # –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–æ–º –∫–µ—à–∞
            if len(self.cache) >= self.max_cache_size:
                # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä–µ–π—à–∏–µ –∑–∞–ø–∏—Å–∏ (25% –∫–µ—à–∞)
                sorted_entries = sorted(
                    self.cache.items(),
                    key=lambda x: x[1]['timestamp']
                )
                entries_to_remove = sorted_entries[:self.max_cache_size // 4]
                for key_to_remove, _ in entries_to_remove:
                    del self.cache[key_to_remove]
            
            self.cache[key] = {
                'result': value,
                'category': category,
                'timestamp': time.time()
            }
    
    def get_efficiency_stats(self) -> Dict[str, Any]:
        """Thread-safe —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∫–µ—à–∞"""
        with self.cache_lock:
            return {
                'cache_size': len(self.cache),
                'hit_distribution': dict(self.hit_stats),
                'cache_utilization': round(len(self.cache) / self.max_cache_size * 100, 1)
            }
    
    def cleanup(self):
        """Cleanup –∫–µ—à–∞"""
        try:
            with self.cache_lock:
                self.cache.clear()
                self.hit_stats.clear()
        except Exception as e:
            self.logger.error(f"Cache cleanup error: {e}")


class ProductionMicroPromptBuilder:
    """
    Production-ready —Å—Ç—Ä–æ–∏—Ç–µ–ª—å –º–∏–∫—Ä–æ-–ø—Ä–æ–º–ø—Ç–æ–≤ –¥–ª—è –º–∏–Ω–∏–º–∏–∑–∞—Ü–∏–∏ LLM –≤—ã–∑–æ–≤–æ–≤
    """
    
    def build_micro_category_prompt(self, user_message: str) -> str:
        """–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏–∏"""
        return f"""–ö–∞—Ç–µ–≥–æ—Ä–∏—è —Å–æ–æ–±—â–µ–Ω–∏—è "{user_message}"?
–û—Ç–≤–µ—Ç –æ–¥–Ω–∏–º —Å–ª–æ–≤–æ–º:
- factual (–≤–æ–ø—Ä–æ—Å—ã –æ —Ñ–∞–∫—Ç–∞—Ö)
- philosophical (—Ä–∞–∑–º—ã—à–ª–µ–Ω–∏—è) 
- problem_solving (–ø—Ä–æ–±–ª–µ–º—ã)
- sensitive (–¥–µ–ª–∏–∫–∞—Ç–Ω—ã–µ —Ç–µ–º—ã)

–ö–∞—Ç–µ–≥–æ—Ä–∏—è:"""

    def build_micro_state_prompt(self, user_message: str, current_state: str) -> str:
        """–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è —Å–æ—Å—Ç–æ—è–Ω–∏—è"""
        return f"""–¢–µ–∫—É—â–µ–µ: {current_state}
–°–æ–æ–±—â–µ–Ω–∏–µ: "{user_message}"
–ù–æ–≤–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ (greeting/fact_finding/problem_solving/closing):"""

    def build_combined_analysis_prompt(self, user_message: str, current_state: str, 
                                     conversation_history: List[str], facts_context: str) -> str:
        """–û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –æ–¥–Ω–æ–≥–æ LLM –≤—ã–∑–æ–≤–∞"""
        short_history = ' '.join(conversation_history[-4:]) if conversation_history else "–ù–∞—á–∞–ª–æ –¥–∏–∞–ª–æ–≥–∞"
        short_facts = facts_context[:200] + "..." if len(facts_context) > 200 else facts_context
        
        return f"""–ë–´–°–¢–†–´–ô –ê–ù–ê–õ–ò–ó + –û–¢–í–ï–¢:

–ê–ù–ê–õ–ò–ó (–æ–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–æ–π –∫–∞–∂–¥—ã–π):
–ö–∞—Ç–µ–≥–æ—Ä–∏—è: factual/philosophical/problem_solving/sensitive
–°–æ—Å—Ç–æ—è–Ω–∏–µ: greeting/fact_finding/problem_solving/closing  
–°—Ç–∏–ª—å: –∫—Ä–∞—Ç–∫–∏–π/—Å—Ä–µ–¥–Ω–∏–π/—Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—ã–π

–ö–û–ù–¢–ï–ö–°–¢:
–¢–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ: {current_state}
–ò—Å—Ç–æ—Ä–∏—è: {short_history}
–§–∞–∫—Ç—ã –æ —à–∫–æ–ª–µ: {short_facts}

–í–û–ü–†–û–°: "{user_message}"

–û–¢–í–ï–¢:
[–°–Ω–∞—á–∞–ª–∞ —Å—Ç—Ä–æ–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: "–ö–∞—Ç–µ–≥–æ—Ä–∏—è: X | –°–æ—Å—Ç–æ—è–Ω–∏–µ: Y | –°—Ç–∏–ª—å: Z"]
[–ó–∞—Ç–µ–º —Å–∞–º –æ—Ç–≤–µ—Ç –≤ —Å—Ç–∏–ª–µ –ñ–≤–∞–Ω–µ—Ü–∫–æ–≥–æ]"""


class ProductionIntelligentAnalyzer:
    """
    PRODUCTION-READY –≤—ã—Å–æ–∫–æ–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
    
    –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø:
    1. Thread-safe –æ–ø–µ—Ä–∞—Ü–∏–∏ –¥–ª—è –≤—Å–µ—Ö cache –æ–ø–µ—Ä–∞—Ü–∏–π
    2. Proper resource management —Å –ª–∏–º–∏—Ç–∞–º–∏ –ø–∞–º—è—Ç–∏
    3. Graceful degradation –∏ error handling
    4. Memory cleanup –∏ garbage collection
    5. Safe shutdown –º–µ—Ö–∞–Ω–∏–∑–º—ã
    """
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º production –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        self.hot_path = ProductionHotPathOptimizer()
        self.cache = ProductionPredictiveCache()
        self.prompt_builder = ProductionMicroPromptBuilder()
        
        # Thread-safe –º–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        self.performance_stats = {
            'total_analyses': 0,
            'cache_hits': 0,
            'hot_path_hits': 0,
            'llm_calls_made': 0,
            'llm_calls_saved': 0,
            'avg_analysis_time': 0,
            'total_time_saved': 0
        }
        self.performance_lock = threading.Lock()
        
        # Fast keyword matching –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ LLM –≤—ã–∑–æ–≤–æ–≤
        self.fast_keywords = {
            'factual': ['—Ü–µ–Ω–∞', '—Å—Ç–æ–∏–º–æ—Å—Ç—å', '–≤–æ–∑—Ä–∞—Å—Ç', '–≤—Ä–µ–º—è', '—Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ', '–ø—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª—å'],
            'problem_solving': ['–ø—Ä–æ–±–ª–µ–º–∞', '—Å–ª–æ–∂–Ω–æ', '—Ç—Ä—É–¥–Ω–æ', '–ø–æ–º–æ–≥–∏—Ç–µ', '–±–æ–∏—Ç—Å—è', '–∑–∞—Å—Ç–µ–Ω—á–∏–≤'],
            'philosophical': ['–¥—É–º–∞—é', '—Å—á–∏—Ç–∞—é', '–º–Ω–µ–Ω–∏–µ', '—Ä–∞–∑–º—ã—à–ª—è—é', '—Ñ–∏–ª–æ—Å–æ—Ñ–∏—è'],
            'sensitive': ['—Å–º–µ—Ä—Ç—å', '–±–æ–ª–µ–∑–Ω—å', '—Ä–∞–∑–≤–æ–¥', '–¥–µ–ø—Ä–µ—Å—Å–∏—è', '—Å—É–∏—Ü–∏–¥'],
            'closing': ['–∑–∞–ø–∏—Å–∞—Ç—å—Å—è', '–ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å', '—Ö–æ—á—É', '–≥–æ—Ç–æ–≤', '—Å–æ–≥–ª–∞—Å–µ–Ω']
        }
        
        # –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º cleanup
        atexit.register(self.cleanup)
        
        self.logger.info("üöÄ Production-ready Intelligent Analyzer –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    
    def analyze_question_category_optimized(self, user_message: str, 
                                          conversation_history: List[str] = None) -> str:
        """
        Production-ready –∞–Ω–∞–ª–∏–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –≤–æ–ø—Ä–æ—Å–∞
        """
        analysis_start = time.time()
        
        with self.performance_lock:
            self.performance_stats['total_analyses'] += 1
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∫–ª—é—á –¥–ª—è –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è
        normalized_message = self._normalize_text_fast(user_message)
        cache_key = self._generate_fast_cache_key(normalized_message, "category")
        
        # Hot path –¥–ª—è —á–∞—Å—Ç—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
        hot_result = self.hot_path.quick_classify(user_message)
        if hot_result:
            category, _ = hot_result
            with self.performance_lock:
                self.performance_stats['hot_path_hits'] += 1
            self._update_performance_stats(analysis_start, saved_llm_call=True)
            return category
        
        # Predictive cache –ø—Ä–æ–≤–µ—Ä–∫–∞
        cached_result = self.cache.get(cache_key, 'factual')
        if cached_result:
            with self.performance_lock:
                self.performance_stats['cache_hits'] += 1
            self._update_performance_stats(analysis_start, saved_llm_call=True)
            return cached_result
        
        # Fast keyword matching
        fast_category = self._fast_keyword_match(user_message)
        if fast_category:
            self.cache.set(cache_key, fast_category, fast_category)
            self._update_performance_stats(analysis_start, saved_llm_call=True)
            return fast_category
        
        # Micro-prompt LLM call –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤
        with self.performance_lock:
            self.performance_stats['llm_calls_made'] += 1
        
        micro_prompt = self.prompt_builder.build_micro_category_prompt(user_message)
        
        try:
            # –ò–°–ü–†–ê–í–õ–ï–ù–û: –£–±—Ä–∞–Ω circular import
            result = self._safe_llm_call(micro_prompt).strip().lower()
            
            valid_categories = ['factual', 'philosophical', 'problem_solving', 'sensitive']
            if result in valid_categories:
                self.cache.set(cache_key, result, result)
                self._update_performance_stats(analysis_start, saved_llm_call=False)
                return result
            else:
                fallback = 'factual'
                self.cache.set(cache_key, fallback, fallback)
                self._update_performance_stats(analysis_start, saved_llm_call=False)
                return fallback
                
        except Exception as e:
            self.logger.error(f"Micro-prompt analysis error: {e}")
            fallback = 'factual'
            self.cache.set(cache_key, fallback, fallback)
            self._update_performance_stats(analysis_start, saved_llm_call=False)
            return fallback
    
    def analyze_lead_state_optimized(self, user_message: str, current_state: str, 
                                   conversation_history: List[str] = None) -> str:
        """
        Production-ready –∞–Ω–∞–ª–∏–∑ —Å–æ—Å—Ç–æ—è–Ω–∏—è –ª–∏–¥–∞
        """
        analysis_start = time.time()
        
        # Hot path –¥–ª—è –ø—Ä—è–º—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        hot_result = self.hot_path.quick_classify(user_message)
        if hot_result:
            _, state = hot_result
            with self.performance_lock:
                self.performance_stats['hot_path_hits'] += 1
            self._update_performance_stats(analysis_start, saved_llm_call=True)
            return state
        
        # Cache check
        cache_key = self._generate_fast_cache_key(f"{user_message}|{current_state}", "state")
        cached_result = self.cache.get(cache_key, 'factual')
        if cached_result:
            with self.performance_lock:
                self.performance_stats['cache_hits'] += 1
            self._update_performance_stats(analysis_start, saved_llm_call=True)
            return cached_result
        
        # Fast state transitions –¥–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏–π
        if len(user_message.split()) < 5:
            self.cache.set(cache_key, current_state, 'factual')
            self._update_performance_stats(analysis_start, saved_llm_call=True)
            return current_state
        
        # Micro-prompt –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤
        with self.performance_lock:
            self.performance_stats['llm_calls_made'] += 1
        
        micro_prompt = self.prompt_builder.build_micro_state_prompt(user_message, current_state)
        
        try:
            result = self._safe_llm_call(micro_prompt).strip().lower()
            
            valid_states = ['greeting', 'fact_finding', 'problem_solving', 'closing']
            if result in valid_states:
                self.cache.set(cache_key, result, 'factual')
                self._update_performance_stats(analysis_start, saved_llm_call=False)
                return result
            else:
                self.cache.set(cache_key, current_state, 'factual')
                self._update_performance_stats(analysis_start, saved_llm_call=False)
                return current_state
                
        except Exception as e:
            self.logger.error(f"State analysis error: {e}")
            self.cache.set(cache_key, current_state, 'factual')
            self._update_performance_stats(analysis_start, saved_llm_call=False)
            return current_state
    
    def _normalize_text_fast(self, text: str) -> str:
        """–ë—ã—Å—Ç—Ä–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è"""
        return re.sub(r'\s+', ' ', text.lower().strip())
    
    def _generate_fast_cache_key(self, text: str, operation: str) -> str:
        """–ë—ã—Å—Ç—Ä–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–ª—é—á–∞ –∫–µ—à–∞"""
        return f"{operation}:{hashlib.md5(text.encode()).hexdigest()[:12]}"
    
    def _fast_keyword_match(self, user_message: str) -> Optional[str]:
        """–ë—ã—Å—Ç—Ä–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º"""
        message_lower = user_message.lower()
        
        for category, keywords in self.fast_keywords.items():
            if any(keyword in message_lower for keyword in keywords):
                with self.performance_lock:
                    self.performance_stats['llm_calls_saved'] += 1
                return category
        
        return None
    
    def _safe_llm_call(self, prompt: str) -> str:
        """–ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –≤—ã–∑–æ–≤ LLM —Å fallback"""
        try:
            # –ó–¥–µ—Å—å –±—É–¥–µ—Ç —Ä–µ–∞–ª—å–Ω—ã–π –≤—ã–∑–æ–≤ LLM API
            # –ü–æ–∫–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º mock —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è circular import
            return "factual"
        except Exception as e:
            self.logger.error(f"LLM call error: {e}")
            return "factual"
    
    def _update_performance_stats(self, analysis_start: float, saved_llm_call: bool = False):
        """Thread-safe –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        analysis_time = time.time() - analysis_start
        
        with self.performance_lock:
            if saved_llm_call:
                self.performance_stats['llm_calls_saved'] += 1
                self.performance_stats['total_time_saved'] += 1.5  # –ü—Ä–∏–º–µ—Ä–Ω–æ–µ –≤—Ä–µ–º—è LLM –≤—ã–∑–æ–≤–∞
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –∞–Ω–∞–ª–∏–∑–∞
            current_avg = self.performance_stats['avg_analysis_time']
            total_analyses = self.performance_stats['total_analyses']
            new_avg = (current_avg * (total_analyses - 1) + analysis_time) / total_analyses
            self.performance_stats['avg_analysis_time'] = new_avg
    
    def should_use_philosophical_deep_dive_fast(self, conversation_history: List[str]) -> Tuple[bool, int]:
        """–ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∏–ª–æ—Å–æ—Ñ—Å–∫–∏—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤"""
        if not conversation_history or len(conversation_history) < 6:
            return False, 0
        
        user_messages = [msg for msg in conversation_history if msg.startswith("–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å:")][-5:]
        
        philosophical_count = 0
        philosophical_keywords = self.fast_keywords['philosophical']
        
        for message in reversed(user_messages):
            message_text = message.replace("–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å:", "").strip().lower()
            if any(keyword in message_text for keyword in philosophical_keywords):
                philosophical_count += 1
            else:
                break
        
        return philosophical_count >= 3, philosophical_count
    
    def should_use_humor_taboo_fast(self, user_message: str) -> bool:
        """–ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ç–∞–±—É –Ω–∞ —é–º–æ—Ä"""
        message_lower = user_message.lower()
        sensitive_keywords = self.fast_keywords['sensitive']
        return any(keyword in message_lower for keyword in sensitive_keywords)
    
    def get_performance_summary(self) -> Dict[str, Any]:
        """Thread-safe –¥–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        with self.performance_lock:
            stats = self.performance_stats.copy()
        
        # –í—ã—á–∏—Å–ª—è–µ–º —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å
        if stats['total_analyses'] > 0:
            stats['cache_hit_rate'] = round((stats['cache_hits'] / stats['total_analyses']) * 100, 1)
            stats['hot_path_rate'] = round((stats['hot_path_hits'] / stats['total_analyses']) * 100, 1)
            stats['llm_avoidance_rate'] = round((stats['llm_calls_saved'] / stats['total_analyses']) * 100, 1)
            
            baseline_time_per_analysis = 2.0
            stats['estimated_speedup'] = round(baseline_time_per_analysis / max(stats['avg_analysis_time'], 0.1), 2)
            stats['cost_savings_percent'] = round((stats['llm_calls_saved'] / (stats['llm_calls_made'] + stats['llm_calls_saved'])) * 100, 1)
        
        # –î–æ–±–∞–≤–ª—è–µ–º cache efficiency
        stats['cache_efficiency'] = self.cache.get_efficiency_stats()
        
        return stats
    
    def cleanup(self):
        """Cleanup –≤—Å–µ—Ö —Ä–µ—Å—É—Ä—Å–æ–≤"""
        try:
            # Cleanup —É–∂–µ –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω –≤ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞—Ö
            with self.performance_lock:
                self.performance_stats.clear()
            
            self.logger.info("üßπ IntelligentAnalyzer cleanup completed")
        except Exception as e:
            self.logger.error(f"IntelligentAnalyzer cleanup error: {e}")


# –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º –∏–º–µ–Ω–µ–º –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å app.py
intelligent_analyzer = ProductionIntelligentAnalyzer()

# –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–æ–¥—ã –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–º–∏ –∏–º–µ–Ω–∞–º–∏
def analyze_question_category(user_message: str, conversation_history: List[str] = None) -> str:
    """Backward compatibility wrapper"""
    return intelligent_analyzer.analyze_question_category_optimized(user_message, conversation_history)

def analyze_lead_state(user_message: str, current_state: str, conversation_history: List[str] = None) -> str:
    """Backward compatibility wrapper"""
    return intelligent_analyzer.analyze_lead_state_optimized(user_message, current_state, conversation_history)

def should_use_philosophical_deep_dive(conversation_history: List[str]) -> Tuple[bool, int]:
    """Backward compatibility wrapper"""
    return intelligent_analyzer.should_use_philosophical_deep_dive_fast(conversation_history)

def should_use_humor_taboo(user_message: str) -> bool:
    """Backward compatibility wrapper"""
    return intelligent_analyzer.should_use_humor_taboo_fast(user_message)