"""
–ê–Ω–∞–ª–∏–∑ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ Pinecone –∏–Ω–¥–µ–∫—Å–∞ "ukido"
–≠—Ç–æ—Ç —Å–∫—Ä–∏–ø—Ç –ø–æ–¥–∫–ª—é—á–∞–µ—Ç—Å—è –∫ Pinecone –∏ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞–Ω–Ω—ã—Ö –≤ –∏–Ω–¥–µ–∫—Å–µ.
"""

import os
import json
import random
import logging
from datetime import datetime
from collections import defaultdict, Counter
from typing import Dict, List, Any, Optional
from pinecone import Pinecone
from dotenv import load_dotenv
from config import config

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class PineconeAnalyzer:
    """
    –ö–ª–∞—Å—Å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ Pinecone –∏–Ω–¥–µ–∫—Å–∞
    """
    
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Pinecone"""
        self.pc = Pinecone(api_key=config.PINECONE_API_KEY)
        self.index = self.pc.Index(host=config.PINECONE_HOST_FACTS)
        self.index_name = "ukido"
        self.analysis_results = {}
        
    def get_index_stats(self) -> Dict[str, Any]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –æ–±—â–µ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∏–Ω–¥–µ–∫—Å–∞
        """
        try:
            stats = self.index.describe_index_stats()
            logger.info(f"üìä –ü–æ–ª—É—á–µ–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏–Ω–¥–µ–∫—Å–∞: {stats}")
            return stats
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}")
            return {}
    
    def get_random_vectors(self, count: int = 10) -> List[Dict[str, Any]]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ª—É—á–∞–π–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        """
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º query —Å —Å–ª—É—á–∞–π–Ω—ã–º –≤–µ–∫—Ç–æ—Ä–æ–º –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å–ª—É—á–∞–π–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            dummy_vector = [random.random() for _ in range(768)]  # –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å 768 –¥–ª—è Gemini
            
            results = self.index.query(
                vector=dummy_vector,
                top_k=count,
                include_metadata=True
            )
            
            logger.info(f"üé≤ –ü–æ–ª—É—á–µ–Ω–æ {len(results['matches'])} —Å–ª—É—á–∞–π–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤")
            return results['matches']
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å–ª—É—á–∞–π–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤: {e}")
            return []
    
    def analyze_metadata_structure(self, vectors: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤
        """
        metadata_fields = defaultdict(int)
        field_types = defaultdict(set)
        field_samples = defaultdict(list)
        
        for vector in vectors:
            metadata = vector.get('metadata', {})
            
            # –ü–æ–¥—Å—á–µ—Ç –ø–æ–ª–µ–π
            for field, value in metadata.items():
                metadata_fields[field] += 1
                field_types[field].add(type(value).__name__)
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–∏–º–µ—Ä—ã –∑–Ω–∞—á–µ–Ω–∏–π (–ø–µ—Ä–≤—ã–µ 3)
                if len(field_samples[field]) < 3:
                    field_samples[field].append(str(value)[:100])  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º sets –≤ lists –¥–ª—è JSON —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏
        field_types_json = {field: list(types) for field, types in field_types.items()}
        
        analysis = {
            'total_vectors_analyzed': len(vectors),
            'metadata_fields': dict(metadata_fields),
            'field_types': field_types_json,
            'field_samples': dict(field_samples)
        }
        
        logger.info(f"üîç –ù–∞–π–¥–µ–Ω–æ {len(metadata_fields)} —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø–æ–ª–µ–π –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö")
        return analysis
    
    def analyze_content_patterns(self, vectors: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        –ê–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –≤ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º
        """
        content_lengths = []
        sources = Counter()
        chunk_types = Counter()
        
        for vector in vectors:
            metadata = vector.get('metadata', {})
            
            # –ê–Ω–∞–ª–∏–∑ –¥–ª–∏–Ω—ã –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            content = metadata.get('content', '')
            if content:
                content_lengths.append(len(content))
            
            # –ê–Ω–∞–ª–∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
            source = metadata.get('source', 'Unknown')
            sources[source] += 1
            
            # –ê–Ω–∞–ª–∏–∑ —Ç–∏–ø–æ–≤ —á–∞–Ω–∫–æ–≤
            chunk_type = metadata.get('chunk_type', 'Unknown')
            chunk_types[chunk_type] += 1
        
        analysis = {
            'content_length_stats': {
                'min': min(content_lengths) if content_lengths else 0,
                'max': max(content_lengths) if content_lengths else 0,
                'avg': sum(content_lengths) / len(content_lengths) if content_lengths else 0
            },
            'top_sources': dict(sources.most_common(10)),
            'chunk_types': dict(chunk_types)
        }
        
        return analysis
    
    def run_full_analysis(self) -> Dict[str, Any]:
        """
        –ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –∏–Ω–¥–µ–∫—Å–∞
        """
        logger.info("üöÄ –ù–∞—á–∏–Ω–∞—é –ø–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ Pinecone –∏–Ω–¥–µ–∫—Å–∞")
        
        # 1. –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∏–Ω–¥–µ–∫—Å–∞
        logger.info("1Ô∏è‚É£ –ü–æ–ª—É—á–µ–Ω–∏–µ –æ–±—â–µ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏...")
        index_stats = self.get_index_stats()
        
        # 2. –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ª—É—á–∞–π–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤
        logger.info("2Ô∏è‚É£ –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ª—É—á–∞–π–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤...")
        random_vectors = self.get_random_vectors(10)
        
        # 3. –ê–Ω–∞–ª–∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        logger.info("3Ô∏è‚É£ –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö...")
        metadata_analysis = self.analyze_metadata_structure(random_vectors)
        
        # 4. –ê–Ω–∞–ª–∏–∑ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ
        logger.info("4Ô∏è‚É£ –ê–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ...")
        content_analysis = self.analyze_content_patterns(random_vectors)
        
        # –°–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –∏—Ç–æ–≥–æ–≤–æ–≥–æ –æ—Ç—á–µ—Ç–∞
        self.analysis_results = {
            'analysis_timestamp': datetime.now().isoformat(),
            'index_name': self.index_name,
            'index_statistics': index_stats,
            'metadata_analysis': metadata_analysis,
            'content_analysis': content_analysis,
            'sample_vectors': [
                {
                    'id': vector['id'],
                    'score': vector.get('score', 0),
                    'metadata_keys': list(vector.get('metadata', {}).keys())
                }
                for vector in random_vectors[:5]  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 5
            ]
        }
        
        logger.info("‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ")
        return self.analysis_results
    
    def save_report(self, filename: str = 'index_analysis.txt') -> None:
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫—Ä–∞—Å–∏–≤–æ–≥–æ –æ—Ç—á–µ—Ç–∞ –≤ —Ñ–∞–π–ª
        """
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                f.write("=" * 80 + "\n")
                f.write("üîç –ê–ù–ê–õ–ò–ó PINECONE –ò–ù–î–ï–ö–°–ê 'UKIDO'\n")
                f.write("=" * 80 + "\n\n")
                
                # –í—Ä–µ–º—è –∞–Ω–∞–ª–∏–∑–∞
                f.write(f"üìÖ –í—Ä–µ–º—è –∞–Ω–∞–ª–∏–∑–∞: {self.analysis_results['analysis_timestamp']}\n\n")
                
                # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
                f.write("üìä –û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ò–ù–î–ï–ö–°–ê\n")
                f.write("-" * 40 + "\n")
                stats = self.analysis_results['index_statistics']
                if stats:
                    f.write(f"‚Ä¢ –í—Å–µ–≥–æ –≤–µ–∫—Ç–æ—Ä–æ–≤: {stats.get('total_vector_count', 'N/A')}\n")
                    f.write(f"‚Ä¢ –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {stats.get('dimension', 'N/A')}\n")
                    f.write(f"‚Ä¢ –ó–∞–ø–æ–ª–Ω–µ–Ω–Ω–æ—Å—Ç—å: {stats.get('index_fullness', 'N/A')}\n")
                    
                    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –Ω–µ–π–º—Å–ø–µ–π—Å–∞—Ö
                    namespaces = stats.get('namespaces', {})
                    if namespaces:
                        f.write(f"‚Ä¢ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–µ–π–º—Å–ø–µ–π—Å–æ–≤: {len(namespaces)}\n")
                        for ns_name, ns_data in namespaces.items():
                            f.write(f"  - {ns_name}: {ns_data.get('vector_count', 0)} –≤–µ–∫—Ç–æ—Ä–æ–≤\n")
                f.write("\n")
                
                # –ê–Ω–∞–ª–∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
                f.write("üè∑Ô∏è –ê–ù–ê–õ–ò–ó –ú–ï–¢–ê–î–ê–ù–ù–´–•\n")
                f.write("-" * 40 + "\n")
                metadata = self.analysis_results['metadata_analysis']
                f.write(f"‚Ä¢ –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –≤–µ–∫—Ç–æ—Ä–æ–≤: {metadata['total_vectors_analyzed']}\n")
                f.write(f"‚Ä¢ –ù–∞–π–¥–µ–Ω–æ –ø–æ–ª–µ–π –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö: {len(metadata['metadata_fields'])}\n\n")
                
                f.write("üìã –ü–æ–ª—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö:\n")
                for field, count in metadata['metadata_fields'].items():
                    types = ", ".join(metadata['field_types'][field])
                    f.write(f"  ‚Ä¢ {field}: {count} –≤–µ–∫—Ç–æ—Ä–æ–≤ (—Ç–∏–ø: {types})\n")
                    
                    # –ü—Ä–∏–º–µ—Ä—ã –∑–Ω–∞—á–µ–Ω–∏–π
                    samples = metadata['field_samples'].get(field, [])
                    if samples:
                        f.write(f"    –ü—Ä–∏–º–µ—Ä—ã: {', '.join(samples[:2])}\n")
                f.write("\n")
                
                # –ê–Ω–∞–ª–∏–∑ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ
                f.write("üìù –ê–ù–ê–õ–ò–ó –°–û–î–ï–†–ñ–ò–ú–û–ì–û\n")
                f.write("-" * 40 + "\n")
                content = self.analysis_results['content_analysis']
                
                # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª–∏–Ω—ã –∫–æ–Ω—Ç–µ–Ω—Ç–∞
                length_stats = content['content_length_stats']
                f.write(f"‚Ä¢ –î–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞:\n")
                f.write(f"  - –ú–∏–Ω–∏–º—É–º: {length_stats['min']} —Å–∏–º–≤–æ–ª–æ–≤\n")
                f.write(f"  - –ú–∞–∫—Å–∏–º—É–º: {length_stats['max']} —Å–∏–º–≤–æ–ª–æ–≤\n")
                f.write(f"  - –°—Ä–µ–¥–Ω–µ–µ: {length_stats['avg']:.1f} —Å–∏–º–≤–æ–ª–æ–≤\n\n")
                
                # –¢–æ–ø –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
                f.write("üìö –¢–æ–ø –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–∞–Ω–Ω—ã—Ö:\n")
                for source, count in content['top_sources'].items():
                    f.write(f"  ‚Ä¢ {source}: {count} –≤–µ–∫—Ç–æ—Ä–æ–≤\n")
                f.write("\n")
                
                # –¢–∏–ø—ã —á–∞–Ω–∫–æ–≤
                f.write("üß© –¢–∏–ø—ã —á–∞–Ω–∫–æ–≤:\n")
                for chunk_type, count in content['chunk_types'].items():
                    f.write(f"  ‚Ä¢ {chunk_type}: {count} –≤–µ–∫—Ç–æ—Ä–æ–≤\n")
                f.write("\n")
                
                # –ü—Ä–∏–º–µ—Ä—ã –≤–µ–∫—Ç–æ—Ä–æ–≤
                f.write("üéØ –ü–†–ò–ú–ï–†–´ –í–ï–ö–¢–û–†–û–í\n")
                f.write("-" * 40 + "\n")
                for i, vector in enumerate(self.analysis_results['sample_vectors'], 1):
                    f.write(f"{i}. ID: {vector['id']}\n")
                    f.write(f"   Score: {vector['score']:.4f}\n")
                    f.write(f"   –ü–æ–ª—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö: {', '.join(vector['metadata_keys'])}\n\n")
                
                f.write("=" * 80 + "\n")
                f.write("‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω. –û—Ç—á–µ—Ç —Å–æ–∑–¥–∞–Ω –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏.\n")
                f.write("=" * 80 + "\n")
            
            logger.info(f"üìÑ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ —Ñ–∞–π–ª: {filename}")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—Ç—á–µ—Ç–∞: {e}")

def main():
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –∞–Ω–∞–ª–∏–∑–∞
    """
    print("üöÄ –ó–∞–ø—É—Å–∫ –∞–Ω–∞–ª–∏–∑–∞ Pinecone –∏–Ω–¥–µ–∫—Å–∞ 'ukido'...")
    
    try:
        # –°–æ–∑–¥–∞–µ–º –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä
        analyzer = PineconeAnalyzer()
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –∞–Ω–∞–ª–∏–∑
        results = analyzer.run_full_analysis()
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç
        analyzer.save_report()
        
        print("\n‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
        print("üìÑ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ —Ñ–∞–π–ª 'index_analysis.txt'")
        
        # –ö—Ä–∞—Ç–∫–∞—è —Å–≤–æ–¥–∫–∞
        stats = results['index_statistics']
        if stats:
            print(f"\nüìä –ö—Ä–∞—Ç–∫–∞—è —Å–≤–æ–¥–∫–∞:")
            print(f"   ‚Ä¢ –í—Å–µ–≥–æ –≤–µ–∫—Ç–æ—Ä–æ–≤: {stats.get('total_vector_count', 'N/A')}")
            print(f"   ‚Ä¢ –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {stats.get('dimension', 'N/A')}")
            print(f"   ‚Ä¢ –ü–æ–ª–µ–π –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö: {len(results['metadata_analysis']['metadata_fields'])}")
        
    except Exception as e:
        logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        print(f"\n‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {e}")

if __name__ == "__main__":
    main()