"""
================================================================
UKIDO MARKDOWN CHUNKER - –ü–†–û–°–¢–û–ô –†–ê–ë–û–ß–ò–ô "–ú–û–õ–û–¢–û–ö"
================================================================
–ü—Ä–æ—Å—Ç–æ–π, –Ω–∞–¥–µ–∂–Ω—ã–π —á–∞–Ω–∫–µ—Ä –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ Markdown —Ñ–∞–π–ª–æ–≤ —à–∫–æ–ª—ã Ukido.
–ë–µ–∑ –∏–∑–ª–∏—à–µ—Å—Ç–≤ - —Ç–æ–ª—å–∫–æ —Ç–æ —á—Ç–æ –Ω—É–∂–Ω–æ –¥–ª—è —Ä–∞–±–æ—Ç—ã.
"""

import os
import time
import google.generativeai as genai
from pinecone import Pinecone
from dotenv import load_dotenv
from typing import List, Dict
import re
import logging

# === –ü–†–û–°–¢–ê–Ø –ù–ê–°–¢–†–û–ô–ö–ê ===
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')
logger = logging.getLogger(__name__)
load_dotenv()

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
PINECONE_API_KEY = os.getenv("PINECONE_API_KEY") 
PINECONE_HOST = os.getenv("PINECONE_HOST_FACTS")

if not all([GEMINI_API_KEY, PINECONE_API_KEY, PINECONE_HOST]):
    raise ValueError("‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è")

logger.info("‚úÖ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø—Ä–æ–≤–µ—Ä–µ–Ω–∞")

class SimpleMarkdownChunker:

    """
    –ü—Ä–æ—Å—Ç–æ–π —á–∞–Ω–∫–µ—Ä –¥–ª—è —Ñ–∞–π–ª–æ–≤ Ukido - –±–µ–∑ –∏–∑–ª–∏—à–µ—Å—Ç–≤!
    """

    def __init__(self):
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ Gemini
        genai.configure(api_key=GEMINI_API_KEY)
        logger.info("üöÄ –ü—Ä–æ—Å—Ç–æ–π —á–∞–Ω–∫–µ—Ä –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ")

    def chunk_courses(self, content: str) -> List[Dict]:
        """
        –ü—Ä–æ—Å—Ç–æ–µ —á–∞–Ω–∫–æ–≤–∞–Ω–∏–µ –∫—É—Ä—Å–æ–≤ –ø–æ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è–º ---
        """
        chunks = []
        sections = content.split('\n---\n')  # –ü—Ä–æ—Å—Ç–æ–π split
        for section in sections:
            section = section.strip()
            if len(section) < 50:  # –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∞—è —Å–µ–∫—Ü–∏—è
                continue
            # –ò—â–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –∫—É—Ä—Å–∞
            course_match = re.search(r'#\s*–ö–£–†–°\s+"([^"]+)"', section)
            if course_match:
                course_name = course_match.group(1).lower().replace(' ', '_')
                logger.info(f"üìö –ù–∞–π–¥–µ–Ω –∫—É—Ä—Å: {course_match.group(1)}")
            else:
                course_name = "unknown_course"
                logger.warning("‚ö†Ô∏è –ö—É—Ä—Å –±–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è")
            chunks.append({
                "text": section,
                "type": "course_detail",
                "course": course_name
            })
        return chunks

    def chunk_teachers(self, content: str) -> List[Dict]:
        """
        –ü—Ä–æ—Å—Ç–æ–µ —á–∞–Ω–∫–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª–µ–π –ø–æ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è–º ---
        """
        chunks = []
        sections = content.split('\n---\n')  # –ü—Ä–æ—Å—Ç–æ–π split
        
        for section in sections:
            section = section.strip()
            if len(section) < 100:
                continue
                
            # –ò—â–µ–º –∏–º—è –ø—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª—è
            # –ò—â–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫, —Å–æ—Å—Ç–æ—è—â–∏–π –¢–û–õ–¨–ö–û –∏–∑ 2-3 –∑–∞–≥–ª–∞–≤–Ω—ã—Ö —Å–ª–æ–≤ (–ò–º—è –§–∞–º–∏–ª–∏—è)
            name_match = re.search(r'#\s*([–ê-–Ø–Å]+\s+[–ê-–Ø–Å]+(\s+[–ê-–Ø–Å]+)?)\s*$', section)
            if name_match:
                teacher_name = name_match.group(1)
                logger.info(f"üë®‚Äçüè´ –ù–∞–π–¥–µ–Ω –ø—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª—å: {teacher_name}")
                
                # –û—Å–Ω–æ–≤–Ω–æ–π —á–∞–Ω–∫
                chunks.append({
                    "text": section,
                    "type": "teacher_overview",
                    "teacher": teacher_name.lower().replace(' ', '_')
                })
                
                # –ú–∏–∫—Ä–æ-—á–∞–Ω–∫–∏ –¥–ª—è —Ñ–∞–∫—Ç–æ–≤
                if "–æ–ø—ã—Ç" in section.lower() and re.search(r'(\d+)', section):
                    experience_match = re.search(r'(\d+)[^\d]*–ª–µ—Ç', section, re.I)
                    if experience_match:
                        years = experience_match.group(1)
                        chunks.append({
                            "text": f"–û–ø—ã—Ç —Ä–∞–±–æ—Ç—ã {teacher_name} —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç {years} –ª–µ—Ç.",
                            "type": "teacher_experience",
                            "teacher": teacher_name.lower().replace(' ', '_')
                        })
        
        return chunks

    def chunk_standard_file(self, content: str, filename: str) -> List[Dict]:
        """
        –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ —á–∞–Ω–∫–æ–≤–∞–Ω–∏–µ –ø–æ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è–º ---
        """
        chunks = []
        sections = content.split('\n---\n')
        for section in sections:
            section = section.strip()
            chunks.append({
                "text": section,
                "type": filename.replace('.md', '').replace('.txt', '')
            })
        return chunks

    def process_files(self, directory: str) -> List[Dict]:
        """
        –ü—Ä–æ—Å—Ç–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤
        """
        all_chunks = []
        chunk_id = 0
        
        # –ò—â–µ–º —Ñ–∞–π–ª—ã .md –∏ .txt
        files = [f for f in os.listdir(directory) if f.endswith(('.md', '.txt'))]
        logger.info(f"üìÅ –ù–∞–π–¥–µ–Ω–æ {len(files)} —Ñ–∞–π–ª–æ–≤")
        
        for filename in files:
            logger.info(f"üìÑ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é: {filename}")
            
            with open(os.path.join(directory, filename), 'r', encoding='utf-8') as f:
                content = f.read()

            # –í—ã–±–∏—Ä–∞–µ–º –º–µ—Ç–æ–¥ —á–∞–Ω–∫–æ–≤–∞–Ω–∏—è
            if 'courses' in filename:
                chunks = self.chunk_courses(content)
            elif 'teachers' in filename:
                chunks = self.chunk_teachers(content)
            else:
                # –í–°–ï –û–°–¢–ê–õ–¨–ù–´–ï –§–ê–ô–õ–´ (pricing, conditions, faq –∏ —Ç.–¥.)
                # –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º –º–µ—Ç–æ–¥–æ–º –ø–æ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—é '---'
                chunks = self.chunk_standard_file(content, filename)
            
            # –î–æ–±–∞–≤–ª—è–µ–º ID –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            for chunk in chunks:
                chunk_id += 1
                all_chunks.append({
                    "id": f"ukido-{chunk_id}",
                    "text": chunk["text"],
                    "metadata": {
                        "source": filename,
                        "type": chunk["type"],
                        **{k: v for k, v in chunk.items() if k not in ["text", "type"]}
                    }
                })
        
        logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(all_chunks)} —á–∞–Ω–∫–æ–≤")
        return all_chunks

    def vectorize_and_upload(self, chunks: List[Dict]) -> bool:
        """
        –ü—Ä–æ—Å—Ç–∞—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –∏ –∑–∞–≥—Ä—É–∑–∫–∞ –≤ Pinecone
        """
        if not chunks:
            logger.error("‚ùå –ù–µ—Ç —á–∞–Ω–∫–æ–≤ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏")
            return False
        
        logger.info(f"üîÑ –í–µ–∫—Ç–æ—Ä–∏–∑—É—é {len(chunks)} —á–∞–Ω–∫–æ–≤...")
        
        vectors = []
        for i, chunk in enumerate(chunks):
            try:
                # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥
                response = genai.embed_content(
                    model='models/text-embedding-004',
                    content=chunk['text'],
                    task_type="RETRIEVAL_DOCUMENT"
                )
                
                vectors.append({
                    "id": chunk['id'],
                    "values": response['embedding'],
                    "metadata": {
                        "text": chunk['text'][:500],  # –ö–æ—Ä–æ—Ç–∫–∏–π –æ—Ç—Ä—ã–≤–æ–∫
                        **chunk['metadata']
                    }
                })
                
                if (i + 1) % 5 == 0:
                    logger.info(f"  üìä {i + 1}/{len(chunks)} –≥–æ—Ç–æ–≤–æ")
                    time.sleep(0.5)  # –ü–∞—É–∑–∞ –¥–ª—è API
                    
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ {chunk['id']}: {e}")
                continue
        
        if not vectors:
            logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –Ω–∏ –æ–¥–Ω–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–∞")
            return False
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤ Pinecone
        logger.info("üîå –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Pinecone...")
        try:
            pc = Pinecone(api_key=PINECONE_API_KEY)
            index = pc.Index(host=PINECONE_HOST)
            
            # –û—á–∏—â–∞–µ–º —Å—Ç–∞—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ
            logger.info("üßπ –û—á–∏—Å—Ç–∫–∞ –±–∞–∑—ã...")
            index.delete(delete_all=True)
            time.sleep(5)
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
            logger.info(f"üì§ –ó–∞–≥—Ä—É–∂–∞—é {len(vectors)} –≤–µ–∫—Ç–æ—Ä–æ–≤...")
            batch_size = 50
            
            for i in range(0, len(vectors), batch_size):
                batch = vectors[i:i + batch_size]
                index.upsert(vectors=batch)
                logger.info(f"  üì¶ –ë–∞—Ç—á {i//batch_size + 1} –∑–∞–≥—Ä—É–∂–µ–Ω")
                time.sleep(1)
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –±–æ–ª—å—à–µ –Ω–µ –Ω—É–∂–Ω–∞, —Ç.–∫. upsert –≤—ã–¥–∞–ª –±—ã –æ—à–∏–±–∫—É.
            # –ü—Ä–æ—Å—Ç–æ —Å–æ–æ–±—â–∞–µ–º –æ–± —É—Å–ø–µ—Ö–µ.
            stats = index.describe_index_stats()
            final_count = stats.get('total_vector_count', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ (—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±–Ω–æ–≤–ª—è–µ—Ç—Å—è)')
            logger.info(f"üéâ –ö–æ–º–∞–Ω–¥–∞ –Ω–∞ –∑–∞–≥—Ä—É–∑–∫—É {len(vectors)} –≤–µ–∫—Ç–æ—Ä–æ–≤ —É—Å–ø–µ—à–Ω–æ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–∞! –§–∏–Ω–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤ –±–∞–∑–µ: {final_count}")

            return True # –°—á–∏—Ç–∞–µ–º —É—Å–ø–µ—à–Ω—ã–º, –µ—Å–ª–∏ –Ω–µ –±—ã–ª–æ –∏—Å–∫–ª—é—á–µ–Ω–∏–π
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ Pinecone: {e}")
            return False

def main():
    """
    –ü—Ä–æ—Å—Ç–æ–π –∑–∞–ø—É—Å–∫ —á–∞–Ω–∫–µ—Ä–∞
    """
    logger.info("üöÄ UKIDO CHUNKER - –ü–†–û–°–¢–û–ô –ú–û–õ–û–¢–û–ö")
    logger.info("=" * 50)
    
    try:
        chunker = SimpleMarkdownChunker()
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ñ–∞–π–ª—ã
        chunks = chunker.process_files("data_facts")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤ Pinecone
        success = chunker.vectorize_and_upload(chunks)
        
        if success:
            logger.info("üéâ –ì–û–¢–û–í–û! –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –æ–±–Ω–æ–≤–ª–µ–Ω–∞!")
        else:
            logger.error("‚ùå –ß—Ç–æ-—Ç–æ –ø–æ—à–ª–æ –Ω–µ —Ç–∞–∫")
            
    except Exception as e:
        logger.error(f"üí• –û—à–∏–±–∫–∞: {e}")

if __name__ == "__main__":
    main()