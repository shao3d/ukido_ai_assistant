import os
import time
import google.generativeai as genai
from pinecone import Pinecone
from dotenv import load_dotenv
import re
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass
from datetime import datetime
import pdfplumber  # –ù–æ–≤–∞—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å PDF

# --- –ù–ê–°–¢–†–û–ô–ö–ò –ò –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ---
load_dotenv()

GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")
PINECONE_HOST_STYLE = os.getenv("PINECONE_HOST_STYLE")

@dataclass
class StyleChunkingConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Ç–∏–ª–µ–≤—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ –ñ–≤–∞–Ω–µ—Ü–∫–æ–≥–æ"""
    
    # –†–∞–∑–º–µ—Ä—ã —á–∞–Ω–∫–æ–≤ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω—ã –ø–æ–¥ —Å—Ç–∏–ª–µ–≤–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç
    min_chunk_size: int = 300         # –ö–æ—Ä–æ—á–µ, —á–µ–º —Å–ø—Ä–∞–≤–æ—á–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã
    ideal_chunk_size: int = 800       # –û–ø—Ç–∏–º—É–º –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å—Ç–∏–ª—è
    max_chunk_size: int = 1500        # –ú–∞–∫—Å–∏–º—É–º –¥–ª—è —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π
    
    # –†–∞–∑–º–µ—Ä—ã –¥–ª—è –∞—Ñ–æ—Ä–∏–∑–º–æ–≤ –∏ –∫–æ—Ä–æ—Ç–∫–∏—Ö –º—ã—Å–ª–µ–π
    aphorism_min_size: int = 50       # –ú–∏–Ω–∏–º—É–º –¥–ª—è –∞—Ñ–æ—Ä–∏–∑–º–∞
    aphorism_max_size: int = 200      # –ú–∞–∫—Å–∏–º—É–º –¥–ª—è –∞—Ñ–æ—Ä–∏–∑–º–∞
    
    # API –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
    api_requests_per_minute: int = 1500
    safety_margin: float = 0.8
    min_delay_between_requests: float = 0.1
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ (–ù–û–í–û–ï)
    enable_content_filtering: bool = True  # –í–∫–ª—é—á–∏—Ç—å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é –¥–ª—è PDF
    filter_confidence_threshold: float = 0.7  # –ü–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è –ø—Ä–∏–Ω—è—Ç–∏—è —á–∞–Ω–∫–∞
    
    # –°–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –ñ–≤–∞–Ω–µ—Ü–∫–æ–≥–æ
    preserve_rhythm: bool = True       # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–∏—Ç–º —Ç–µ–∫—Å—Ç–∞
    respect_pauses: bool = True        # –£–≤–∞–∂–∞–µ–º –∞–≤—Ç–æ—Ä—Å–∫–∏–µ –ø–∞—É–∑—ã
    keep_dialogue_intact: bool = True  # –ù–µ —Ä–∞–∑—Ä—ã–≤–∞–µ–º –¥–∏–∞–ª–æ–≥–∏
    
    @property
    def calculated_delay(self) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –∑–∞–¥–µ—Ä–∂–∫—É –º–µ–∂–¥—É API –∑–∞–ø—Ä–æ—Å–∞–º–∏"""
        max_safe_requests_per_minute = self.api_requests_per_minute * self.safety_margin
        optimal_delay = 60.0 / max_safe_requests_per_minute
        return max(optimal_delay, self.min_delay_between_requests)

class SmartRetryHandler:
    """
    –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –ø–æ–ø—ã—Ç–æ–∫ –¥–ª—è API –≤—ã–∑–æ–≤–æ–≤.
    
    –≠—Ç–∞ —Å–∏—Å—Ç–µ–º–∞ —Ä–µ–∞–ª–∏–∑—É–µ—Ç best practices –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ —Å–±–æ—è–º–∏ API,
    –≤–∫–ª—é—á–∞—è –æ–±—Ä–∞–±–æ—Ç–∫—É –æ—à–∏–±–æ–∫ rate limiting, —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –∑–∞–¥–µ—Ä–∂–∫–∏ –∏
    —É–≤–∞–∂–µ–Ω–∏–µ –∫ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º —Å–µ—Ä–≤–µ—Ä–∞ –æ –≤—Ä–µ–º–µ–Ω–∏ –æ–∂–∏–¥–∞–Ω–∏—è.
    
    –ü—Ä–∏–Ω—Ü–∏–ø—ã —Ä–∞–±–æ—Ç—ã:
    1. –†–∞—Å–ø–æ–∑–Ω–∞–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ç–∏–ø—ã –æ—à–∏–±–æ–∫ –∏ –ø—Ä–∏–º–µ–Ω—è–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
    2. –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –∑–∞–¥–µ—Ä–∂–∫–∏ –∏–∑ –æ—Ç–≤–µ—Ç–æ–≤ API
    3. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –∑–∞–¥–µ—Ä–∂–∫–∏ –∫–∞–∫ fallback —Å—Ç—Ä–∞—Ç–µ–≥–∏—é
    4. –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã—Ö —Ü–∏–∫–ª–æ–≤
    5. –ü—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –ø–æ–¥—Ä–æ–±–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –∏ –æ—Ç–ª–∞–¥–∫–∏
    """
    
    def __init__(self, max_retries: int = 5, base_delay: float = 1.0):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –ø–æ–ø—ã—Ç–æ–∫.
        
        Args:
            max_retries: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –ø–æ–ø—ã—Ç–æ–∫
            base_delay: –ë–∞–∑–æ–≤–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö –¥–ª—è —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–≥–æ backoff
        """
        self.max_retries = max_retries
        self.base_delay = base_delay
        self.retry_stats = {
            "total_retries": 0,
            "successful_retries": 0,
            "failed_operations": 0,
            "rate_limit_hits": 0
        }
        
        print(f"üîÑ Smart Retry Handler –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω:")
        print(f"   –ú–∞–∫—Å–∏–º—É–º –ø–æ–ø—ã—Ç–æ–∫: {max_retries}")
        print(f"   –ë–∞–∑–æ–≤–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞: {base_delay}—Å")
    
    def extract_retry_delay_from_error(self, error_message: str) -> Optional[float]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º—É—é –∑–∞–¥–µ—Ä–∂–∫—É –∏–∑ —Å–æ–æ–±—â–µ–Ω–∏—è –æ–± –æ—à–∏–±–∫–µ Google API.
        
        Google API –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç structured –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –æ—à–∏–±–∫–∞—Ö, –≤–∫–ª—é—á–∞—è
        —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º–æ–µ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è –≤ –ø–æ–ª–µ retry_delay. –≠—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è
        –ø–∞—Ä—Å–∏—Ç —Å–æ–æ–±—â–µ–Ω–∏–µ –æ–± –æ—à–∏–±–∫–µ –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç —ç—Ç–æ –∑–Ω–∞—á–µ–Ω–∏–µ.
        
        Args:
            error_message: –°—Ç—Ä–æ–∫–∞ —Å —Å–æ–æ–±—â–µ–Ω–∏–µ–º –æ–± –æ—à–∏–±–∫–µ –æ—Ç API
            
        Returns:
            –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö –∏–ª–∏ None –µ—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞
        """
        import re
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –ø–æ–∏—Å–∫–∞ retry_delay –≤ structured –æ—à–∏–±–∫–µ Google API
        # –§–æ—Ä–º–∞—Ç: retry_delay { seconds: X }
        delay_pattern = r'retry_delay\s*{\s*seconds:\s*(\d+)'
        
        match = re.search(delay_pattern, error_message)
        if match:
            return float(match.group(1))
        
        # Fallback: –ø–æ–∏—Å–∫ –æ–±—â–∏—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –≤—Ä–µ–º–µ–Ω–∏ –æ–∂–∏–¥–∞–Ω–∏—è
        # –ù–µ–∫–æ—Ç–æ—Ä—ã–µ API –º–æ–≥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥—Ä—É–≥–∏–µ —Ñ–æ—Ä–º–∞—Ç—ã
        general_patterns = [
            r'retry after (\d+) seconds?',
            r'try again in (\d+) seconds?',
            r'wait (\d+) seconds?'
        ]
        
        for pattern in general_patterns:
            match = re.search(pattern, error_message.lower())
            if match:
                return float(match.group(1))
        
        return None
    
    def calculate_exponential_backoff(self, attempt: int) -> float:
        """
        –í—ã—á–∏—Å–ª—è–µ—Ç —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω—É—é –∑–∞–¥–µ—Ä–∂–∫—É –¥–ª—è –ø–æ–ø—ã—Ç–∫–∏.
        
        –≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–π backoff ‚Äî —ç—Ç–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è, –ø—Ä–∏ –∫–æ—Ç–æ—Ä–æ–π –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è
        –º–µ–∂–¥—É –ø–æ–ø—ã—Ç–∫–∞–º–∏ —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç—Å—è —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ. –≠—Ç–æ –ø–æ–º–æ–≥–∞–µ—Ç –∏–∑–±–µ–∂–∞—Ç—å
        "thundering herd" –ø—Ä–æ–±–ª–µ–º—ã, –∫–æ–≥–¥–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–æ –∫–ª–∏–µ–Ω—Ç–æ–≤ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ
        –ø–æ–≤—Ç–æ—Ä—è—é—Ç –∑–∞–ø—Ä–æ—Å—ã –ø–æ—Å–ª–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Å–±–æ—è.
        
        –§–æ—Ä–º—É–ª–∞: base_delay * (2 ^ (attempt - 1)) + jitter
        Jitter –¥–æ–±–∞–≤–ª—è–µ—Ç—Å—è –¥–ª—è —Ä–∞–Ω–¥–æ–º–∏–∑–∞—Ü–∏–∏ –∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –Ω–∞–≥—Ä—É–∑–∫–∏.
        
        Args:
            attempt: –ù–æ–º–µ—Ä —Ç–µ–∫—É—â–µ–π –ø–æ–ø—ã—Ç–∫–∏ (1-based)
            
        Returns:
            –ó–∞–¥–µ—Ä–∂–∫–∞ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
        """
        import random
        
        # –û—Å–Ω–æ–≤–Ω–∞—è —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞
        exponential_delay = self.base_delay * (2 ** (attempt - 1))
        
        # –î–æ–±–∞–≤–ª—è–µ–º jitter (—Å–ª—É—á–∞–π–Ω–æ—Å—Ç—å) –¥–ª—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –Ω–∞–≥—Ä—É–∑–∫–∏
        # Jitter —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç ¬±25% –æ—Ç –æ—Å–Ω–æ–≤–Ω–æ–π –∑–∞–¥–µ—Ä–∂–∫–∏
        jitter_range = exponential_delay * 0.25
        jitter = random.uniform(-jitter_range, jitter_range)
        
        total_delay = exponential_delay + jitter
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –∑–∞–¥–µ—Ä–∂–∫—É —Ä–∞–∑—É–º–Ω—ã–º –∑–Ω–∞—á–µ–Ω–∏–µ–º
        max_delay = 300  # 5 –º–∏–Ω—É—Ç –º–∞–∫—Å–∏–º—É–º
        return min(total_delay, max_delay)
    
    def retry_api_call(self, api_function, *args, **kwargs):
        """
        –í—ã–ø–æ–ª–Ω—è–µ—Ç API –≤—ã–∑–æ–≤ —Å –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–π —Å–∏—Å—Ç–µ–º–æ–π –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –ø–æ–ø—ã—Ç–æ–∫.
        
        –≠—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è —è–≤–ª—è–µ—Ç—Å—è —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–º –Ω–∞—à–µ–π retry —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏.
        –û–Ω–∞ wraps –ª—é–±—É—é API —Ñ—É–Ω–∫—Ü–∏—é –∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤—Ä–µ–º–µ–Ω–Ω—ã–µ
        —Å–±–æ–∏, –≤–∫–ª—é—á–∞—è rate limiting, —Å–µ—Ç–µ–≤—ã–µ –æ—à–∏–±–∫–∏ –∏ –≤—Ä–µ–º–µ–Ω–Ω—É—é –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å.
        
        –ê–ª–≥–æ—Ä–∏—Ç–º —Ä–∞–±–æ—Ç—ã:
        1. –ü—ã—Ç–∞–µ—Ç—Å—è –≤—ã–ø–æ–ª–Ω–∏—Ç—å API –≤—ã–∑–æ–≤
        2. –ü—Ä–∏ —É—Å–ø–µ—Ö–µ ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        3. –ü—Ä–∏ –æ—à–∏–±–∫–µ ‚Äî –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–∏–ø –æ—à–∏–±–∫–∏
        4. –î–ª—è recoverable –æ—à–∏–±–æ–∫ ‚Äî –∂–¥–µ—Ç –∏ –ø–æ–≤—Ç–æ—Ä—è–µ—Ç –ø–æ–ø—ã—Ç–∫—É
        5. –î–ª—è fatal –æ—à–∏–±–æ–∫ ‚Äî –ø—Ä–æ–∫–∏–¥—ã–≤–∞–µ—Ç –∏—Å–∫–ª—é—á–µ–Ω–∏–µ –¥–∞–ª—å—à–µ
        
        Args:
            api_function: –§—É–Ω–∫—Ü–∏—è API –¥–ª—è –≤—ã–∑–æ–≤–∞
            *args, **kwargs: –ê—Ä–≥—É–º–µ–Ω—Ç—ã –¥–ª—è –ø–µ—Ä–µ–¥–∞—á–∏ –≤ API —Ñ—É–Ω–∫—Ü–∏—é
            
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç —É—Å–ø–µ—à–Ω–æ–≥–æ API –≤—ã–∑–æ–≤–∞
            
        Raises:
            Exception: –ï—Å–ª–∏ –≤—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –∏—Å—á–µ—Ä–ø–∞–Ω—ã –∏–ª–∏ –ø—Ä–æ–∏–∑–æ—à–ª–∞ fatal –æ—à–∏–±–∫–∞
        """
        last_exception = None
        
        for attempt in range(1, self.max_retries + 1):
            try:
                # –ü—ã—Ç–∞–µ–º—Å—è –≤—ã–ø–æ–ª–Ω–∏—Ç—å API –≤—ã–∑–æ–≤
                result = api_function(*args, **kwargs)
                
                # –ï—Å–ª–∏ –º—ã –¥–æ–±—Ä–∞–ª–∏—Å—å –¥–æ —ç—Ç–æ–π —Å—Ç—Ä–æ–∫–∏, –≤—ã–∑–æ–≤ —É—Å–ø–µ—à–µ–Ω
                if attempt > 1:
                    # –≠—Ç–æ –±—ã–ª–∞ –ø–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞, –æ–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
                    self.retry_stats["successful_retries"] += 1
                    print(f"      ‚úÖ –£—Å–ø–µ—à–Ω–∞—è –ø–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ #{attempt}")
                
                return result
                
            except Exception as e:
                last_exception = e
                error_message = str(e)
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ recoverable –æ—à–∏–±–∫–æ–π
                if "429" in error_message or "quota" in error_message.lower():
                    # –≠—Ç–æ –æ—à–∏–±–∫–∞ rate limiting ‚Äî –º—ã –º–æ–∂–µ–º —Å –Ω–µ–π —Å–ø—Ä–∞–≤–∏—Ç—å—Å—è
                    self.retry_stats["rate_limit_hits"] += 1
                    
                    if attempt == self.max_retries:
                        # –ò—Å—á–µ—Ä–ø–∞–Ω—ã –≤—Å–µ –ø–æ–ø—ã—Ç–∫–∏
                        print(f"      ‚ùå –ò—Å—á–µ—Ä–ø–∞–Ω—ã –≤—Å–µ {self.max_retries} –ø–æ–ø—ã—Ç–æ–∫ –¥–ª—è rate limiting")
                        break
                    
                    # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º—É—é –∑–∞–¥–µ—Ä–∂–∫—É –∏–∑ –æ—à–∏–±–∫–∏
                    recommended_delay = self.extract_retry_delay_from_error(error_message)
                    
                    if recommended_delay:
                        # API –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–∏–ª —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—é ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ—ë
                        wait_time = recommended_delay
                        delay_source = f"API —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: {recommended_delay}—Å"
                    else:
                        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–π backoff –∫–∞–∫ fallback
                        wait_time = self.calculate_exponential_backoff(attempt)
                        delay_source = f"—ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–π backoff: {wait_time:.1f}—Å"
                    
                    print(f"      ‚è≥ Rate limit –¥–æ—Å—Ç–∏–≥–Ω—É—Ç. –ü–æ–ø—ã—Ç–∫–∞ {attempt}/{self.max_retries}")
                    print(f"         –û–∂–∏–¥–∞–Ω–∏–µ {delay_source}")
                    
                    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –æ–∂–∏–¥–∞–Ω–∏—è –¥–ª—è –¥–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞–¥–µ—Ä–∂–µ–∫
                    if wait_time > 10:
                        self._show_countdown(wait_time)
                    else:
                        time.sleep(wait_time)
                    
                    self.retry_stats["total_retries"] += 1
                    continue
                    
                elif "5" in error_message[:3]:  # 5xx —Å–µ—Ä–≤–µ—Ä–Ω—ã–µ –æ—à–∏–±–∫–∏
                    # –í—Ä–µ–º–µ–Ω–Ω—ã–µ —Å–µ—Ä–≤–µ—Ä–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã ‚Äî —Å—Ç–æ–∏—Ç –ø–æ–≤—Ç–æ—Ä–∏—Ç—å
                    if attempt == self.max_retries:
                        print(f"      ‚ùå –ò—Å—á–µ—Ä–ø–∞–Ω—ã –≤—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –¥–ª—è —Å–µ—Ä–≤–µ—Ä–Ω–æ–π –æ—à–∏–±–∫–∏")
                        break
                    
                    wait_time = self.calculate_exponential_backoff(attempt)
                    print(f"      ‚ö†Ô∏è –°–µ—Ä–≤–µ—Ä–Ω–∞—è –æ—à–∏–±–∫–∞. –ü–æ–ø—ã—Ç–∫–∞ {attempt}/{self.max_retries}")
                    print(f"         –û–∂–∏–¥–∞–Ω–∏–µ {wait_time:.1f}—Å")
                    
                    time.sleep(wait_time)
                    self.retry_stats["total_retries"] += 1
                    continue
                    
                else:
                    # –≠—Ç–æ —Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ fatal –æ—à–∏–±–∫–∞ (400, –ø—Ä–æ–±–ª–µ–º—ã —Å –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–µ–π, etc.)
                    # –ü–æ–≤—Ç–æ—Ä–Ω—ã–µ –ø–æ–ø—ã—Ç–∫–∏ –Ω–µ –ø–æ–º–æ–≥—É—Ç
                    print(f"      üíÄ Fatal –æ—à–∏–±–∫–∞ API (–Ω–µ recoverable): {error_message[:100]}...")
                    raise e
        
        # –ï—Å–ª–∏ –º—ã –¥–æ–±—Ä–∞–ª–∏—Å—å —Å—é–¥–∞, –≤—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –∏—Å—á–µ—Ä–ø–∞–Ω—ã
        self.retry_stats["failed_operations"] += 1
        print(f"      ‚ùå –û–ø–µ—Ä–∞—Ü–∏—è –ø—Ä–æ–≤–∞–ª–µ–Ω–∞ –ø–æ—Å–ª–µ {self.max_retries} –ø–æ–ø—ã—Ç–æ–∫")
        raise last_exception
    
    def _show_countdown(self, wait_time: float):
        """
        –ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç countdown –¥–ª—è –¥–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –æ–∂–∏–¥–∞–Ω–∏–π.
        
        –ö–æ–≥–¥–∞ API —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç –∂–¥–∞—Ç—å –±–æ–ª–µ–µ 10 —Å–µ–∫—É–Ω–¥, –ø–æ–ª–µ–∑–Ω–æ –ø–æ–∫–∞–∑–∞—Ç—å
        –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é –ø—Ä–æ–≥—Ä–µ—Å—Å –æ–∂–∏–¥–∞–Ω–∏—è. –≠—Ç–æ —É–ª—É—á—à–∞–µ—Ç user experience
        –∏ –ø–æ–º–æ–≥–∞–µ—Ç –ø–æ–Ω—è—Ç—å, —á—Ç–æ —Å–∏—Å—Ç–µ–º–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç, –∞ –Ω–µ –∑–∞–≤–∏—Å–ª–∞.
        
        Args:
            wait_time: –í—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
        """
        import sys
        
        print(f"         Countdown: ", end="", flush=True)
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º countdown –ø–æ —Å–µ–∫—É–Ω–¥–∞–º –¥–ª—è –ø–µ—Ä–≤—ã—Ö 10 —Å–µ–∫—É–Ω–¥
        initial_countdown = min(10, int(wait_time))
        for i in range(initial_countdown, 0, -1):
            print(f"{i}...", end="", flush=True)
            time.sleep(1)
            wait_time -= 1
        
        # –ï—Å–ª–∏ –æ—Å—Ç–∞–ª–æ—Å—å –±–æ–ª—å—à–µ –≤—Ä–µ–º–µ–Ω–∏, –∂–¥–µ–º –æ—Å—Ç–∞–≤—à–µ–µ—Å—è –≤—Ä–µ–º—è –º–æ–ª—á–∞
        if wait_time > 0:
            print(f"(+{wait_time:.0f}—Å)", end="", flush=True)
            time.sleep(wait_time)
        
        print(" ‚úì", flush=True)
    
    def get_retry_statistics(self) -> Dict:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ä–∞–±–æ—Ç—ã retry —Å–∏—Å—Ç–µ–º—ã.
        
        –≠—Ç–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø–æ–ª–µ–∑–Ω–∞ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Å–∏—Å—Ç–µ–º—ã.
        –ú—ã –º–æ–∂–µ–º –æ—Ç—Å–ª–µ–∂–∏–≤–∞—Ç—å, –∫–∞–∫ —á–∞—Å—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥—è—Ç retry, –∫–∞–∫–∏–µ —Ç–∏–ø—ã
        –æ—à–∏–±–æ–∫ –Ω–∞–∏–±–æ–ª–µ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω—ã, –∏ –Ω–∞—Å–∫–æ–ª—å–∫–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞ –Ω–∞—à–∞ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è.
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π retry –æ–ø–µ—Ä–∞—Ü–∏–π
        """
        return self.retry_stats.copy()


class ContentRelevanceFilter:
    """
    –°–∏—Å—Ç–µ–º–∞ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫ API.
    –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –ø–æ–¥—Ö–æ–¥–∏—Ç –ª–∏ —á–∞–Ω–∫ –ñ–≤–∞–Ω–µ—Ü–∫–æ–≥–æ –¥–ª—è —à–∫–æ–ª—ã soft skills.
    
    –û–ë–ù–û–í–õ–ï–ù–ò–ï: –¢–µ–ø–µ—Ä—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç SmartRetryHandler –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ rate limiting
    –∏ –¥—Ä—É–≥–∏—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º —Å API.
    """
    
    def __init__(self, config: StyleChunkingConfig):
        self.config = config
        self.generation_model = genai.GenerativeModel('gemini-1.5-flash')
        
        # –ù–û–í–û–ï: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —É–º–Ω—É—é —Å–∏—Å—Ç–µ–º—É –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –ø–æ–ø—ã—Ç–æ–∫
        self.retry_handler = SmartRetryHandler(
            max_retries=5,      # –î–æ 5 –ø–æ–ø—ã—Ç–æ–∫ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ API –≤—ã–∑–æ–≤–∞
            base_delay=2.0      # –ù–∞—á–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ 2 —Å–µ–∫—É–Ω–¥—ã
        )
        
        # –ü—Ä–æ–º–ø—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        self.relevance_prompt = """
–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω–æ–º—É –∫–æ–Ω—Ç–µ–Ω—Ç—É –¥–ª—è –æ–Ω–ª–∞–π–Ω-—à–∫–æ–ª—ã —Ä–∞–∑–≤–∏—Ç–∏—è soft skills —É –¥–µ—Ç–µ–π.

–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —ç—Ç–æ—Ç —Ñ—Ä–∞–≥–º–µ–Ω—Ç —Ç–µ–∫—Å—Ç–∞ –ú–∏—Ö–∞–∏–ª–∞ –ñ–≤–∞–Ω–µ—Ü–∫–æ–≥–æ –∏ –æ—Ü–µ–Ω–∏, –ø–æ–¥—Ö–æ–¥–∏—Ç –ª–∏ –æ–Ω –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–º, –∫–æ—Ç–æ—Ä—ã–π –∫–æ–Ω—Å—É–ª—å—Ç–∏—Ä—É–µ—Ç —Ä–æ–¥–∏—Ç–µ–ª–µ–π –ø–æ –≤–æ–ø—Ä–æ—Å–∞–º –≤–æ—Å–ø–∏—Ç–∞–Ω–∏—è –∏ —Ä–∞–∑–≤–∏—Ç–∏—è –¥–µ—Ç–µ–π.

–ö–†–ò–¢–ï–†–ò–ò –ü–û–î–•–û–î–Ø–©–ï–ì–û –ö–û–ù–¢–ï–ù–¢–ê:
- –ú—É–¥—Ä—ã–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è –æ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–π –ø—Ä–∏—Ä–æ–¥–µ
- –†–∞–∑–º—ã—à–ª–µ–Ω–∏—è –æ –≤–æ—Å–ø–∏—Ç–∞–Ω–∏–∏, –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–∏, –æ—Ç–Ω–æ—à–µ–Ω–∏—è—Ö
- –Æ–º–æ—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ, –Ω–æ –ø–æ—É—á–∏—Ç–µ–ª—å–Ω—ã–µ –∏—Å—Ç–æ—Ä–∏–∏
- –ê—Ñ–æ—Ä–∏–∑–º—ã –æ –∂–∏–∑–Ω–∏, –æ–±—â–µ–Ω–∏–∏, —Ä–∞–∑–≤–∏—Ç–∏–∏ –ª–∏—á–Ω–æ—Å—Ç–∏
- –ú–∞—Ç–µ—Ä–∏–∞–ª, –∫–æ—Ç–æ—Ä—ã–π –º–æ–∂–µ—Ç –ø–æ–º–æ—á—å —Ä–æ–¥–∏—Ç–µ–ª—è–º –ª—É—á—à–µ –ø–æ–Ω–∏–º–∞—Ç—å –¥–µ—Ç–µ–π

–ö–†–ò–¢–ï–†–ò–ò –ù–ï–ü–û–î–•–û–î–Ø–©–ï–ì–û –ö–û–ù–¢–ï–ù–¢–ê:
- –ü–æ–ª–∏—Ç–∏—á–µ—Å–∫–∞—è —Å–∞—Ç–∏—Ä–∞ –∏–ª–∏ –∫—Ä–∏—Ç–∏–∫–∞ –≤–ª–∞—Å—Ç–∏
- –°–ª–∏—à–∫–æ–º —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Å–æ–≤–µ—Ç—Å–∫–∏–µ —Ä–µ–∞–ª–∏–∏
- –ì—Ä—É–±—ã–π –∏–ª–∏ –Ω–µ–ø–æ–¥—Ö–æ–¥—è—â–∏–π –¥–ª—è —Å–µ–º–µ–π–Ω–æ–π –∞—É–¥–∏—Ç–æ—Ä–∏–∏ —é–º–æ—Ä
- –ú–∞—Ç–µ—Ä–∏–∞–ª –±–µ–∑ –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω–æ–π —Ü–µ–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è —Ä–æ–¥–∏—Ç–µ–ª–µ–π
- –°–ª–∏—à–∫–æ–º —É–∑–∫–æ—Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–µ–º—ã

–§—Ä–∞–≥–º–µ–Ω—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞:
{text}

–û—Ç–≤–µ—Ç—å –¢–û–õ–¨–ö–û –æ–¥–Ω–∏–º —Å–ª–æ–≤–æ–º:
- "–ü–û–î–•–û–î–ò–¢" - –µ—Å–ª–∏ –º–∞—Ç–µ—Ä–∏–∞–ª —É–º–µ—Å—Ç–µ–Ω –¥–ª—è —à–∫–æ–ª—ã soft skills
- "–ù–ï_–ü–û–î–•–û–î–ò–¢" - –µ—Å–ª–∏ –º–∞—Ç–µ—Ä–∏–∞–ª –Ω–µ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –Ω–∞—à–µ–π –∞—É–¥–∏—Ç–æ—Ä–∏–∏

–¢–≤–æ–π –æ—Ç–≤–µ—Ç:"""
    
    def _make_api_call_for_filtering(self, chunk: str) -> str:
        """
        –í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è API –≤—ã–∑–æ–≤–∞ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏.
        
        –≠—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è –∏–∑–æ–ª–∏—Ä—É–µ—Ç –ª–æ–≥–∏–∫—É API –≤—ã–∑–æ–≤–∞ –æ—Ç retry –ª–æ–≥–∏–∫–∏,
        —á—Ç–æ –¥–µ–ª–∞–µ—Ç –∫–æ–¥ –±–æ–ª–µ–µ –º–æ–¥—É–ª—å–Ω—ã–º –∏ —Ç–µ—Å—Ç–∏—Ä—É–µ–º—ã–º. SmartRetryHandler
        –±—É–¥–µ—Ç –≤—ã–∑—ã–≤–∞—Ç—å —ç—Ç—É —Ñ—É–Ω–∫—Ü–∏—é –∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –æ—à–∏–±–∫–∏.
        
        Args:
            chunk: –¢–µ–∫—Å—Ç–æ–≤—ã–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            
        Returns:
            –û—Ç–≤–µ—Ç –æ—Ç Gemini API
            
        Raises:
            Exception: –ü—Ä–∏ –ª—é–±—ã—Ö –ø—Ä–æ–±–ª–µ–º–∞—Ö —Å API
        """
        # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–º–ø—Ç —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º —á–∞–Ω–∫–æ–º
        full_prompt = self.relevance_prompt.format(text=chunk[:1000])  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –¥–ª—è API
        
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ Gemini (–±–µ–∑ retry –ª–æ–≥–∏–∫–∏ - –µ—ë –æ–±—Ä–∞–±–æ—Ç–∞–µ—Ç wrapper)
        response = self.generation_model.generate_content(full_prompt)
        return response.text.strip().upper()
    
    def evaluate_chunk_relevance(self, chunk: str) -> Tuple[bool, str]:
        """
        –û—Ü–µ–Ω–∏–≤–∞–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å —á–∞–Ω–∫–∞ –¥–ª—è —à–∫–æ–ª—ã soft skills.
        
        –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –£–±—Ä–∞–Ω–∞ –¥—É–±–ª–∏—Ä—É—é—â–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫, –∫–æ—Ç–æ—Ä–∞—è –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤–∞–ª–∞
        —Å SmartRetryHandler. –¢–µ–ø–µ—Ä—å —Ñ—É–Ω–∫—Ü–∏—è –ø–æ–ª–Ω–æ—Å—Ç—å—é –ø–æ–ª–∞–≥–∞–µ—Ç—Å—è –Ω–∞ retry handler
        –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—Å–µ—Ö —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ–±–ª–µ–º —Å API, –≤–∫–ª—é—á–∞—è rate limiting.
        
        –≠—Ç–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å–ª–µ–¥—É–µ—Ç –ø—Ä–∏–Ω—Ü–∏–ø—É "–µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–æ–π –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏" - 
        —Ñ—É–Ω–∫—Ü–∏—è —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –Ω–∞ –±–∏–∑–Ω–µ—Å-–ª–æ–≥–∏–∫–µ –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞,
        –∞ –≤—Å–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∞—Å–ø–µ–∫—Ç—ã –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è –Ω–∞ —É—Ä–æ–≤–Ω–µ middleware.
        
        Args:
            chunk: –¢–µ–∫—Å—Ç–æ–≤—ã–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            
        Returns:
            Tuple (–ø–æ–¥—Ö–æ–¥–∏—Ç –ª–∏ —á–∞–Ω–∫, –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ —Ä–µ—à–µ–Ω–∏—è)
        """
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º retry handler –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ–≥–æ API –≤—ã–∑–æ–≤–∞
        # –í—Å–µ –æ—à–∏–±–∫–∏ rate limiting, –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Å–±–æ–∏ –∏ recoverable –ø—Ä–æ–±–ª–µ–º—ã
        # –±—É–¥—É—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã SmartRetryHandler
        ai_decision = self.retry_handler.retry_api_call(
            self._make_api_call_for_filtering, 
            chunk
        )
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç AI
        is_relevant = "–ü–û–î–•–û–î–ò–¢" in ai_decision
        explanation = f"AI —Ä–µ—à–µ–Ω–∏–µ: {ai_decision}"
        
        return is_relevant, explanation
    """
    –ù–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —á–µ—Ä–µ–∑ Gemini API.
    –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –ø–æ–¥—Ö–æ–¥–∏—Ç –ª–∏ —á–∞–Ω–∫ –ñ–≤–∞–Ω–µ—Ü–∫–æ–≥–æ –¥–ª—è —à–∫–æ–ª—ã soft skills.
    """
    
    def __init__(self, config: StyleChunkingConfig):
        self.config = config
        self.generation_model = genai.GenerativeModel('gemini-1.5-flash')
        
        # –ü—Ä–æ–º–ø—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        self.relevance_prompt = """
–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω–æ–º—É –∫–æ–Ω—Ç–µ–Ω—Ç—É –¥–ª—è –æ–Ω–ª–∞–π–Ω-—à–∫–æ–ª—ã —Ä–∞–∑–≤–∏—Ç–∏—è soft skills —É –¥–µ—Ç–µ–π.

–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —ç—Ç–æ—Ç —Ñ—Ä–∞–≥–º–µ–Ω—Ç —Ç–µ–∫—Å—Ç–∞ –ú–∏—Ö–∞–∏–ª–∞ –ñ–≤–∞–Ω–µ—Ü–∫–æ–≥–æ –∏ –æ—Ü–µ–Ω–∏, –ø–æ–¥—Ö–æ–¥–∏—Ç –ª–∏ –æ–Ω –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–º, –∫–æ—Ç–æ—Ä—ã–π –∫–æ–Ω—Å—É–ª—å—Ç–∏—Ä—É–µ—Ç —Ä–æ–¥–∏—Ç–µ–ª–µ–π –ø–æ –≤–æ–ø—Ä–æ—Å–∞–º –≤–æ—Å–ø–∏—Ç–∞–Ω–∏—è –∏ —Ä–∞–∑–≤–∏—Ç–∏—è –¥–µ—Ç–µ–π.

–ö–†–ò–¢–ï–†–ò–ò –ü–û–î–•–û–î–Ø–©–ï–ì–û –ö–û–ù–¢–ï–ù–¢–ê:
- –ú—É–¥—Ä—ã–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è –æ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–π –ø—Ä–∏—Ä–æ–¥–µ
- –†–∞–∑–º—ã—à–ª–µ–Ω–∏—è –æ –≤–æ—Å–ø–∏—Ç–∞–Ω–∏–∏, –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–∏, –æ—Ç–Ω–æ—à–µ–Ω–∏—è—Ö
- –Æ–º–æ—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ, –Ω–æ –ø–æ—É—á–∏—Ç–µ–ª—å–Ω—ã–µ –∏—Å—Ç–æ—Ä–∏–∏
- –ê—Ñ–æ—Ä–∏–∑–º—ã –æ –∂–∏–∑–Ω–∏, –æ–±—â–µ–Ω–∏–∏, —Ä–∞–∑–≤–∏—Ç–∏–∏ –ª–∏—á–Ω–æ—Å—Ç–∏
- –ú–∞—Ç–µ—Ä–∏–∞–ª, –∫–æ—Ç–æ—Ä—ã–π –º–æ–∂–µ—Ç –ø–æ–º–æ—á—å —Ä–æ–¥–∏—Ç–µ–ª—è–º –ª—É—á—à–µ –ø–æ–Ω–∏–º–∞—Ç—å –¥–µ—Ç–µ–π

–ö–†–ò–¢–ï–†–ò–ò –ù–ï–ü–û–î–•–û–î–Ø–©–ï–ì–û –ö–û–ù–¢–ï–ù–¢–ê:
- –ü–æ–ª–∏—Ç–∏—á–µ—Å–∫–∞—è —Å–∞—Ç–∏—Ä–∞ –∏–ª–∏ –∫—Ä–∏—Ç–∏–∫–∞ –≤–ª–∞—Å—Ç–∏
- –°–ª–∏—à–∫–æ–º —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Å–æ–≤–µ—Ç—Å–∫–∏–µ —Ä–µ–∞–ª–∏–∏
- –ì—Ä—É–±—ã–π –∏–ª–∏ –Ω–µ–ø–æ–¥—Ö–æ–¥—è—â–∏–π –¥–ª—è —Å–µ–º–µ–π–Ω–æ–π –∞—É–¥–∏—Ç–æ—Ä–∏–∏ —é–º–æ—Ä
- –ú–∞—Ç–µ—Ä–∏–∞–ª –±–µ–∑ –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω–æ–π —Ü–µ–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è —Ä–æ–¥–∏—Ç–µ–ª–µ–π
- –°–ª–∏—à–∫–æ–º —É–∑–∫–æ—Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–µ–º—ã

–§—Ä–∞–≥–º–µ–Ω—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞:
{text}

–û—Ç–≤–µ—Ç—å –¢–û–õ–¨–ö–û –æ–¥–Ω–∏–º —Å–ª–æ–≤–æ–º:
- "–ü–û–î–•–û–î–ò–¢" - –µ—Å–ª–∏ –º–∞—Ç–µ—Ä–∏–∞–ª —É–º–µ—Å—Ç–µ–Ω –¥–ª—è —à–∫–æ–ª—ã soft skills
- "–ù–ï_–ü–û–î–•–û–î–ò–¢" - –µ—Å–ª–∏ –º–∞—Ç–µ—Ä–∏–∞–ª –Ω–µ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –Ω–∞—à–µ–π –∞—É–¥–∏—Ç–æ—Ä–∏–∏

–¢–≤–æ–π –æ—Ç–≤–µ—Ç:"""
    
    def evaluate_chunk_relevance(self, chunk: str) -> Tuple[bool, str]:
        """
        –û—Ü–µ–Ω–∏–≤–∞–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å —á–∞–Ω–∫–∞ –¥–ª—è —à–∫–æ–ª—ã soft skills.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (–ø–æ–¥—Ö–æ–¥–∏—Ç –ª–∏ —á–∞–Ω–∫, –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ —Ä–µ—à–µ–Ω–∏—è)
        """
        try:
            # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–º–ø—Ç —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º —á–∞–Ω–∫–æ–º
            full_prompt = self.relevance_prompt.format(text=chunk[:1000])  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –¥–ª—è API
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ Gemini
            response = self.generation_model.generate_content(full_prompt)
            ai_decision = response.text.strip().upper()
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç AI
            is_relevant = "–ü–û–î–•–û–î–ò–¢" in ai_decision
            explanation = f"AI —Ä–µ—à–µ–Ω–∏–µ: {ai_decision}"
            
            return is_relevant, explanation
            
        except Exception as e:
            print(f"      ‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ —á–∞–Ω–∫–∞: {e}")
            # –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ API, –ø—Ä–∏–Ω–∏–º–∞–µ–º –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ
            return True, f"–û—à–∏–±–∫–∞ API, —á–∞–Ω–∫ –ø—Ä–∏–Ω—è—Ç –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: {str(e)[:100]}"

class ApiRateLimiter:
    """–ö–æ–Ω—Ç—Ä–æ–ª–ª–µ—Ä —á–∞—Å—Ç–æ—Ç—ã API –∑–∞–ø—Ä–æ—Å–æ–≤ (—Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏)"""
    
    def __init__(self, config: StyleChunkingConfig):
        self.config = config
        self.last_request_time = 0
        self.request_count = 0
        self.start_time = time.time()
        
        print(f"üö¶ API Rate Limiter –¥–ª—è —Å—Ç–∏–ª–µ–≤–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞:")
        print(f"   –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏: {config.calculated_delay:.2f} —Å–µ–∫—É–Ω–¥")
        print(f"   –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {'–≤–∫–ª—é—á–µ–Ω–∞' if config.enable_content_filtering else '–æ—Ç–∫–ª—é—á–µ–Ω–∞'}")
    
    def wait_if_needed(self):
        current_time = time.time()
        time_since_last_request = current_time - self.last_request_time
        
        if time_since_last_request < self.config.calculated_delay:
            sleep_time = self.config.calculated_delay - time_since_last_request
            time.sleep(sleep_time)
        
        self.last_request_time = time.time()
        self.request_count += 1
    
    def get_stats(self) -> Dict:
        elapsed_time = time.time() - self.start_time
        requests_per_minute = (self.request_count / elapsed_time) * 60 if elapsed_time > 0 else 0
        
        return {
            "total_requests": self.request_count,
            "elapsed_minutes": elapsed_time / 60,
            "requests_per_minute": requests_per_minute,
            "limit_utilization": (requests_per_minute / self.config.api_requests_per_minute) * 100
        }

class PDFTextExtractor:
    """
    –ù–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ –∏–∑ PDF —Ñ–∞–π–ª–æ–≤.
    """
    
    def __init__(self):
        print("üìÑ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è PDF Text Extractor")
    
    def extract_text_from_pdf(self, pdf_path: str) -> str:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ PDF —Ñ–∞–π–ª–∞ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—ã.
        """
        try:
            print(f"   üìÑ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ PDF: {os.path.basename(pdf_path)}")
            
            extracted_text = ""
            
            with pdfplumber.open(pdf_path) as pdf:
                total_pages = len(pdf.pages)
                print(f"      üìä –í—Å–µ–≥–æ —Å—Ç—Ä–∞–Ω–∏—Ü –≤ PDF: {total_pages}")
                
                for page_num, page in enumerate(pdf.pages, 1):
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                    page_text = page.extract_text()
                    
                    if page_text:
                        # –ë–∞–∑–æ–≤–∞—è –æ—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –æ—Ç –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤ PDF
                        cleaned_text = self._clean_pdf_text(page_text)
                        extracted_text += cleaned_text + "\n\n"
                    
                    # –ü—Ä–æ–≥—Ä–µ—Å—Å-–∏–Ω–¥–∏–∫–∞—Ç–æ—Ä –¥–ª—è –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤
                    if page_num % 50 == 0:
                        print(f"      üìñ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {page_num}/{total_pages}")
                
                print(f"   ‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(extracted_text)} —Å–∏–º–≤–æ–ª–æ–≤ –∏–∑ PDF")
                return extracted_text.strip()
                
        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ —Ç–µ–∫—Å—Ç–∞ –∏–∑ PDF {pdf_path}: {e}")
            return ""
    
    def _clean_pdf_text(self, text: str) -> str:
        """
        –û—á–∏—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –æ—Ç —Ç–∏–ø–∏—á–Ω—ã—Ö –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤ PDF.
        """
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫
        text = re.sub(r'\n{3,}', '\n\n', text)
        
        # –£–±–∏—Ä–∞–µ–º —Ä–∞–∑—Ä—ã–≤—ã —Å–ª–æ–≤ –Ω–∞ –≥—Ä–∞–Ω–∏—Ü–∞—Ö —Å—Ç—Ä–æ–∫ (—á–∞—Å—Ç–∞—è –ø—Ä–æ–±–ª–µ–º–∞ PDF)
        text = re.sub(r'(\w+)-\s*\n\s*(\w+)', r'\1\2', text)
        
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        text = re.sub(r' {2,}', ' ', text)
        
        # –£–±–∏—Ä–∞–µ–º –≤–∏—Å—è—á–∏–µ –Ω–æ–º–µ—Ä–∞ —Å—Ç—Ä–∞–Ω–∏—Ü (–µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å)
        text = re.sub(r'\n\s*\d+\s*\n', '\n\n', text)
        
        return text

class ZhvanetskyStyleAnalyzer:
    """
    –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã—Ö –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–µ–π —Ç–µ–∫—Å—Ç–æ–≤ –ñ–≤–∞–Ω–µ—Ü–∫–æ–≥–æ.
    (–ë–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π - –∫–ª–∞—Å—Å —É–∂–µ —Ö–æ—Ä–æ—à–æ —Ä–∞–±–æ—Ç–∞–µ—Ç)
    """
    
    @staticmethod
    def detect_aphorism(text: str) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ñ—Ä–∞–≥–º–µ–Ω—Ç –∞—Ñ–æ—Ä–∏–∑–º–æ–º –ñ–≤–∞–Ω–µ—Ü–∫–æ–≥–æ"""
        clean_text = re.sub(r'\s+', ' ', text.strip())
        
        if len(clean_text) > 300:
            return False
        
        has_contrast = any(word in clean_text.lower() for word in 
                          ['–Ω–æ', '–∞', '–æ–¥–Ω–∞–∫–æ', '–∑–∞—Ç–æ', '–Ω–µ', '—Ç–æ–ª—å–∫–æ', '–ª–∏—à—å'])
        
        has_wisdom_words = any(word in clean_text.lower() for word in 
                              ['–≤–æ—Å–ø–∏—Ç–∞–Ω–∏–µ', '–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ', '–∂–∏–∑–Ω—å', '–ª—é–¥–∏', '—á–µ–ª–æ–≤–µ–∫', 
                               '–¥–µ—Ç–∏', '—Ä–æ–¥–∏—Ç–µ–ª–∏', '–º—ã—Å–ª—å', '—Å–ª–æ–≤–∞', '–ø–∞–º—è—Ç—å'])
        
        has_parallelism = bool(re.search(r'[–ê-–Ø–Å][^.!?]*[.!?]\s*[–ê-–Ø–Å][^.!?]*[.!?]', clean_text))
        ends_definitively = clean_text.endswith(('.', '!', '?'))
        sentence_count = len(re.findall(r'[.!?]+', clean_text))
        
        is_likely_aphorism = (
            len(clean_text) <= 200 and 
            sentence_count <= 3 and
            ends_definitively and
            (has_contrast or has_wisdom_words or has_parallelism)
        )
        
        return is_likely_aphorism
    
    @staticmethod
    def detect_dialogue(text: str) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ –¥–∏–∞–ª–æ–≥–∞ –≤ —Ç–µ–∫—Å—Ç–µ –ñ–≤–∞–Ω–µ—Ü–∫–æ–≥–æ"""
        dialogue_markers = [
            r'‚Äî\s*[–ê-–Ø–Å]',           
            r':\s*‚Äî',               
            r'[–ê-–Ø–Å][–∞-—è—ë]*:',       
            r'—Å–ø—Ä–æ—Å–∏–ª.*?:',         
            r'—Å–∫–∞–∑–∞–ª.*?:',          
            r'–æ—Ç–≤–µ—á–∞–ª.*?:',         
            r'‚Äî [–ê-–Ø–Å]',            
            r'–î–æ—Ä–æ–≥–∏–µ —Ç–æ–≤–∞—Ä–∏—â–∏',    
            r'–º–∞–ª–µ–Ω—å–∫–∏–π –º–∞–ª—å—á–∏–∫.*–ª–µ—Ç',
        ]
        
        marker_count = sum(1 for pattern in dialogue_markers 
                          if re.search(pattern, text))
        
        return marker_count >= 2 or bool(re.search(r'‚Äî.*‚Äî.*‚Äî', text, re.DOTALL))
    
    @staticmethod
    def detect_logical_chain(text: str) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –ª–æ–≥–∏—á–µ—Å–∫—É—é —Ü–µ–ø–æ—á–∫—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π"""
        chain_markers = [
            r'–≤–æ-–ø–µ—Ä–≤—ã—Ö|–≤–æ-–≤—Ç–æ—Ä—ã—Ö|–≤-—Ç—Ä–µ—Ç—å–∏—Ö',
            r'–ø–æ—Ç–æ–º—É —á—Ç–æ|–ø–æ—ç—Ç–æ–º—É|—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ',
            r'–Ω–∞–ø—Ä–∏–º–µ—Ä|–∫ –ø—Ä–∏–º–µ—Ä—É|—Å–∫–∞–∂–µ–º',
            r'–Ω–æ|–æ–¥–Ω–∞–∫–æ|—Å –¥—Ä—É–≥–æ–π —Å—Ç–æ—Ä–æ–Ω—ã',
            r'–∑–Ω–∞—á–∏—Ç|–∏—Ç–∞–∫|—Ç–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º'
        ]
        
        return any(re.search(pattern, text.lower()) for pattern in chain_markers)
    
    @staticmethod
    def find_natural_breaks(text: str) -> List[int]:
        """–ù–∞—Ö–æ–¥–∏—Ç –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –º–µ—Å—Ç–∞ –¥–ª—è —Ä–∞–∑—Ä—ã–≤–∞ —Ç–µ–∫—Å—Ç–∞"""
        break_points = []
        
        for match in re.finditer(r'\n\n+', text):
            break_points.append(match.start())
        
        for match in re.finditer(r'\.\s+[–ê-–Ø–Å]', text):
            break_points.append(match.start() + 1)
        
        for match in re.finditer(r'[!?]\s+[–ê-–Ø–Å]', text):
            break_points.append(match.start() + 1)
        
        return sorted(set(break_points))

class ZhvanetskyStyleChunker:
    """
    –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —á–∞–Ω–∫–µ—Ä –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤ –ú–∏—Ö–∞–∏–ª–∞ –ñ–≤–∞–Ω–µ—Ü–∫–æ–≥–æ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π PDF –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏.
    """
    
    def __init__(self, config: StyleChunkingConfig):
        self.config = config
        self.rate_limiter = ApiRateLimiter(config)
        self.analyzer = ZhvanetskyStyleAnalyzer()
        self.pdf_extractor = PDFTextExtractor()  # –ù–û–í–û–ï
        self.content_filter = ContentRelevanceFilter(config) if config.enable_content_filtering else None  # –ù–û–í–û–ï
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Gemini
        genai.configure(api_key=GEMINI_API_KEY)
        self.embedding_model = 'models/text-embedding-004'
        
        print("üé≠ –†–ê–°–®–ò–†–ï–ù–ù–´–ô –ß–ê–ù–ö–ï–† –î–õ–Ø –°–¢–ò–õ–ï–í–û–ì–û –ö–û–ù–¢–ï–ù–¢–ê –ú–ò–•–ê–ò–õ–ê –ñ–í–ê–ù–ï–¶–ö–û–ì–û")
        print(f"üìù –¶–µ–ª–µ–≤—ã–µ —Ä–∞–∑–º–µ—Ä—ã: {config.min_chunk_size}-{config.ideal_chunk_size} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"üíé –ê—Ñ–æ—Ä–∏–∑–º—ã: {config.aphorism_min_size}-{config.aphorism_max_size} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"üéµ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–∏—Ç–º–∞: {'‚úì' if config.preserve_rhythm else '‚úó'}")
        print(f"üìÑ –ü–æ–¥–¥–µ—Ä–∂–∫–∞ PDF: ‚úì")
        print(f"üîç –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {'‚úì' if config.enable_content_filtering else '‚úó'}")
        print(f"üîß ASCII-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–µ ID: ‚úì")
    
    def generate_safe_vector_id(self, index_name: str, filename: str, chunk_idx: int) -> str:
        """
        –ù–û–í–ê–Ø –§–£–ù–ö–¶–ò–Ø: –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç ASCII-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –¥–ª—è Pinecone.
        
        –ê–ª–≥–æ—Ä–∏—Ç–º —Ä–∞–±–æ—Ç—ã:
        1. –ë–∞–∑–æ–≤–∞—è —Ç—Ä–∞–Ω—Å–ª–∏—Ç–µ—Ä–∞—Ü–∏—è –∫–∏—Ä–∏–ª–ª–∏—á–µ—Å–∫–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
        2. –£–¥–∞–ª–µ–Ω–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–π —Ñ–∞–π–ª–æ–≤ –∏ –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤
        3. –ó–∞–º–µ–Ω–∞ –ø—Ä–æ–±–µ–ª–æ–≤ –∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ –Ω–∞ –¥–µ—Ñ–∏—Å—ã
        4. –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª–∏–Ω—ã –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã—Ö ID
        5. –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∏ –æ—á–∏—Å—Ç–∫–∞
        
        Args:
            index_name: –ù–∞–∑–≤–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, "ukido-style")
            filename: –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞
            chunk_idx: –ù–æ–º–µ—Ä —á–∞–Ω–∫–∞
        
        Returns:
            ASCII-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π —Å—Ç—Ä–æ–∫–æ–≤—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä
        """
        # –°–ª–æ–≤–∞—Ä—å –¥–ª—è —Ç—Ä–∞–Ω—Å–ª–∏—Ç–µ—Ä–∞—Ü–∏–∏ –æ—Å–Ω–æ–≤–Ω—ã—Ö –∫–∏—Ä–∏–ª–ª–∏—á–µ—Å–∫–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
        # –û—Å–Ω–æ–≤–∞–Ω –Ω–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–µ BGN/PCGN –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
        cyrillic_to_latin = {
            '–∞': 'a', '–±': 'b', '–≤': 'v', '–≥': 'g', '–¥': 'd', '–µ': 'e', '—ë': 'yo',
            '–∂': 'zh', '–∑': 'z', '–∏': 'i', '–π': 'y', '–∫': 'k', '–ª': 'l', '–º': 'm',
            '–Ω': 'n', '–æ': 'o', '–ø': 'p', '—Ä': 'r', '—Å': 's', '—Ç': 't', '—É': 'u',
            '—Ñ': 'f', '—Ö': 'kh', '—Ü': 'ts', '—á': 'ch', '—à': 'sh', '—â': 'shch',
            '—ä': '', '—ã': 'y', '—å': '', '—ç': 'e', '—é': 'yu', '—è': 'ya',
            
            '–ê': 'A', '–ë': 'B', '–í': 'V', '–ì': 'G', '–î': 'D', '–ï': 'E', '–Å': 'Yo',
            '–ñ': 'Zh', '–ó': 'Z', '–ò': 'I', '–ô': 'Y', '–ö': 'K', '–õ': 'L', '–ú': 'M',
            '–ù': 'N', '–û': 'O', '–ü': 'P', '–†': 'R', '–°': 'S', '–¢': 'T', '–£': 'U',
            '–§': 'F', '–•': 'Kh', '–¶': 'Ts', '–ß': 'Ch', '–®': 'Sh', '–©': 'Shch',
            '–™': '', '–´': 'Y', '–¨': '', '–≠': 'E', '–Æ': 'Yu', '–Ø': 'Ya'
        }
        
        # –®–∞–≥ 1: –£–¥–∞–ª—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Ñ–∞–π–ª–∞ –¥–ª—è –±–æ–ª–µ–µ —á–∏—Å—Ç—ã—Ö ID
        clean_filename = os.path.splitext(filename)[0]
        
        # –®–∞–≥ 2: –ü—Ä–∏–º–µ–Ω—è–µ–º —Ç—Ä–∞–Ω—Å–ª–∏—Ç–µ—Ä–∞—Ü–∏—é –∫–∏—Ä–∏–ª–ª–∏—á–µ—Å–∫–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
        transliterated = ""
        for char in clean_filename:
            if char in cyrillic_to_latin:
                transliterated += cyrillic_to_latin[char]
            else:
                transliterated += char
        
        # –®–∞–≥ 3: –ó–∞–º–µ–Ω—è–µ–º –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã –Ω–∞ –¥–µ—Ñ–∏—Å—ã
        # –£–¥–∞–ª—è–µ–º –∏–ª–∏ –∑–∞–º–µ–Ω—è–µ–º –≤—Å–µ, —á—Ç–æ –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –±—É–∫–≤–æ–π, —Ü–∏—Ñ—Ä–æ–π –∏–ª–∏ –¥–µ—Ñ–∏—Å–æ–º
        import string
        safe_chars = string.ascii_letters + string.digits + '-_'
        normalized = ""
        for char in transliterated:
            if char in safe_chars:
                normalized += char
            elif char in ' .()[]{}':
                normalized += '-'  # –ó–∞–º–µ–Ω—è–µ–º –ø—Ä–æ–±–µ–ª—ã –∏ —Å–∫–æ–±–∫–∏ –Ω–∞ –¥–µ—Ñ–∏—Å—ã
            # –û—Å—Ç–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã –ø—Ä–æ—Å—Ç–æ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
        
        # –®–∞–≥ 4: –£–±–∏—Ä–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –¥–µ—Ñ–∏—Å—ã –∏ –¥–µ—Ñ–∏—Å—ã –≤ –Ω–∞—á–∞–ª–µ/–∫–æ–Ω—Ü–µ
        # –≠—Ç–æ –≤–∞–∂–Ω–æ –¥–ª—è —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏ –∏ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º —Å –Ω–µ–∫–æ—Ç–æ—Ä—ã–º–∏ —Å–∏—Å—Ç–µ–º–∞–º–∏
        while '--' in normalized:
            normalized = normalized.replace('--', '-')
        normalized = normalized.strip('-')
        
        # –®–∞–≥ 5: –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É —á–∞—Å—Ç–∏ —Å –∏–º–µ–Ω–µ–º —Ñ–∞–π–ª–∞
        # Pinecone –∏–º–µ–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –Ω–∞ –æ–±—â—É—é –¥–ª–∏–Ω—É ID
        max_filename_length = 50  # –†–∞–∑—É–º–Ω–æ–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª—è —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏
        if len(normalized) > max_filename_length:
            normalized = normalized[:max_filename_length].rstrip('-')
        
        # –®–∞–≥ 6: –°–æ–∑–¥–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π ID
        # –§–æ—Ä–º–∞—Ç: {index_name}-{safe_filename}-{chunk_number}
        safe_id = f"{index_name}-{normalized}-{chunk_idx}"
        
        # –®–∞–≥ 7: –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ ASCII —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å
        # –≠—Ç–æ –∑–∞—â–∏—Ç–∞ –æ—Ç –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥–ª–∏ –ø—Ä–æ—Å–∫–æ–ª—å–∑–Ω—É—Ç—å
        try:
            safe_id.encode('ascii')
        except UnicodeEncodeError:
            # –ï—Å–ª–∏ –≤—Å–µ –µ—â–µ –µ—Å—Ç—å –ø—Ä–æ–±–ª–µ–º—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ö–µ—à–∞
            import hashlib
            hash_part = hashlib.md5(clean_filename.encode('utf-8')).hexdigest()[:8]
            safe_id = f"{index_name}-file-{hash_part}-{chunk_idx}"
            print(f"      üîß Fallback ID –¥–ª—è –ø—Ä–æ–±–ª–µ–º–Ω–æ–≥–æ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞: {safe_id}")
        
        return safe_id
    
    def extract_content_from_file(self, file_path: str) -> Optional[str]:
        """
        –ù–û–í–ê–Ø –§–£–ù–ö–¶–ò–Ø: –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∏–∑ —Ñ–∞–π–ª–∞ (txt –∏–ª–∏ pdf)
        """
        file_extension = os.path.splitext(file_path)[1].lower()
        
        if file_extension == '.txt':
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Ñ–∞–π–ª–∞ (—Å—É—â–µ—Å—Ç–≤—É—é—â–∞—è –ª–æ–≥–∏–∫–∞)
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    return f.read().strip()
            except Exception as e:
                print(f"   ‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Ñ–∞–π–ª–∞ {file_path}: {e}")
                return None
                
        elif file_extension == '.pdf':
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ PDF —Ñ–∞–π–ª–∞ (–Ω–æ–≤–∞—è –ª–æ–≥–∏–∫–∞)
            return self.pdf_extractor.extract_text_from_pdf(file_path)
        
        else:
            print(f"   ‚ö†Ô∏è –ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞: {file_extension}")
            return None
    
    def analyze_text_structure(self, content: str, filename: str) -> Dict:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Ç–µ–∫—Å—Ç–∞ –ñ–≤–∞–Ω–µ—Ü–∫–æ–≥–æ (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)"""
        structure = {
            "total_length": len(content),
            "has_dialogue": self.analyzer.detect_dialogue(content),
            "has_logical_chains": self.analyzer.detect_logical_chain(content),
            "natural_breaks": self.analyzer.find_natural_breaks(content),
            "estimated_aphorisms": 0,
            "paragraphs": len([p for p in content.split('\n\n') if p.strip()])
        }
        
        paragraphs = [p.strip() for p in content.split('\n\n') if p.strip()]
        for paragraph in paragraphs:
            if self.analyzer.detect_aphorism(paragraph):
                structure["estimated_aphorisms"] += 1
        
        print(f"   üîç –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã '{filename}':")
        print(f"      üìè –î–ª–∏–Ω–∞: {structure['total_length']} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"      üí¨ –î–∏–∞–ª–æ–≥–∏: {'‚úì' if structure['has_dialogue'] else '‚úó'}")
        print(f"      üîó –õ–æ–≥–∏—á–µ—Å–∫–∏–µ —Ü–µ–ø–∏: {'‚úì' if structure['has_logical_chains'] else '‚úó'}")
        print(f"      üíé –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö –∞—Ñ–æ—Ä–∏–∑–º–æ–≤: {structure['estimated_aphorisms']}")
        print(f"      üìÑ –ê–±–∑–∞—Ü–µ–≤: {structure['paragraphs']}")
        
        return structure
    
    def create_style_aware_chunks(self, content: str, filename: str) -> List[str]:
        """–°–æ–∑–¥–∞–µ—Ç —á–∞–Ω–∫–∏ —Å —É—á–µ—Ç–æ–º —Å—Ç–∏–ª–µ–≤—ã—Ö –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–µ–π (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π –≤ –æ—Å–Ω–æ–≤–Ω–æ–π –ª–æ–≥–∏–∫–µ)"""
        print(f"   ‚úÇÔ∏è –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç–∏–ª–µ–≤—ã—Ö —á–∞–Ω–∫–æ–≤: {filename}")
        
        structure = self.analyze_text_structure(content, filename)
        paragraphs = [p.strip() for p in content.split('\n\n') if p.strip()]
        
        chunks = []
        current_chunk = []
        current_size = 0
        
        for i, paragraph in enumerate(paragraphs):
            paragraph_length = len(paragraph)
            
            if self.analyzer.detect_aphorism(paragraph):
                if current_chunk and current_size >= self.config.min_chunk_size:
                    chunks.append('\n\n'.join(current_chunk))
                    current_chunk = []
                    current_size = 0
                
                chunks.append(paragraph)
                print(f"      üíé –ê—Ñ–æ—Ä–∏–∑–º –≤—ã–¥–µ–ª–µ–Ω: {paragraph[:50]}...")
                continue
            
            if self.analyzer.detect_dialogue(paragraph):
                if paragraph_length <= self.config.max_chunk_size:
                    if current_chunk:
                        chunks.append('\n\n'.join(current_chunk))
                        current_chunk = []
                        current_size = 0
                    
                    chunks.append(paragraph)
                    print(f"      üí¨ –î–∏–∞–ª–æ–≥ —Å–æ—Ö—Ä–∞–Ω–µ–Ω —Ü–µ–ª–∏–∫–æ–º: {paragraph_length} —Å–∏–º–≤–æ–ª–æ–≤")
                    continue
            
            if current_chunk:
                potential_size = current_size + paragraph_length + 2
            else:
                potential_size = paragraph_length
            
            if (potential_size > self.config.ideal_chunk_size and 
                current_size >= self.config.min_chunk_size):
                
                chunks.append('\n\n'.join(current_chunk))
                print(f"      üì¶ –ß–∞–Ω–∫ —Å–æ–∑–¥–∞–Ω: {current_size} —Å–∏–º–≤–æ–ª–æ–≤")
                
                current_chunk = [paragraph]
                current_size = paragraph_length
            else:
                current_chunk.append(paragraph)
                if len(current_chunk) == 1:
                    current_size = paragraph_length
                else:
                    current_size += paragraph_length + 2
        
        if current_chunk:
            chunks.append('\n\n'.join(current_chunk))
            print(f"      üì¶ –§–∏–Ω–∞–ª—å–Ω—ã–π —á–∞–Ω–∫: {current_size} —Å–∏–º–≤–æ–ª–æ–≤")
        
        processed_chunks = self._post_process_style_chunks(chunks)
        
        print(f"   üéØ –°–æ–∑–¥–∞–Ω–æ —Å—Ç–∏–ª–µ–≤—ã—Ö —á–∞–Ω–∫–æ–≤: {len(processed_chunks)}")
        return processed_chunks
    
    def _post_process_style_chunks(self, chunks: List[str]) -> List[str]:
        """–ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ —á–∞–Ω–∫–æ–≤ –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)"""
        processed = []
        
        for i, chunk in enumerate(chunks):
            cleaned_chunk = chunk.strip()
            chunk_length = len(cleaned_chunk)
            
            if (chunk_length < self.config.min_chunk_size and 
                not self.analyzer.detect_aphorism(cleaned_chunk)):
                
                if processed and len(processed[-1] + '\n\n' + cleaned_chunk) <= self.config.max_chunk_size:
                    processed[-1] = processed[-1] + '\n\n' + cleaned_chunk
                    print(f"      üîó –ö–æ—Ä–æ—Ç–∫–∏–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç –æ–±—ä–µ–¥–∏–Ω–µ–Ω —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º")
                    continue
                else:
                    print(f"      ‚ö†Ô∏è –ö–æ—Ä–æ—Ç–∫–∏–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω ({chunk_length} —Å–∏–º–≤–æ–ª–æ–≤)")
            
            cleaned_chunk = re.sub(r'\n{3,}', '\n\n', cleaned_chunk)
            cleaned_chunk = re.sub(r' {2,}', ' ', cleaned_chunk)
            
            processed.append(cleaned_chunk)
        
        return processed
    
    def filter_chunk_if_needed(self, chunk: str, source_file: str) -> Tuple[bool, str]:
        """
        –ù–û–í–ê–Ø –§–£–ù–ö–¶–ò–Ø: –§–∏–ª—å—Ç—Ä—É–µ—Ç —á–∞–Ω–∫, –µ—Å–ª–∏ —ç—Ç–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (–ø—Ä–∏–Ω—è—Ç—å –ª–∏ —á–∞–Ω–∫, –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ —Ä–µ—à–µ–Ω–∏—è)
        """
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, –Ω—É–∂–Ω–∞ –ª–∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–∏–ø–∞ —Ñ–∞–π–ª–∞
        file_extension = os.path.splitext(source_file)[1].lower()
        
        # –¢–µ–∫—Å—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã –Ω–µ —Ñ–∏–ª—å—Ç—Ä—É–µ–º (–æ–Ω–∏ —É–∂–µ –æ—Ç–æ–±—Ä–∞–Ω—ã –≤—Ä—É—á–Ω—É—é)
        if file_extension == '.txt':
            return True, "–¢–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª - —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –Ω–µ –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è"
        
        # PDF —Ñ–∞–π–ª—ã —Ñ–∏–ª—å—Ç—Ä—É–µ–º, –µ—Å–ª–∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –≤–∫–ª—é—á–µ–Ω–∞
        if file_extension == '.pdf' and self.config.enable_content_filtering and self.content_filter:
            self.rate_limiter.wait_if_needed()  # –°–æ–±–ª—é–¥–∞–µ–º –ª–∏–º–∏—Ç—ã API
            return self.content_filter.evaluate_chunk_relevance(chunk)
        
        # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –ø—Ä–∏–Ω–∏–º–∞–µ–º —á–∞–Ω–∫
        return True, "–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –æ—Ç–∫–ª—é—á–µ–Ω–∞"
    
    def vectorize_style_chunk(self, chunk: str, index_name: str, filename: str, chunk_idx: int) -> Optional[Dict]:
        """
        –í–µ–∫—Ç–æ—Ä–∏–∑—É–µ—Ç —Å—Ç–∏–ª–µ–≤–æ–π —á–∞–Ω–∫ —Å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–º–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ (–æ–±–Ω–æ–≤–ª–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è).
        
        –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –£–¥–∞–ª–µ–Ω–∞ –¥—É–±–ª–∏—Ä—É—é—â–∞—è –ª–æ–≥–∏–∫–∞ rate limiting, –∫–æ—Ç–æ—Ä–∞—è –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤–∞–ª–∞
        —Å SmartRetryHandler. –¢–µ–ø–µ—Ä—å —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Å–∫–æ—Ä–æ—Å—Ç—å—é API –≤—ã–∑–æ–≤–æ–≤ –ø–æ–ª–Ω–æ—Å—Ç—å—é
        —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–æ –≤ retry handler, —á—Ç–æ –∏—Å–∫–ª—é—á–∞–µ—Ç –∫–æ–Ω—Ñ–ª–∏–∫—Ç—ã –∏ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç
        –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã.
        """
        # –ù–û–í–û–ï: –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º ASCII-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π ID
        safe_chunk_id = self.generate_safe_vector_id(index_name, filename, chunk_idx)
        
        # –ù–û–í–û–ï: –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ —Ñ–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å —á–∞–Ω–∫
        should_accept, filter_reason = self.filter_chunk_if_needed(chunk, filename)
        
        if not should_accept:
            print(f"      üö´ –ß–∞–Ω–∫ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω: {filter_reason}")
            return None
        
        # –£–î–ê–õ–ï–ù–û: rate_limiter.wait_if_needed() - —Ç–µ–ø–µ—Ä—å —ç—Ç–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç SmartRetryHandler
        # –ï—Å–ª–∏ vectorize_style_chunk –±—É–¥–µ—Ç –æ–±–µ—Ä–Ω—É—Ç –≤ retry handler, –∑–∞–¥–µ—Ä–∂–∫–∏ –±—É–¥—É—Ç –ø—Ä–∏–º–µ–Ω–µ–Ω—ã –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
        
        try:
            # –°–æ–∑–¥–∞–µ–º embedding –¥–ª—è —Å—Ç–∏–ª–µ–≤–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            response = genai.embed_content(
                model=self.embedding_model,
                content=chunk,
                task_type="RETRIEVAL_DOCUMENT",
                title="Zhvanetsky Style Sample"
            )
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –¥–ª—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
            content_type = "aphorism" if self.analyzer.detect_aphorism(chunk) else "narrative"
            if self.analyzer.detect_dialogue(chunk):
                content_type = "dialogue"
            
            # –†–ê–°–®–ò–†–ï–ù–ù–´–ï –ú–ï–¢–ê–î–ê–ù–ù–´–ï
            return {
                "id": safe_chunk_id,  # –û–ë–ù–û–í–õ–ï–ù–û: –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–µ–∑–æ–ø–∞—Å–Ω—ã–π ID
                "values": response['embedding'],
                "metadata": {
                    "text": chunk,
                    "chunk_size": len(chunk),
                    "content_type": content_type,
                    "style_source": "zhvanetsky",
                    "has_dialogue": self.analyzer.detect_dialogue(chunk),
                    "is_aphorism": self.analyzer.detect_aphorism(chunk),
                    "embedding_model": self.embedding_model,
                    "task_type": "RETRIEVAL_DOCUMENT",
                    "source_file": filename,  # –ù–û–í–û–ï
                    "source_file_type": os.path.splitext(filename)[1].lower(),  # –ù–û–í–û–ï
                    "original_filename": filename,  # –ù–û–í–û–ï: —Å–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ –∏–º—è –¥–ª—è —Å–ø—Ä–∞–≤–∫–∏
                    "safe_id": safe_chunk_id,  # –ù–û–í–û–ï: —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π ID –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
                    "content_filtered": should_accept,  # –ù–û–í–û–ï
                    "filter_reason": filter_reason,  # –ù–û–í–û–ï
                    "created_at": datetime.now().isoformat()
                }
            }
            
        except Exception as e:
            print(f"      ‚ùå –û—à–∏–±–∫–∞ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ —Å—Ç–∏–ª–µ–≤–æ–≥–æ —á–∞–Ω–∫–∞ –¥–ª—è —Ñ–∞–π–ª–∞ {filename}: {e}")
            return None
    
    def process_style_directory(self, directory_path: str, index_name: str) -> Dict:
        """
        –ú–û–î–ò–§–ò–¶–ò–†–û–í–ê–ù–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø: –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é —Å —Ç–µ–∫—Å—Ç–æ–≤—ã–º–∏ –∏ PDF —Ñ–∞–π–ª–∞–º–∏.
        """
        start_time = time.time()
        
        print(f"\nüé≠ –û–ë–†–ê–ë–û–¢–ö–ê –°–¢–ò–õ–ï–í–´–• –¢–ï–ö–°–¢–û–í –ú–ò–•–ê–ò–õ–ê –ñ–í–ê–ù–ï–¶–ö–û–ì–û")
        print(f"üìÇ –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {directory_path}")
        print(f"üéØ –ò–Ω–¥–µ–∫—Å: {index_name}")
        print("=" * 65)
        
        # –ü–æ–¥–∫–ª—é—á–∞–µ–º—Å—è –∫ Pinecone
        try:
            pc = Pinecone(api_key=PINECONE_API_KEY)
            index = pc.Index(host=PINECONE_HOST_STYLE)
            print("üîå –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Pinecone (—Å—Ç–∏–ª–µ–≤–æ–π –∏–Ω–¥–µ–∫—Å) —É—Å–ø–µ—à–Ω–æ")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Pinecone: {e}")
            return {"success": False, "error": str(e)}
        
        # –û—á–∏—â–∞–µ–º —Å—Ç–∏–ª–µ–≤–æ–π –∏–Ω–¥–µ–∫—Å
        print("üóëÔ∏è –û—á–∏—Å—Ç–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Å—Ç–∏–ª–µ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
        index.delete(delete_all=True)
        time.sleep(3)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        if not os.path.exists(directory_path):
            print(f"‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è '{directory_path}' –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
            return {"success": False, "error": f"Directory '{directory_path}' not found"}
        
        # –û–ë–ù–û–í–õ–ï–ù–ù–ê–Ø –õ–û–ì–ò–ö–ê: –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ (txt –∏ pdf)
        try:
            all_files = os.listdir(directory_path)
            supported_files = [f for f in all_files if f.endswith(('.txt', '.pdf'))]
        except PermissionError:
            print(f"‚ùå –ù–µ—Ç –ø—Ä–∞–≤ –¥–æ—Å—Ç—É–ø–∞ –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ '{directory_path}'")
            return {"success": False, "error": f"Permission denied for directory '{directory_path}'"}
        
        if not supported_files:
            print(f"‚ùå –í –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ '{directory_path}' –Ω–µ –Ω–∞–π–¥–µ–Ω–æ .txt –∏–ª–∏ .pdf —Ñ–∞–π–ª–æ–≤")
            return {"success": False, "error": f"No .txt or .pdf files found in '{directory_path}'"}
            
        print(f"üìÅ –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(supported_files)} (.txt –∏ .pdf)")
        
        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        stats = {
            "files_processed": 0,
            "txt_files_processed": 0,  # –ù–û–í–û–ï
            "pdf_files_processed": 0,  # –ù–û–í–û–ï
            "total_chunks": 0,
            "chunks_accepted": 0,  # –ù–û–í–û–ï
            "chunks_filtered": 0,  # –ù–û–í–û–ï
            "aphorism_chunks": 0,
            "dialogue_chunks": 0,
            "narrative_chunks": 0,
            "vectors_uploaded": 0,
            "total_content_size": 0,
            "average_chunk_size": 0,
            "processing_time": 0,
            "api_stats": {},
            "file_details": []
        }
        
        # –û–ë–ù–û–í–õ–ï–ù–ù–´–ô –¶–ò–ö–õ: –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π —Ñ–∞–π–ª (txt –∏ pdf)
        for file_idx, filename in enumerate(supported_files):
            print(f"\nüìñ –§–∞–π–ª {file_idx + 1}/{len(supported_files)}: {filename}")
            
            file_start_time = time.time()
            file_path = os.path.join(directory_path, filename)
            file_extension = os.path.splitext(filename)[1].lower()
            
            try:
                # –ù–û–í–û–ï: –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
                content = self.extract_content_from_file(file_path)
                
                if not content or len(content) < 50:
                    print(f"   ‚ö†Ô∏è –§–∞–π–ª —Å–ª–∏—à–∫–æ–º –º–∞–ª –∏–ª–∏ –ø—É—Å—Ç ({len(content) if content else 0} —Å–∏–º–≤–æ–ª–æ–≤), –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                    continue
                
                stats["total_content_size"] += len(content)
                
                # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Ç–∏–ø–∞–º —Ñ–∞–π–ª–æ–≤
                if file_extension == '.txt':
                    stats["txt_files_processed"] += 1
                elif file_extension == '.pdf':
                    stats["pdf_files_processed"] += 1
                
                # –°–æ–∑–¥–∞–µ–º —Å—Ç–∏–ª–µ–≤—ã–µ —á–∞–Ω–∫–∏
                chunks = self.create_style_aware_chunks(content, filename)
                
                if not chunks:
                    print(f"   ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —Å—Ç–∏–ª–µ–≤—ã–µ —á–∞–Ω–∫–∏, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Ñ–∞–π–ª")
                    continue
                
                # –í–µ–∫—Ç–æ—Ä–∏–∑—É–µ–º –∏ –∑–∞–≥—Ä—É–∂–∞–µ–º —á–∞–Ω–∫–∏ —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π
                print(f"   üîÑ –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è {len(chunks)} —Å—Ç–∏–ª–µ–≤—ã—Ö —á–∞–Ω–∫–æ–≤...")
                
                file_vectors = 0
                file_aphorisms = 0
                file_dialogues = 0
                file_narratives = 0
                file_accepted = 0
                file_filtered = 0
                
                for chunk_idx, chunk in enumerate(chunks):
                    # –û–ë–ù–û–í–õ–ï–ù–û: –¢–µ–ø–µ—Ä—å –ø–µ—Ä–µ–¥–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ ID
                    vector_data = self.vectorize_style_chunk(chunk, index_name, filename, chunk_idx)
                    
                    if vector_data:
                        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Ç–∏–ø—ã –∫–æ–Ω—Ç–µ–Ω—Ç–∞
                        content_type = vector_data["metadata"]["content_type"]
                        if content_type == "aphorism":
                            file_aphorisms += 1
                        elif content_type == "dialogue":
                            file_dialogues += 1
                        else:
                            file_narratives += 1
                        
                        # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤ Pinecone
                        index.upsert(vectors=[vector_data])
                        file_vectors += 1
                        file_accepted += 1
                        
                        # –ü—Ä–æ–≥—Ä–µ—Å—Å-–∏–Ω–¥–∏–∫–∞—Ç–æ—Ä
                        if chunk_idx % 3 == 0:
                            print(f"      üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —á–∞–Ω–∫–æ–≤: {chunk_idx + 1}/{len(chunks)}")
                    else:
                        file_filtered += 1
                
                file_time = time.time() - file_start_time
                
                # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ñ–∞–π–ª—É
                file_stat = {
                    "filename": filename,
                    "file_type": file_extension,
                    "content_size": len(content),
                    "chunks_created": len(chunks),
                    "chunks_accepted": file_accepted,
                    "chunks_filtered": file_filtered,
                    "aphorisms": file_aphorisms,
                    "dialogues": file_dialogues,
                    "narratives": file_narratives,
                    "vectors_uploaded": file_vectors,
                    "processing_time": file_time,
                    "average_chunk_size": len(content) // len(chunks) if chunks else 0
                }
                
                stats["file_details"].append(file_stat)
                stats["files_processed"] += 1
                stats["total_chunks"] += len(chunks)
                stats["chunks_accepted"] += file_accepted
                stats["chunks_filtered"] += file_filtered
                stats["aphorism_chunks"] += file_aphorisms
                stats["dialogue_chunks"] += file_dialogues
                stats["narrative_chunks"] += file_narratives
                stats["vectors_uploaded"] += file_vectors
                
                print(f"   ‚úÖ –§–∞–π–ª –æ–±—Ä–∞–±–æ—Ç–∞–Ω –∑–∞ {file_time:.1f}—Å:")
                print(f"      üìÑ –¢–∏–ø: {file_extension.upper()}")
                print(f"      ‚úÖ –ü—Ä–∏–Ω—è—Ç–æ —á–∞–Ω–∫–æ–≤: {file_accepted}")
                print(f"      üö´ –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ: {file_filtered}")
                print(f"      üíé –ê—Ñ–æ—Ä–∏–∑–º–æ–≤: {file_aphorisms}")
                print(f"      üí¨ –î–∏–∞–ª–æ–≥–æ–≤: {file_dialogues}")
                print(f"      üìù –ü–æ–≤–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–π: {file_narratives}")
                
            except Exception as e:
                print(f"   ‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞ {filename}: {e}")
                continue
        
        # –§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        total_time = time.time() - start_time
        stats["processing_time"] = total_time
        stats["average_chunk_size"] = (stats["total_content_size"] // stats["total_chunks"] 
                                     if stats["total_chunks"] > 0 else 0)
        stats["api_stats"] = self.rate_limiter.get_stats()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ Pinecone
        time.sleep(3)
        final_stats = index.describe_index_stats()
        
        # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É retry –æ–ø–µ—Ä–∞—Ü–∏–π –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞
        retry_stats = None
        if self.content_filter and self.content_filter.retry_handler:
            retry_stats = self.content_filter.retry_handler.get_retry_statistics()
        
        # –†–ê–°–®–ò–†–ï–ù–ù–´–ô –û–¢–ß–ï–¢ —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ retry –æ–ø–µ—Ä–∞—Ü–∏—è—Ö
        print(f"\nüéâ –û–ë–†–ê–ë–û–¢–ö–ê –°–¢–ò–õ–ï–í–´–• –¢–ï–ö–°–¢–û–í –ó–ê–í–ï–†–®–ï–ù–ê!")
        print("=" * 55)
        print(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã:")
        print(f"   üìÅ –§–∞–π–ª–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {stats['files_processed']}")
        print(f"      üìù TXT —Ñ–∞–π–ª–æ–≤: {stats['txt_files_processed']}")
        print(f"      üìÑ PDF —Ñ–∞–π–ª–æ–≤: {stats['pdf_files_processed']}")
        print(f"   üìù –í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤ —Å–æ–∑–¥–∞–Ω–æ: {stats['total_chunks']}")
        print(f"      ‚úÖ –ü—Ä–∏–Ω—è—Ç–æ: {stats['chunks_accepted']}")
        print(f"      üö´ –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ: {stats['chunks_filtered']}")
        print(f"   üíé –ê—Ñ–æ—Ä–∏–∑–º–æ–≤: {stats['aphorism_chunks']}")
        print(f"   üí¨ –î–∏–∞–ª–æ–≥–æ–≤: {stats['dialogue_chunks']}")
        print(f"   üìñ –ü–æ–≤–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–π: {stats['narrative_chunks']}")
        print(f"   üíæ –í–µ–∫—Ç–æ—Ä–æ–≤ –∑–∞–≥—Ä—É–∂–µ–Ω–æ: {stats['vectors_uploaded']}")
        print(f"   üìè –°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞: {stats['average_chunk_size']} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"   ‚è±Ô∏è –û–±—â–µ–µ –≤—Ä–µ–º—è: {total_time/60:.1f} –º–∏–Ω—É—Ç")
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± API –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏
        print(f"üîå API —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        print(f"   üîÑ –í—Å–µ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤: {stats['api_stats']['total_requests']}")
        print(f"   üìà –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ª–∏–º–∏—Ç–∞: {stats['api_stats']['limit_utilization']:.1f}%")
        
        # –ù–û–í–ê–Ø –°–ï–ö–¶–ò–Ø: –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ retry –æ–ø–µ—Ä–∞—Ü–∏–π
        if retry_stats:
            print(f"üîÑ –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ retry —Å–∏—Å—Ç–µ–º—ã:")
            print(f"   üîÅ –í—Å–µ–≥–æ –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –ø–æ–ø—ã—Ç–æ–∫: {retry_stats['total_retries']}")
            print(f"   ‚úÖ –£—Å–ø–µ—à–Ω—ã—Ö retry: {retry_stats['successful_retries']}")
            print(f"   üö´ –ü—Ä–æ–≤–∞–ª–∏–≤—à–∏—Ö—Å—è –æ–ø–µ—Ä–∞—Ü–∏–π: {retry_stats['failed_operations']}")
            print(f"   ‚è±Ô∏è Rate limit —Å–æ–±—ã—Ç–∏–π: {retry_stats['rate_limit_hits']}")
            
            # –í—ã—á–∏—Å–ª—è–µ–º —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å retry —Å–∏—Å—Ç–µ–º—ã
            if retry_stats['total_retries'] > 0:
                success_rate = (retry_stats['successful_retries'] / retry_stats['total_retries']) * 100
                print(f"   üìä –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å retry: {success_rate:.1f}%")
        
        print(f"‚úÖ –°—Ç–∏–ª–µ–≤—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤ –≤ Pinecone: {final_stats.total_vector_count}")
        
        return {"success": True, "stats": stats, "retry_stats": retry_stats}

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Ç–∏–ª–µ–≤—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ –ñ–≤–∞–Ω–µ—Ü–∫–æ–≥–æ (txt –∏ pdf)"""
    
    # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è —Å—Ç–∏–ª–µ–≤–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π
    config = StyleChunkingConfig()
    
    # –°–æ–∑–¥–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —á–∞–Ω–∫–µ—Ä
    chunker = ZhvanetskyStyleChunker(config)
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å—Ç–∏–ª–µ–≤—ã–µ —Ç–µ–∫—Å—Ç—ã (–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∏ txt, –∏ pdf)
    result = chunker.process_style_directory("data_style", "ukido-style")
    
    if result["success"]:
        print("\n‚ú® –°—Ç–∏–ª–µ–≤—ã–µ —Ç–µ–∫—Å—Ç—ã –ñ–≤–∞–Ω–µ—Ü–∫–æ–≥–æ —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã –∏ –≥–æ—Ç–æ–≤—ã –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é!")
        print("üé≠ AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç —Ç–µ–ø–µ—Ä—å –º–æ–∂–µ—Ç –æ—Ç–≤–µ—á–∞—Ç—å –≤ —Å—Ç–∏–ª–µ –≤–µ–ª–∏–∫–æ–≥–æ —Å–∞—Ç–∏—Ä–∏–∫–∞")
        print("üí° –°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ —Å–æ—á–µ—Ç–∞—Ç—å –º—É–¥—Ä–æ—Å—Ç—å –ñ–≤–∞–Ω–µ—Ü–∫–æ–≥–æ —Å —Ñ–∞–∫—Ç–∞–º–∏ –æ —à–∫–æ–ª–µ Ukido")
        print("üîç PDF –∫–æ–Ω—Ç–µ–Ω—Ç –ø—Ä–æ—à–µ–ª –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—É—é —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é –¥–ª—è —à–∫–æ–ª—ã soft skills")
    else:
        print(f"\n‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {result['error']}")

if __name__ == "__main__":
    main()