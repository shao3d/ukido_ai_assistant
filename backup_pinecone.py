#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –∫–æ–ø–∏–∏ –≤—Å–µ—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤ –∏–∑ Pinecone –∏–Ω–¥–µ–∫—Å–∞.
–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤—Å–µ –≤–µ–∫—Ç–æ—Ä—ã —Å –∏—Ö –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ –≤ JSON —Ñ–∞–π–ª —Å timestamp –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏.
"""

import json
import logging
import os
import pinecone
from datetime import datetime
from typing import Dict, List, Any

try:
    from config import config
except ImportError:
    import sys
    sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
    from config import config

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã
PINECONE_INDEX_NAME = "ukido"
BACKUP_DIR = "backups"
BATCH_SIZE = 100  # –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –≤—ã–≥—Ä—É–∑–∫–∏ –≤–µ–∫—Ç–æ—Ä–æ–≤

def ensure_backup_directory():
    """–°–æ–∑–¥–∞–µ—Ç –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –±—ç–∫–∞–ø–æ–≤ –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç."""
    if not os.path.exists(BACKUP_DIR):
        os.makedirs(BACKUP_DIR)
        logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è –±—ç–∫–∞–ø–æ–≤: {BACKUP_DIR}")

def generate_backup_filename() -> str:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∏–º—è —Ñ–∞–π–ª–∞ –±—ç–∫–∞–ø–∞ —Å timestamp."""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"pinecone_backup_{PINECONE_INDEX_NAME}_{timestamp}.json"
    return os.path.join(BACKUP_DIR, filename)

def get_all_vector_ids(index) -> List[str]:
    """
    –ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö ID –≤–µ–∫—Ç–æ—Ä–æ–≤ –≤ –∏–Ω–¥–µ–∫—Å–µ.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç list_paginated –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –≤—Å–µ—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤.
    """
    logger.info("üîç –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –≤—Å–µ—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤...")
    
    all_ids = []
    pagination_token = None
    
    while True:
        try:
            if pagination_token:
                response = index.list_paginated(pagination_token=pagination_token)
            else:
                response = index.list_paginated()
            
            # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ ID –∏–∑ –æ—Ç–≤–µ—Ç–∞
            vector_ids = response.get('vectors', [])
            if vector_ids:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º ID –∏–∑ –≤–µ–∫—Ç–æ—Ä–æ–≤
                ids = [vector['id'] for vector in vector_ids]
                all_ids.extend(ids)
                logger.info(f"üì¶ –ü–æ–ª—É—á–µ–Ω–æ {len(ids)} ID –≤–µ–∫—Ç–æ—Ä–æ–≤ (–≤—Å–µ–≥–æ: {len(all_ids)})")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –µ—â–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            pagination_token = response.get('pagination', {}).get('next')
            if not pagination_token:
                break
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å–ø–∏—Å–∫–∞ –≤–µ–∫—Ç–æ—Ä–æ–≤: {e}")
            # –ü—Ä–æ–±—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥ —á–µ—Ä–µ–∑ describe_index_stats
            stats = index.describe_index_stats()
            total_count = stats.get('total_vector_count', 0)
            logger.warning(f"‚ö†Ô∏è –ü–µ—Ä–µ—Ö–æ–¥ –∫ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–æ–º—É –º–µ—Ç–æ–¥—É. –û–∂–∏–¥–∞–µ—Ç—Å—è ~{total_count} –≤–µ–∫—Ç–æ—Ä–æ–≤")
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º ID –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–∑–≤–µ—Å—Ç–Ω–æ–≥–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞ (–µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å)
            # –î–ª—è —ç—Ç–æ–≥–æ –Ω—É–∂–Ω–æ –∑–Ω–∞—Ç—å –∫–∞–∫ –≥–µ–Ω–µ—Ä–∏—Ä—É—é—Ç—Å—è ID –≤ –≤–∞—à–µ–π —Å–∏—Å—Ç–µ–º–µ
            break
    
    logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(all_ids)} –≤–µ–∫—Ç–æ—Ä–æ–≤ –≤ –∏–Ω–¥–µ–∫—Å–µ")
    return all_ids

def fetch_vectors_batch(index, vector_ids: List[str]) -> List[Dict[str, Any]]:
    """
    –ü–æ–ª—É—á–∞–µ—Ç –≤–µ–∫—Ç–æ—Ä—ã –±–∞—Ç—á–∞–º–∏ —Å –∏—Ö –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏.
    """
    vectors_data = []
    
    for i in range(0, len(vector_ids), BATCH_SIZE):
        batch_ids = vector_ids[i:i + BATCH_SIZE]
        
        try:
            logger.info(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞—Ç—á–∞ {i//BATCH_SIZE + 1}/{(len(vector_ids) + BATCH_SIZE - 1)//BATCH_SIZE} ({len(batch_ids)} –≤–µ–∫—Ç–æ—Ä–æ–≤)")
            
            # –ü–æ–ª—É—á–∞–µ–º –≤–µ–∫—Ç–æ—Ä—ã —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
            response = index.fetch(ids=batch_ids)
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ—Ç–≤–µ—Ç
            for vector_id, vector_data in response.get('vectors', {}).items():
                vector_info = {
                    'id': vector_id,
                    'values': vector_data.get('values', []),
                    'metadata': vector_data.get('metadata', {}),
                    'sparse_values': vector_data.get('sparse_values', {})
                }
                vectors_data.append(vector_info)
            
            logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω –±–∞—Ç—á: {len(response.get('vectors', {}))} –≤–µ–∫—Ç–æ—Ä–æ–≤")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –±–∞—Ç—á–∞ {i//BATCH_SIZE + 1}: {e}")
            continue
    
    return vectors_data

def save_backup_to_json(vectors_data: List[Dict[str, Any]], filename: str):
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä–æ–≤ –≤ JSON —Ñ–∞–π–ª.
    """
    logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –±—ç–∫–∞–ø–∞ –≤ —Ñ–∞–π–ª: {filename}")
    
    backup_data = {
        'backup_info': {
            'timestamp': datetime.now().isoformat(),
            'index_name': PINECONE_INDEX_NAME,
            'total_vectors': len(vectors_data),
            'created_by': 'backup_pinecone.py'
        },
        'vectors': vectors_data
    }
    
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(backup_data, f, ensure_ascii=False, indent=2)
        
        file_size = os.path.getsize(filename) / (1024 * 1024)  # –†–∞–∑–º–µ—Ä –≤ MB
        logger.info(f"‚úÖ –ë—ç–∫–∞–ø —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {filename} ({file_size:.2f} MB)")
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞: {e}")
        return False

def verify_backup(filename: str, expected_count: int) -> bool:
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å —Å–æ–∑–¥–∞–Ω–Ω–æ–≥–æ –±—ç–∫–∞–ø–∞.
    """
    logger.info(f"üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏ –±—ç–∫–∞–ø–∞...")
    
    try:
        with open(filename, 'r', encoding='utf-8') as f:
            backup_data = json.load(f)
        
        saved_count = len(backup_data.get('vectors', []))
        backup_info = backup_data.get('backup_info', {})
        
        logger.info(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –±—ç–∫–∞–ø–∞:")
        logger.info(f"  ‚Ä¢ –ò–Ω–¥–µ–∫—Å: {backup_info.get('index_name', 'N/A')}")
        logger.info(f"  ‚Ä¢ –í—Ä–µ–º—è —Å–æ–∑–¥–∞–Ω–∏—è: {backup_info.get('timestamp', 'N/A')}")
        logger.info(f"  ‚Ä¢ –û–∂–∏–¥–∞–ª–æ—Å—å –≤–µ–∫—Ç–æ—Ä–æ–≤: {expected_count}")
        logger.info(f"  ‚Ä¢ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤–µ–∫—Ç–æ—Ä–æ–≤: {saved_count}")
        
        if saved_count == expected_count:
            logger.info("‚úÖ –ë—ç–∫–∞–ø –ø—Ä–æ—à–µ–ª –ø—Ä–æ–≤–µ—Ä–∫—É —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏")
            return True
        else:
            logger.warning(f"‚ö†Ô∏è –ù–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –≤–µ–∫—Ç–æ—Ä–æ–≤: –æ–∂–∏–¥–∞–ª–æ—Å—å {expected_count}, —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ {saved_count}")
            return False
            
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –±—ç–∫–∞–ø–∞: {e}")
        return False

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –±—ç–∫–∞–ø–∞."""
    logger.info("üöÄ –ó–∞–ø—É—Å–∫ —Å–æ–∑–¥–∞–Ω–∏—è —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –∫–æ–ø–∏–∏ Pinecone –∏–Ω–¥–µ–∫—Å–∞...")
    
    try:
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –±—ç–∫–∞–ø–æ–≤
        ensure_backup_directory()
        
        # –ü–æ–¥–∫–ª—é—á–∞–µ–º—Å—è –∫ Pinecone
        logger.info("üîå –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Pinecone...")
        pc = pinecone.Pinecone(api_key=config.PINECONE_API_KEY)
        index = pc.Index(PINECONE_INDEX_NAME)
        
        # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏–Ω–¥–µ–∫—Å–∞
        stats = index.describe_index_stats()
        total_vectors = stats.get('total_vector_count', 0)
        logger.info(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏–Ω–¥–µ–∫—Å–∞ '{PINECONE_INDEX_NAME}':")
        logger.info(f"  ‚Ä¢ –í—Å–µ–≥–æ –≤–µ–∫—Ç–æ—Ä–æ–≤: {total_vectors}")
        logger.info(f"  ‚Ä¢ –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {stats.get('dimension', 'N/A')}")
        
        if total_vectors == 0:
            logger.warning("‚ö†Ô∏è –ò–Ω–¥–µ–∫—Å –ø—É—Å—Ç–æ–π, –Ω–µ—á–µ–≥–æ –±—ç–∫–∞–ø–∏—Ç—å")
            return
        
        # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤
        vector_ids = get_all_vector_ids(index)
        
        if not vector_ids:
            logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –≤–µ–∫—Ç–æ—Ä–æ–≤")
            return
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ –≤–µ–∫—Ç–æ—Ä—ã
        logger.info(f"üì• –ù–∞—á–∞–ª–æ –∑–∞–≥—Ä—É–∑–∫–∏ {len(vector_ids)} –≤–µ–∫—Ç–æ—Ä–æ–≤...")
        vectors_data = fetch_vectors_batch(index, vector_ids)
        
        if not vectors_data:
            logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –≤–µ–∫—Ç–æ—Ä—ã")
            return
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–º—è —Ñ–∞–π–ª–∞ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º
        backup_filename = generate_backup_filename()
        success = save_backup_to_json(vectors_data, backup_filename)
        
        if success:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å
            verify_backup(backup_filename, len(vector_ids))
            logger.info(f"üéâ –ë—ç–∫–∞–ø —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω: {backup_filename}")
        else:
            logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –±—ç–∫–∞–ø")
            
    except Exception as e:
        logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        raise

if __name__ == "__main__":
    main()