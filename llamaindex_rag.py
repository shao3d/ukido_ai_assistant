# llamaindex_rag.py (–í–µ—Ä—Å–∏—è –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è gpt-4o-mini —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–º –∏–º–ø–æ—Ä—Ç–æ–º)
"""
–ú–æ–¥—É–ª—å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å RAG —Å–∏—Å—Ç–µ–º–æ–π –Ω–∞ –±–∞–∑–µ LlamaIndex.
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏ (app.py) –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π.
–í–∫–ª—é—á–∞–µ—Ç —Ä–µ—Ä–∞–Ω–∫–µ—Ä –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏.
"""
import logging
import time
from typing import Tuple, Dict, Any

import pinecone
from llama_index.core import VectorStoreIndex, Settings
from llama_index.vector_stores.pinecone import PineconeVectorStore
from llama_index.llms.openrouter import OpenRouter
from llama_index.embeddings.gemini import GeminiEmbedding
# --- –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô –ò–ú–ü–û–†–¢ ---
from llama_index.postprocessor.sbert_rerank import SentenceTransformerRerank

# –ü–æ–ø—ã—Ç–∫–∞ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å 'config' –∏–∑ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
try:
    from config import config
except ImportError:
    import os
    import sys
    sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
    from config import config


class LlamaIndexRAG:
    """
    –ö–ª–∞—Å—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å RAG —Å–∏—Å—Ç–µ–º–æ–π –Ω–∞ –±–∞–∑–µ LlamaIndex —Å —Ä–µ—Ä–∞–Ω–∫–µ—Ä–æ–º.
    """
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.pinecone_index_name = "ukido"
        self.query_engine = None

        try:
            # 1. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –º–æ–¥–µ–ª–µ–π
            Settings.embed_model = GeminiEmbedding(
                model_name=config.EMBEDDING_MODEL, api_key=config.GEMINI_API_KEY
            )
            Settings.llm = OpenRouter(
                api_key=config.OPENROUTER_API_KEY,
                model="openai/gpt-4o-mini",
                max_tokens=2048,
                temperature=0.2,
            )

            # 2. –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Pinecone
            pc = pinecone.Pinecone(api_key=config.PINECONE_API_KEY)
            pinecone_index = pc.Index(self.pinecone_index_name)

            # 3. –°–æ–∑–¥–∞–Ω–∏–µ VectorStore
            vector_store = PineconeVectorStore(pinecone_index=pinecone_index)
            index = VectorStoreIndex.from_vector_store(vector_store=vector_store)

            # 4. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ä–µ—Ä–∞–Ω–∫–µ—Ä–∞
            # üî• –ò–°–ü–û–õ–¨–ó–£–ï–ú –ö–õ–ê–°–° –ò–ó –ü–†–ê–í–ò–õ–¨–ù–û–ì–û –ò–ú–ü–û–†–¢–ê
            reranker = SentenceTransformerRerank(
                model="cross-encoder/ms-marco-MiniLM-L-2-v2", # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å
                top_n=4
            )

            # 5. –°–æ–∑–¥–∞–Ω–∏–µ Query Engine
            self.query_engine = index.as_query_engine(
                similarity_top_k=15,
                node_postprocessors=[reranker]
            )

            self.logger.info("‚úÖ LlamaIndex RAG —Å–∏—Å—Ç–µ–º–∞ —Å —Ä–µ—Ä–∞–Ω–∫–µ—Ä–æ–º —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ (–º–æ–¥–µ–ª—å: gpt-4o-mini)")

        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ LlamaIndex RAG: {e}", exc_info=True)
            raise

    def search_knowledge_base(self, query: str) -> Tuple[str, Dict[str, Any]]:
        """
        –ü–æ–∏—Å–∫ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º LlamaIndex.
        """
        search_start = time.time()
        if not self.query_engine:
            self.logger.error("LlamaIndex query engine –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω.")
            return "–û—à–∏–±–∫–∞: LlamaIndex RAG –Ω–µ –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ.", {}

        try:
            self.logger.info(f"üîç LlamaIndex RAG: –ü–æ–∏—Å–∫ –ø–æ –∑–∞–ø—Ä–æ—Å—É: '{query}'")
            response = self.query_engine.query(query)

            context_chunks = [node.get_content() for node in response.source_nodes]
            context = "\n\n".join(context_chunks)

            search_time = time.time() - search_start
            scores = [node.get_score() for node in response.source_nodes]
            average_score = sum(scores) / len(scores) if scores else 0.0

            metrics = {
                'search_time': search_time,
                'chunks_found': len(context_chunks),
                'average_score': average_score,
                'max_score': max(scores) if scores else 0.0,
            }

            self.logger.info(f"‚úÖ LlamaIndex RAG: –ù–∞–π–¥–µ–Ω–æ {metrics['chunks_found']} —á–∞–Ω–∫–æ–≤ –∑–∞ {search_time:.2f}—Å")
            return context, metrics

        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤–æ –≤—Ä–µ–º—è –ø–æ–∏—Å–∫–∞ LlamaIndex RAG: {e}", exc_info=True)
            return f"–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏: {e}", {}

# –°–æ–∑–¥–∞–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä
try:
    llama_index_rag = LlamaIndexRAG()
except Exception as e:
    llama_index_rag = None
    logging.getLogger(__name__).critical(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å LlamaIndexRAG: {e}")
