# llamaindex_rag.py
"""
‚úÖ –§–ò–ù–ê–õ–¨–ù–ê–Ø –í–ï–†–°–ò–Ø v3: RAG-—Å–∏—Å—Ç–µ–º–∞ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –ú–∞—à–∏–Ω—ã –°–æ—Å—Ç–æ—è–Ω–∏–π.
- –ü—Ä–∏–Ω–∏–º–∞–µ—Ç 'current_state' –∏ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç.
- –ò–Ω—Å—Ç—Ä—É–∫—Ç–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç–æ–∫–µ–Ω [ACTION:SEND_LESSON_LINK] –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–∏ 'closing'.
"""
import logging
import time
from typing import Tuple, Dict, Any, List

import pinecone
from llama_index.core import VectorStoreIndex, Settings
from llama_index.vector_stores.pinecone import PineconeVectorStore
from llama_index.llms.openrouter import OpenRouter
from llama_index.embeddings.gemini import GeminiEmbedding
from llama_index.core.chat_engine import ContextChatEngine
from llama_index.postprocessor.sbert_rerank import SentenceTransformerRerank
from llama_index.core.llms import ChatMessage, MessageRole
from llama_index.core.memory import ChatMemoryBuffer

try:
    from rag_debug_logger import rag_debug
except ImportError:
    class DummyDebug:
        def log_enricher_input(self, *args): pass
        def log_enricher_prompt(self, *args): pass
        def log_enricher_output(self, *args): pass
        def log_retrieval_results(self, *args): pass
    rag_debug = DummyDebug()

try:
    from config import config
except ImportError:
    import os, sys
    sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
    from config import config

class LlamaIndexRAG:
    """
    ‚úÖ –§–ò–ù–ê–õ–¨–ù–ê–Ø –í–ï–†–°–ò–Ø v3: RAG —Å–∏—Å—Ç–µ–º–∞ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Å–æ—Å—Ç–æ—è–Ω–∏–π –¥–∏–∞–ª–æ–≥–∞.
    """
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.pinecone_index_name = "ukido"
        self.chat_engine = None

        try:
            Settings.embed_model = GeminiEmbedding(
                model_name=config.EMBEDDING_MODEL, 
                api_key=config.GEMINI_API_KEY
            )
            Settings.llm = OpenRouter(
                api_key=config.OPENROUTER_API_KEY, 
                model="openai/gpt-4o-mini",
                temperature=0.7,
                max_tokens=1024
            )

            pc = pinecone.Pinecone(api_key=config.PINECONE_API_KEY)
            pinecone_index = pc.Index(self.pinecone_index_name)
            vector_store = PineconeVectorStore(pinecone_index=pinecone_index)
            index = VectorStoreIndex.from_vector_store(vector_store=vector_store)

            reranker = SentenceTransformerRerank(
                model="cross-encoder/ms-marco-MiniLM-L-2-v2", 
                top_n=4
            )
            
            memory = ChatMemoryBuffer.from_defaults(token_limit=16384)

            # –í–ê–ñ–ù–û: –ú—ã —Å–æ–∑–¥–∞–µ–º –¥–≤–∏–∂–æ–∫ —Å –±–∞–∑–æ–≤—ã–º –ø—Ä–æ–º–ø—Ç–æ–º. –û–Ω –±—É–¥–µ—Ç –æ–±–Ω–æ–≤–ª—è—Ç—å—Å—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏.
            self.chat_engine = ContextChatEngine.from_defaults(
                retriever=index.as_retriever(
                    similarity_top_k=15,
                    node_postprocessors=[reranker]
                ),
                llm=Settings.llm,
                memory=memory,
            )

            self.logger.info("‚úÖ LlamaIndex ChatEngine (v3, —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Å–æ—Å—Ç–æ—è–Ω–∏–π) –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")

        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ LlamaIndexRAG: {e}", exc_info=True)
            raise

    def _build_dynamic_system_prompt(self, current_state: str) -> str:
        """–°–æ–±–∏—Ä–∞–µ—Ç —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—É—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è –¥–∏–∞–ª–æ–≥–∞."""
        
        base_prompt = """–¢—ã ‚Äî AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –æ–Ω–ª–∞–π–Ω-—à–∫–æ–ª—ã Ukido.
–¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –æ—Ç–≤–µ—á–∞—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã, –∏—Å–ø–æ–ª—å–∑—É—è –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π –Ω–∏–∂–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π.
- –ë—É–¥—å –∫—Ä–∞—Ç–∫–∏–º –∏ –æ—Ç–≤–µ—á–∞–π –ø–æ —Å—É—â–µ—Å—Ç–≤—É.
- –ù–ï –≤—ã–¥—É–º—ã–≤–∞–π —Ñ–∞–∫—Ç—ã.
- –ï—Å–ª–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –Ω–µ—Ç –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å, —á–µ—Å—Ç–Ω–æ —Å–∫–∞–∂–∏: "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, –≤ –º–æ–µ–π –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø–æ —ç—Ç–æ–º—É –≤–æ–ø—Ä–æ—Å—É."
"""

        state_instructions = {
            'greeting': "–≠—Ç–æ –Ω–∞—á–∞–ª–æ –¥–∏–∞–ª–æ–≥–∞. –ù–∞—á–Ω–∏ —Å –∫–æ—Ä–æ—Ç–∫–æ–≥–æ –¥—Ä—É–∂–µ–ª—é–±–Ω–æ–≥–æ –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏—è.",
            'fact_finding': "–°–æ—Å—Ä–µ–¥–æ—Ç–æ—á—å—Å—è –Ω–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–∏ —Ç–æ—á–Ω—ã—Ö —Ñ–∞–∫—Ç–æ–≤ –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. –ë—É–¥—å –∫—Ä–∞—Ç–∫–∏–º –∏ —á–µ—Ç–∫–∏–º.",
            'problem_solving': "–ü—Ä–æ—è–≤–∏ —ç–º–ø–∞—Ç–∏—é –∫ –ø—Ä–æ–±–ª–µ–º–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è. –ò—Å–ø–æ–ª—å–∑—É–π –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Ñ–∞–∫—Ç—ã, —á—Ç–æ–±—ã –ø—Ä–µ–¥–ª–æ–∂–∏—Ç—å —Ä–µ—à–µ–Ω–∏–µ –∏–ª–∏ —Å–æ–≤–µ—Ç.",
            'closing': '–í–ê–ñ–ù–û: –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –≤—ã—Ä–∞–∂–∞–µ—Ç –∂–µ–ª–∞–Ω–∏–µ –∑–∞–ø–∏—Å–∞—Ç—å—Å—è –Ω–∞ —É—Ä–æ–∫. –¢–≤–æ—è –≥–ª–∞–≤–Ω–∞—è —Ü–µ–ª—å ‚Äî –ø–æ–º–æ—á—å –µ–º—É –∏ –ø–æ–¥—Ç–≤–µ—Ä–¥–∏—Ç—å –∑–∞–ø–∏—Å—å. –í –∫–æ–Ω—Ü–µ –æ—Ç–≤–µ—Ç–∞ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –¥–æ–±–∞–≤—å —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π —Ç–æ–∫–µ–Ω [ACTION:SEND_LESSON_LINK].'
        }
        
        instruction = state_instructions.get(current_state, state_instructions['fact_finding'])
        
        return f"{base_prompt}\n–ò–ù–°–¢–†–£–ö–¶–ò–Ø –ü–û –°–ò–¢–£–ê–¶–ò–ò: {instruction}"


    def _prepare_chat_history(self, conversation_history: List[str] = None) -> List[ChatMessage]:
        if not conversation_history: return []
        smart_history = conversation_history[-4:]
        chat_messages = []
        for i, msg in enumerate(smart_history):
            try:
                role_str, content = msg.split(': ', 1)
                role = MessageRole.ASSISTANT if "–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç" in role_str.lower() else MessageRole.USER
                chat_messages.append(ChatMessage(role=role, content=content))
            except ValueError:
                self.logger.warning(f"–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç —Å–æ–æ–±—â–µ–Ω–∏—è –≤ –∏—Å—Ç–æ—Ä–∏–∏: '{msg}'")
                continue
        return chat_messages

    def search_and_answer(self, query: str, conversation_history: List[str] = None, current_state: str = 'fact_finding') -> Tuple[str, Dict[str, Any]]:
        """
        ‚úÖ –ì–õ–ê–í–ù–´–ô –ú–ï–¢–û–î v3: –ü—Ä–∏–Ω–∏–º–∞–µ—Ç —Å–æ—Å—Ç–æ—è–Ω–∏–µ –∏ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ —É–ø—Ä–∞–≤–ª—è–µ—Ç –ø–æ–≤–µ–¥–µ–Ω–∏–µ–º –º–æ–¥–µ–ª–∏.
        """
        search_start = time.time()
        
        if not self.chat_engine:
            self.logger.error("ChatEngine –Ω–µ –≥–æ—Ç–æ–≤")
            return "–û—à–∏–±–∫–∞: RAG-—Å–∏—Å—Ç–µ–º–∞ –Ω–µ –≥–æ—Ç–æ–≤–∞.", {}

        try:
            # 1. –°–æ–±–∏—Ä–∞–µ–º –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –ø—Ä–æ–º–ø—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è
            system_prompt = self._build_dynamic_system_prompt(current_state)
            
            # 2. –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–º–ø—Ç –≤ –¥–≤–∏–∂–∫–µ –ü–ï–†–ï–î –≤—ã–∑–æ–≤–æ–º
            self.chat_engine.update_prompts({"system_prompt": system_prompt})
            
            chat_history = self._prepare_chat_history(conversation_history)
            history_len = len(chat_history)
            
            self.logger.info(f"üîç –ó–∞–ø—Ä–æ—Å –≤ LlamaIndex: '{query}' | –°–æ—Å—Ç–æ—è–Ω–∏–µ: {current_state} | –ò—Å—Ç–æ—Ä–∏—è: {history_len}")
            rag_debug.log_enricher_prompt(f"DYNAMIC SYSTEM PROMPT:\n{system_prompt}")
            
            response = self.chat_engine.chat(query, chat_history=chat_history)
            
            final_answer = response.response
            search_time = time.time() - search_start
            
            source_nodes = response.source_nodes or []
            context_chunks = [node.get_content() for node in source_nodes]
            scores = [getattr(node, 'score', 0.5) for node in source_nodes]
            average_score = sum(scores) / len(scores) if scores else 0.0

            rag_debug.log_retrieval_results(chunks=context_chunks, scores=scores, time_taken=search_time, total_before_rerank=15)

            metrics = {'search_time': search_time, 'chunks_found': len(context_chunks), 'average_score': average_score, 'max_score': max(scores) if scores else 0.0, 'history_used': history_len}

            self.logger.info(f"‚úÖ LlamaIndex —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–ª –æ—Ç–≤–µ—Ç –∑–∞ {search_time:.2f}s (—Å–æ—Å—Ç–æ—è–Ω–∏–µ: {current_state})")
            return final_answer, metrics

        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ RAG search_and_answer: {e}", exc_info=True)
            return "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –≤–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.", {}


# –ì–ª–æ–±–∞–ª—å–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
try:
    llama_index_rag = LlamaIndexRAG()
except Exception as e:
    llama_index_rag = None
    logging.getLogger(__name__).error(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å LlamaIndexRAG: {e}")