# llamaindex_rag.py
"""
‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–û: LlamaIndex RAG —Å–∏—Å—Ç–µ–º–∞ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –∏—Å—Ç–æ—Ä–∏–∏ –¥–∏–∞–ª–æ–≥–∞
"""
import logging
import time
from typing import Tuple, Dict, Any, List

import pinecone
from llama_index.core import VectorStoreIndex, Settings
from llama_index.vector_stores.pinecone import PineconeVectorStore
from llama_index.llms.openrouter import OpenRouter
from llama_index.embeddings.gemini import GeminiEmbedding
from llama_index.core.chat_engine import ContextChatEngine
from llama_index.postprocessor.sbert_rerank import SentenceTransformerRerank

# –ò–º–ø–æ—Ä—Ç debug –ª–æ–≥–≥–µ—Ä–∞
try:
    from rag_debug_logger import rag_debug
except ImportError:
    # Fallback –µ—Å–ª–∏ debug –ª–æ–≥–≥–µ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
    class DummyDebug:
        def log_enricher_input(self, *args): pass
        def log_enricher_prompt(self, *args): pass
        def log_enricher_output(self, *args): pass
        def log_retrieval_results(self, *args): pass
    rag_debug = DummyDebug()

try:
    from config import config
except ImportError:
    import os
    import sys
    sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
    from config import config

class LlamaIndexRAG:
    """
    ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–û: RAG —Å–∏—Å—Ç–µ–º–∞ —Å ChatEngine –∏ –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –∏—Å—Ç–æ—Ä–∏–∏ –¥–∏–∞–ª–æ–≥–∞.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç GPT-4o mini –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –Ω–µ–ø–æ–ª–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤.
    """
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.pinecone_index_name = "ukido"
        self.chat_engine = None

        try:
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –º–æ–¥–µ–ª–µ–π
            Settings.embed_model = GeminiEmbedding(
                model_name=config.EMBEDDING_MODEL, 
                api_key=config.GEMINI_API_KEY
            )
            Settings.llm = OpenRouter(
                api_key=config.OPENROUTER_API_KEY, 
                model="openai/gpt-4o-mini"
            )

            # –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Pinecone
            pc = pinecone.Pinecone(api_key=config.PINECONE_API_KEY)
            pinecone_index = pc.Index(self.pinecone_index_name)
            vector_store = PineconeVectorStore(pinecone_index=pinecone_index)
            index = VectorStoreIndex.from_vector_store(vector_store=vector_store)

            # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ä–µ—Ä–∞–Ω–∫–µ—Ä–∞
            reranker = SentenceTransformerRerank(
                model="cross-encoder/ms-marco-MiniLM-L-2-v2", 
                top_n=4
            )

            # –°–æ–∑–¥–∞–Ω–∏–µ ChatEngine
            self.chat_engine = ContextChatEngine.from_defaults(
                retriever=index.as_retriever(
                    similarity_top_k=15,
                    node_postprocessors=[reranker]
                ),
                llm=Settings.llm,
                system_prompt="""–¢—ã –ø–æ–º–æ—â–Ω–∏–∫ –ø–æ –ø–æ–∏—Å–∫—É –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π —à–∫–æ–ª—ã Ukido. 

–ï—Å–ª–∏ —Å–ø—Ä–∞—à–∏–≤–∞—é—Ç "–°–∫–æ–ª—å–∫–æ —Å—Ç–æ–∏—Ç?" - –∏—â–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –ø—Ä–æ —Å—Ç–æ–∏–º–æ—Å—Ç—å –∫—É—Ä—Å–æ–≤.
–ï—Å–ª–∏ —Å–ø—Ä–∞—à–∏–≤–∞—é—Ç "–ß—Ç–æ –Ω—É–∂–Ω–æ?" - –¥–æ–±–∞–≤—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –æ —á–µ–º —Ä–µ—á—å.
–ï—Å–ª–∏ —Å–ø—Ä–∞—à–∏–≤–∞—é—Ç "–ö–æ–≥–¥–∞?" - —É—Ç–æ—á–Ω–∏ –ø—Ä–æ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ.

–í–æ–∑–≤—Ä–∞—â–∞–π —Ç–æ–ª—å–∫–æ –Ω–∞–π–¥–µ–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –Ω–µ –¥–æ–±–∞–≤–ª—è–π –æ—Ç —Å–µ–±—è."""
            )

            self.logger.info("‚úÖ LlamaIndex ChatEngine –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")

        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏: {e}")
            raise

    def _prepare_smart_history(self, conversation_history: List[str] = None) -> List[str]:
        """
        –£–º–Ω–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∏—Å—Ç–æ—Ä–∏–∏ –¥–∏–∞–ª–æ–≥–∞ - –º–∞–∫—Å–∏–º—É–º 4 —Å–æ–æ–±—â–µ–Ω–∏—è
        """
        if not conversation_history or len(conversation_history) == 0:
            return []  # –ü–µ—Ä–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
        
        # –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –∏—Å—Ç–æ—Ä–∏—è: –º–∞–∫—Å–∏–º—É–º 4 —Å–æ–æ–±—â–µ–Ω–∏—è (2 –ø–∞—Ä—ã –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç)
        return conversation_history[-4:]

    def search_knowledge_base(self, query: str, conversation_history: List[str] = None) -> Tuple[str, Dict[str, Any]]:
        """
        ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–û: –ü–æ–∏—Å–∫ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –∏—Å—Ç–æ—Ä–∏–∏ –¥–∏–∞–ª–æ–≥–∞
        """
        search_start = time.time()
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —É–º–Ω—É—é –∏—Å—Ç–æ—Ä–∏—é
        smart_history = self._prepare_smart_history(conversation_history)
        
        # Debug –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –†–ï–ê–õ–¨–ù–û–ô –∏—Å—Ç–æ—Ä–∏–µ–π
        rag_debug.log_enricher_input(query, smart_history)
        
        if not self.chat_engine:
            self.logger.error("ChatEngine –Ω–µ –≥–æ—Ç–æ–≤")
            return "–û—à–∏–±–∫–∞: —Å–∏—Å—Ç–µ–º–∞ –Ω–µ –≥–æ—Ç–æ–≤–∞", {}

        try:
            self.logger.info(f"üîç –ü–æ–∏—Å–∫: '{query}' | –ò—Å—Ç–æ—Ä–∏—è: {len(smart_history)} —Å–æ–æ–±—â–µ–Ω–∏–π")
            
            # –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
            system_prompt = "–¢—ã –ø–æ–º–æ—â–Ω–∏–∫ –ø–æ –ø–æ–∏—Å–∫—É –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π —à–∫–æ–ª—ã Ukido..."
            
            # –û–±–æ–≥–∞—â–∞–µ–º –∑–∞–ø—Ä–æ—Å –∏—Å—Ç–æ—Ä–∏–µ–π –µ—Å–ª–∏ –æ–Ω–∞ –µ—Å—Ç—å
            enriched_query = query
            if smart_history:
                history_context = "\n".join(smart_history[-2:])  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 2 —Å–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
                enriched_query = f"–ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–∏–∞–ª–æ–≥–∞: {history_context}\n\n–í–æ–ø—Ä–æ—Å: {query}"
            
            # –õ–æ–≥–∏—Ä—É–µ–º –æ–±–æ–≥–∞—â–µ–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç
            rag_debug.log_enricher_prompt(f"SYSTEM: {system_prompt}\nENRICHED QUERY: {enriched_query}")
            
            # ChatEngine –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–±–æ–≥–∞—â–µ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å
            enrichment_start = time.time()
            response = self.chat_engine.chat(enriched_query)
            enrichment_time = time.time() - enrichment_start
            
            # Debug –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            rag_debug.log_enricher_output(f"ChatEngine –æ–±—Ä–∞–±–æ—Ç–∞–ª –∑–∞–ø—Ä–æ—Å —Å –∏—Å—Ç–æ—Ä–∏–µ–π ({len(smart_history)} msg)", enrichment_time)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —á–∞–Ω–∫–∏
            context_chunks = []
            scores = []
            
            if hasattr(response, 'source_nodes') and response.source_nodes:
                context_chunks = [node.get_content() for node in response.source_nodes]
                scores = [getattr(node, 'score', 0.5) for node in response.source_nodes]
            
            # Fallback –µ—Å–ª–∏ —á–∞–Ω–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã
            if not context_chunks and hasattr(response, 'response'):
                context_chunks = [str(response.response)]
                scores = [0.7]
            
            context = "\n\n".join(context_chunks) if context_chunks else "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞"
            search_time = time.time() - search_start
            
            # –î–æ–ø–æ–ª–Ω—è–µ–º scores –¥–æ –Ω—É–∂–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞
            while len(scores) < len(context_chunks):
                scores.append(0.5)
            
            # Debug –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞
            rag_debug.log_retrieval_results(
                chunks=context_chunks,
                scores=scores[:len(context_chunks)],
                time_taken=search_time,
                total_before_rerank=15
            )

            # –ú–µ—Ç—Ä–∏–∫–∏
            average_score = sum(scores) / len(scores) if scores else 0.5
            metrics = {
                'search_time': search_time,
                'chunks_found': len(context_chunks),
                'average_score': average_score,
                'max_score': max(scores) if scores else 0.5,
                'history_used': len(smart_history)  # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∏—Å—Ç–æ—Ä–∏–∏
            }

            self.logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(context_chunks)} —á–∞–Ω–∫–æ–≤ –∑–∞ {search_time:.2f}s (–∏—Å—Ç–æ—Ä–∏—è: {len(smart_history)})")
            return context, metrics

        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
            return "–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏", {}

# –ì–ª–æ–±–∞–ª—å–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
try:
    llama_index_rag = LlamaIndexRAG()
except Exception as e:
    llama_index_rag = None
    logging.getLogger(__name__).error(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å LlamaIndexRAG: {e}")