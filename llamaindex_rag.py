# llamaindex_rag.py (–§–ò–ù–ê–õ–¨–ù–ê–Ø –í–ï–†–°–ò–Ø —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –ª–∏–º–∏—Ç–æ–≤ API)
import logging
import time
from typing import Tuple, Dict, Any

import pinecone
from llama_index.core import VectorStoreIndex, Settings
from llama_index.vector_stores.pinecone import PineconeVectorStore
from llama_index.llms.gemini import Gemini
from llama_index.embeddings.gemini import GeminiEmbedding
from llama_index.postprocessor.sbert_rerank import SentenceTransformerRerank

# --- –ù–û–í–´–ô –ë–õ–û–ö: –ò–º–ø–æ—Ä—Ç—ã –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ª–∏–º–∏—Ç–æ–≤ ---
from tenacity import retry, stop_after_attempt, wait_exponential
from google.api_core.exceptions import ResourceExhausted
# -------------------------------------------------

try:
    from config import config
except ImportError:
    import os
    import sys
    sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
    from config import config

class LlamaIndexRAG:
    """
    –û—Ç–∫–∞–∑–æ—É—Å—Ç–æ–π—á–∏–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å RAG —Å–∏—Å—Ç–µ–º–æ–π, –∫–æ—Ç–æ—Ä—ã–π —É–º–µ–µ—Ç
    –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –ª–∏–º–∏—Ç—ã API Gemini.
    """
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.pinecone_index_name = "ukido"
        self.query_engine = None

        try:
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –º–æ–¥–µ–ª–µ–π
            Settings.embed_model = GeminiEmbedding(model_name=config.EMBEDDING_MODEL, api_key=config.GEMINI_API_KEY)
            Settings.llm = Gemini(model_name="models/gemini-1.5-pro-latest", api_key=config.GEMINI_API_KEY)

            # –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Pinecone
            pc = pinecone.Pinecone(api_key=config.PINECONE_API_KEY)
            pinecone_index = pc.Index(self.pinecone_index_name)
            vector_store = PineconeVectorStore(pinecone_index=pinecone_index)
            index = VectorStoreIndex.from_vector_store(vector_store=vector_store)

            # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ä–µ—Ä–∞–Ω–∫–µ—Ä–∞
            reranker = SentenceTransformerRerank(model="cross-encoder/ms-marco-MiniLM-L-2-v2", top_n=4)

            # –°–æ–∑–¥–∞–Ω–∏–µ Query Engine
            self.query_engine = index.as_query_engine(
                similarity_top_k=15,
                node_postprocessors=[reranker]
            )

            self.logger.info("‚úÖ LlamaIndex RAG —Å–∏—Å—Ç–µ–º–∞ —Å —Ä–µ—Ä–∞–Ω–∫–µ—Ä–æ–º —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ (–º–æ–¥–µ–ª—å: Gemini 1.5 Pro)")

        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ LlamaIndex RAG: {e}", exc_info=True)
            raise

    # --- üî• –ö–õ–Æ–ß–ï–í–û–ï –ò–ó–ú–ï–ù–ï–ù–ò–ï: –î–µ–∫–æ—Ä–∞—Ç–æ—Ä –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ª–∏–º–∏—Ç–æ–≤ üî• ---
    @retry(
        wait=wait_exponential(multiplier=2, min=5, max=30), # –ñ–¥–µ–º 5—Å, –ø–æ—Ç–æ–º 10—Å, –ø–æ—Ç–æ–º 20—Å, –º–∞–∫—Å 30—Å
        stop=stop_after_attempt(4), # –î–µ–ª–∞–µ–º 4 –ø–æ–ø—ã—Ç–∫–∏
        retry_error_callback=lambda state: logging.warning(f"–î–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç API Gemini. –ü–æ–ø—ã—Ç–∫–∞ #{state.attempt_number}, –∂–¥–µ–º..."),
        retry=retry_if_exception_type(ResourceExhausted) # –ü–æ–≤—Ç–æ—Ä—è–µ–º —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –æ—à–∏–±–∫–µ –ª–∏–º–∏—Ç–∞
    )
    def search_knowledge_base(self, query: str) -> Tuple[str, Dict[str, Any]]:
        """
        –ü–æ–∏—Å–∫ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –ª–∏–º–∏—Ç–æ–≤ API.
        """
        search_start = time.time()
        if not self.query_engine:
            self.logger.error("LlamaIndex query engine –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω.")
            return "–û—à–∏–±–∫–∞: LlamaIndex RAG –Ω–µ –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ.", {}

        try:
            self.logger.info(f"üîç LlamaIndex RAG: –ü–æ–∏—Å–∫ –ø–æ –∑–∞–ø—Ä–æ—Å—É: '{query}'")
            # –≠—Ç–æ—Ç –≤—ã–∑–æ–≤ —Ç–µ–ø–µ—Ä—å –∑–∞—â–∏—â–µ–Ω –¥–µ–∫–æ—Ä–∞—Ç–æ—Ä–æ–º @retry
            response = self.query_engine.query(query)

            context_chunks = [node.get_content() for node in response.source_nodes]
            context = "\n\n".join(context_chunks)

            search_time = time.time() - search_start
            scores = [node.get_score() for node in response.source_nodes]
            average_score = sum(scores) / len(scores) if scores else 0.0

            metrics = {
                'search_time': search_time,
                'chunks_found': len(context_chunks),
                'average_score': average_score,
                'max_score': max(scores) if scores else 0.0,
            }

            self.logger.info(f"‚úÖ LlamaIndex RAG: –ù–∞–π–¥–µ–Ω–æ {metrics['chunks_found']} —á–∞–Ω–∫–æ–≤ –∑–∞ {search_time:.2f}—Å")
            return context, metrics

        except ResourceExhausted as e:
            # –≠—Ç–æ—Ç –±–ª–æ–∫ —Å—Ä–∞–±–æ—Ç–∞–µ—Ç, –µ—Å–ª–∏ –≤—Å–µ 4 –ø–æ–ø—ã—Ç–∫–∏ –Ω–µ —É–≤–µ–Ω—á–∞–ª–∏—Å—å —É—Å–ø–µ—Ö–æ–º
            self.logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–ø–æ–ª–Ω–∏—Ç—å –∑–∞–ø—Ä–æ—Å –∫ Gemini –ø–æ—Å–ª–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø–æ–ø—ã—Ç–æ–∫: {e}")
            return "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, —Å–µ—Ä–≤–µ—Ä AI —Å–µ–π—á–∞—Å –ø–µ—Ä–µ–≥—Ä—É–∂–µ–Ω. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–≤—Ç–æ—Ä–∏—Ç–µ –≤–∞—à –∑–∞–ø—Ä–æ—Å —á–µ—Ä–µ–∑ –º–∏–Ω—É—Ç—É.", {}
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤–æ –≤—Ä–µ–º—è –ø–æ–∏—Å–∫–∞ LlamaIndex RAG: {e}", exc_info=True)
            return f"–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –≤–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.", {}

# –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –¥–µ–∫–æ—Ä–∞—Ç–æ—Ä–∞
def retry_if_exception_type(exception_type):
    return lambda e: isinstance(e, exception_type)

try:
    llama_index_rag = LlamaIndexRAG()
except Exception as e:
    llama_index_rag = None
    logging.getLogger(__name__).critical(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å LlamaIndexRAG: {e}")
