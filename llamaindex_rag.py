# llamaindex_rag.py
"""
–ü—Ä–æ—Å—Ç–∞—è LlamaIndex RAG —Å–∏—Å—Ç–µ–º–∞ —Å ChatEngine –¥–ª—è —É–º–Ω–æ–≥–æ –æ–±–æ–≥–∞—â–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
"""
import logging
import time
from typing import Tuple, Dict, Any

import pinecone
from llama_index.core import VectorStoreIndex, Settings
from llama_index.vector_stores.pinecone import PineconeVectorStore
from llama_index.llms.openrouter import OpenRouter
from llama_index.embeddings.gemini import GeminiEmbedding
from llama_index.core.chat_engine import ContextChatEngine
from llama_index.postprocessor.sbert_rerank import SentenceTransformerRerank

# –ò–º–ø–æ—Ä—Ç debug –ª–æ–≥–≥–µ—Ä–∞
try:
    from rag_debug_logger import rag_debug
except ImportError:
    # Fallback –µ—Å–ª–∏ debug –ª–æ–≥–≥–µ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
    class DummyDebug:
        def log_enricher_input(self, *args): pass
        def log_enricher_prompt(self, *args): pass
        def log_enricher_output(self, *args): pass
        def log_retrieval_results(self, *args): pass
    rag_debug = DummyDebug()

try:
    from config import config
except ImportError:
    import os
    import sys
    sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
    from config import config

class LlamaIndexRAG:
    """
    –ü—Ä–æ—Å—Ç–∞—è RAG —Å–∏—Å—Ç–µ–º–∞ —Å ChatEngine –¥–ª—è –æ–±–æ–≥–∞—â–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç GPT-4o mini –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –Ω–µ–ø–æ–ª–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤.
    """
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.pinecone_index_name = "ukido"
        self.chat_engine = None

        try:
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –º–æ–¥–µ–ª–µ–π
            Settings.embed_model = GeminiEmbedding(
                model_name=config.EMBEDDING_MODEL, 
                api_key=config.GEMINI_API_KEY
            )
            Settings.llm = OpenRouter(
                api_key=config.OPENROUTER_API_KEY, 
                model="openai/gpt-4o-mini"
            )

            # –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Pinecone
            pc = pinecone.Pinecone(api_key=config.PINECONE_API_KEY)
            pinecone_index = pc.Index(self.pinecone_index_name)
            vector_store = PineconeVectorStore(pinecone_index=pinecone_index)
            index = VectorStoreIndex.from_vector_store(vector_store=vector_store)

            # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ä–µ—Ä–∞–Ω–∫–µ—Ä–∞
            reranker = SentenceTransformerRerank(
                model="cross-encoder/ms-marco-MiniLM-L-2-v2", 
                top_n=4
            )

            # –°–æ–∑–¥–∞–Ω–∏–µ ChatEngine
            self.chat_engine = ContextChatEngine.from_defaults(
                retriever=index.as_retriever(
                    similarity_top_k=15,
                    node_postprocessors=[reranker]
                ),
                llm=Settings.llm,
                system_prompt="""–¢—ã –ø–æ–º–æ—â–Ω–∏–∫ –ø–æ –ø–æ–∏—Å–∫—É –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π —à–∫–æ–ª—ã Ukido. 

–ï—Å–ª–∏ —Å–ø—Ä–∞—à–∏–≤–∞—é—Ç "–°–∫–æ–ª—å–∫–æ —Å—Ç–æ–∏—Ç?" - –∏—â–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –ø—Ä–æ —Å—Ç–æ–∏–º–æ—Å—Ç—å –∫—É—Ä—Å–æ–≤.
–ï—Å–ª–∏ —Å–ø—Ä–∞—à–∏–≤–∞—é—Ç "–ß—Ç–æ –Ω—É–∂–Ω–æ?" - –¥–æ–±–∞–≤—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –æ —á–µ–º —Ä–µ—á—å.
–ï—Å–ª–∏ —Å–ø—Ä–∞—à–∏–≤–∞—é—Ç "–ö–æ–≥–¥–∞?" - —É—Ç–æ—á–Ω–∏ –ø—Ä–æ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ.

–í–æ–∑–≤—Ä–∞—â–∞–π —Ç–æ–ª—å–∫–æ –Ω–∞–π–¥–µ–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –Ω–µ –¥–æ–±–∞–≤–ª—è–π –æ—Ç —Å–µ–±—è."""
            )

            self.logger.info("‚úÖ LlamaIndex ChatEngine –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")

        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏: {e}")
            raise

    def search_knowledge_base(self, query: str) -> Tuple[str, Dict[str, Any]]:
        """
        –ü–æ–∏—Å–∫ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π —Å –æ–±–æ–≥–∞—â–µ–Ω–∏–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        """
        search_start = time.time()
        
        # Debug –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
        rag_debug.log_enricher_input(query, [])
        
        if not self.chat_engine:
            self.logger.error("ChatEngine –Ω–µ –≥–æ—Ç–æ–≤")
            return "–û—à–∏–±–∫–∞: —Å–∏—Å—Ç–µ–º–∞ –Ω–µ –≥–æ—Ç–æ–≤–∞", {}

        try:
            self.logger.info(f"üîç –ü–æ–∏—Å–∫: '{query}'")
            
            # –õ–æ–≥–∏—Ä—É–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
            system_prompt = "–¢—ã –ø–æ–º–æ—â–Ω–∏–∫ –ø–æ –ø–æ–∏—Å–∫—É –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π —à–∫–æ–ª—ã Ukido..."
            rag_debug.log_enricher_prompt(f"SYSTEM: {system_prompt}\nUSER: {query}")
            
            # ChatEngine –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∑–∞–ø—Ä–æ—Å
            enrichment_start = time.time()
            response = self.chat_engine.chat(query)
            enrichment_time = time.time() - enrichment_start
            
            # Debug –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            rag_debug.log_enricher_output("ChatEngine –æ–±—Ä–∞–±–æ—Ç–∞–ª –∑–∞–ø—Ä–æ—Å", enrichment_time)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —á–∞–Ω–∫–∏
            context_chunks = []
            scores = []
            
            if hasattr(response, 'source_nodes') and response.source_nodes:
                context_chunks = [node.get_content() for node in response.source_nodes]
                scores = [getattr(node, 'score', 0.5) for node in response.source_nodes]
            
            # Fallback –µ—Å–ª–∏ —á–∞–Ω–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã
            if not context_chunks and hasattr(response, 'response'):
                context_chunks = [str(response.response)]
                scores = [0.7]
            
            context = "\n\n".join(context_chunks) if context_chunks else "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞"
            search_time = time.time() - search_start
            
            # –î–æ–ø–æ–ª–Ω—è–µ–º scores –¥–æ –Ω—É–∂–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞
            while len(scores) < len(context_chunks):
                scores.append(0.5)
            
            # Debug –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞
            rag_debug.log_retrieval_results(
                chunks=context_chunks,
                scores=scores[:len(context_chunks)],
                time_taken=search_time,
                total_before_rerank=15
            )

            # –ú–µ—Ç—Ä–∏–∫–∏
            average_score = sum(scores) / len(scores) if scores else 0.5
            metrics = {
                'search_time': search_time,
                'chunks_found': len(context_chunks),
                'average_score': average_score,
                'max_score': max(scores) if scores else 0.5,
            }

            self.logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(context_chunks)} —á–∞–Ω–∫–æ–≤ –∑–∞ {search_time:.2f}s")
            return context, metrics

        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
            return "–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏", {}

# –ì–ª–æ–±–∞–ª—å–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
try:
    llama_index_rag = LlamaIndexRAG()
except Exception as e:
    llama_index_rag = None
    logging.getLogger(__name__).error(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å LlamaIndexRAG: {e}")