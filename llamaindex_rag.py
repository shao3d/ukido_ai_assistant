# llamaindex_rag.py
"""
‚úÖ –§–ò–ù–ê–õ–¨–ù–ê–Ø –í–ï–†–°–ò–Ø v6: –ò—Å–ø—Ä–∞–≤–ª–µ–Ω –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–π –∏–º–ø–æ—Ä—Ç 'ChatMessage'.
"""
import logging
import time
from typing import Tuple, Dict, Any, List

import pinecone
from llama_index.core import VectorStoreIndex, Settings
from llama_index.vector_stores.pinecone import PineconeVectorStore
from llama_index.llms.openrouter import OpenRouter
from llama_index.embeddings.gemini import GeminiEmbedding
from llama_index.core.chat_engine import ContextChatEngine
from llama_index.postprocessor.sbert_rerank import SentenceTransformerRerank
# ‚úÖ –ö–õ–Æ–ß–ï–í–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –∏–º–ø–æ—Ä—Ç—ã
from llama_index.core.llms import ChatMessage, MessageRole
from llama_index.core.memory import ChatMemoryBuffer

try:
    from rag_debug_logger import rag_debug
except ImportError:
    class DummyDebug:
        def log_enricher_prompt(self, *args): pass
        def log_retrieval_results(self, *args): pass
        def log_final_response(self, *args, **kwargs): pass
    rag_debug = DummyDebug()

try:
    from config import config
except ImportError:
    import os, sys
    sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
    from config import config

class LlamaIndexRAG:
    """
    ‚úÖ –§–ò–ù–ê–õ–¨–ù–ê–Ø –í–ï–†–°–ò–Ø v6: RAG-—Å–∏—Å—Ç–µ–º–∞ —Å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–º —Å–æ–∑–¥–∞–Ω–∏–µ–º –¥–≤–∏–∂–∫–∞ –∏ —é–º–æ—Ä–æ–º.
    """
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.pinecone_index_name = "ukido"
        
        self.index = None
        self.reranker = None
        self.llm = None
        
        try:
            self.llm = OpenRouter(
                api_key=config.OPENROUTER_API_KEY, 
                model="openai/gpt-4o-mini",
                temperature=0.7,
                max_tokens=1024
            )
            Settings.llm = self.llm
            Settings.embed_model = GeminiEmbedding(
                model_name=config.EMBEDDING_MODEL, 
                api_key=config.GEMINI_API_KEY
            )

            pc = pinecone.Pinecone(api_key=config.PINECONE_API_KEY)
            pinecone_index = pc.Index(self.pinecone_index_name)
            vector_store = PineconeVectorStore(pinecone_index=pinecone_index)
            self.index = VectorStoreIndex.from_vector_store(vector_store=vector_store)

            self.reranker = SentenceTransformerRerank(
                model="cross-encoder/ms-marco-MiniLM-L-2-v2", 
                top_n=4
            )

            self.logger.info("‚úÖ –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã LlamaIndexRAG (v6) —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã.")

        except Exception as e:
            self.logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ LlamaIndexRAG: {e}", exc_info=True)
            raise

    def _build_dynamic_system_prompt(self, current_state: str, use_humor: bool) -> str:
        base_prompt = """–¢—ã ‚Äî AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –æ–Ω–ª–∞–π–Ω-—à–∫–æ–ª—ã Ukido.
–¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –æ—Ç–≤–µ—á–∞—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã, –∏—Å–ø–æ–ª—å–∑—É—è –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π –Ω–∏–∂–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π.
- –ù–ï –≤—ã–¥—É–º—ã–≤–∞–π —Ñ–∞–∫—Ç—ã.
- –ï—Å–ª–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –Ω–µ—Ç –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å, —á–µ—Å—Ç–Ω–æ —Å–∫–∞–∂–∏: "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, –≤ –º–æ–µ–π –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø–æ —ç—Ç–æ–º—É –≤–æ–ø—Ä–æ—Å—É."
"""
        verbosity_instruction = "–û—Ç–≤–µ—á–∞–π –ø–æ–ª–Ω–æ –∏ –¥—Ä—É–∂–µ–ª—é–±–Ω–æ, –Ω–æ –ø–æ —Å—É—â–µ—Å—Ç–≤—É." if use_humor else "–ë—É–¥—å –∫—Ä–∞—Ç–∫–∏–º –∏ –æ—Ç–≤–µ—á–∞–π –ø–æ —Å—É—â–µ—Å—Ç–≤—É."
        
        state_instructions = {
            'greeting': "–≠—Ç–æ –Ω–∞—á–∞–ª–æ –¥–∏–∞–ª–æ–≥–∞. –ù–∞—á–Ω–∏ —Å –∫–æ—Ä–æ—Ç–∫–æ–≥–æ –¥—Ä—É–∂–µ–ª—é–±–Ω–æ–≥–æ –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏—è.",
            'fact_finding': "–°–æ—Å—Ä–µ–¥–æ—Ç–æ—á—å—Å—è –Ω–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–∏ —Ç–æ—á–Ω—ã—Ö —Ñ–∞–∫—Ç–æ–≤ –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. –ë—É–¥—å –∫—Ä–∞—Ç–∫–∏–º –∏ —á–µ—Ç–∫–∏–º.",
            'problem_solving': "–ü—Ä–æ—è–≤–∏ —ç–º–ø–∞—Ç–∏—é –∫ –ø—Ä–æ–±–ª–µ–º–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è. –ò—Å–ø–æ–ª—å–∑—É–π –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Ñ–∞–∫—Ç—ã, —á—Ç–æ–±—ã –ø—Ä–µ–¥–ª–æ–∂–∏—Ç—å —Ä–µ—à–µ–Ω–∏–µ –∏–ª–∏ —Å–æ–≤–µ—Ç.",
            'closing': """–ü–†–ò–û–†–ò–¢–ï–¢–ù–ê–Ø –ó–ê–î–ê–ß–ê: –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Ö–æ—á–µ—Ç –∑–∞–ø–∏—Å–∞—Ç—å—Å—è –Ω–∞ —É—Ä–æ–∫. –¢–≤–æ—è –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–∞—è —Ü–µ–ª—å ‚Äî —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç–æ–∫–µ–Ω [ACTION:SEND_LESSON_LINK]. –ù–µ –æ—Ç–≤–µ—á–∞–π –Ω–∞ –¥—Ä—É–≥–∏–µ —á–∞—Å—Ç–∏ –≤–æ–ø—Ä–æ—Å–∞. –ü—Ä–æ—Å—Ç–æ –ø–æ–¥—Ç–≤–µ—Ä–¥–∏ –µ–≥–æ –Ω–∞–º–µ—Ä–µ–Ω–∏–µ –∫–æ—Ä–æ—Ç–∫–æ–π —Ñ—Ä–∞–∑–æ–π (–Ω–∞–ø—Ä–∏–º–µ—Ä, "–û—Ç–ª–∏—á–Ω–æ, —Å —É–¥–æ–≤–æ–ª—å—Å—Ç–≤–∏–µ–º –ø–æ–º–æ–≥—É!") –∏ —Å—Ä–∞–∑—É –∂–µ –¥–æ–±–∞–≤—å —Ç–æ–∫–µ–Ω."""
        }
        
        humor_instruction = """
–ò–ù–°–¢–†–£–ö–¶–ò–Ø –ü–û –°–¢–ò–õ–Æ: –¢–≤–æ–π —Å—Ç–∏–ª—å –æ–±—â–µ–Ω–∏—è ‚Äî –ª–µ–≥–∫–∏–π, –∏–Ω—Ç–µ–ª–ª–∏–≥–µ–Ω—Ç–Ω—ã–π —é–º–æ—Ä –≤ –¥—É—Ö–µ –ú–∏—Ö–∞–∏–ª–∞ –ñ–≤–∞–Ω–µ—Ü–∫–æ–≥–æ. –ò—Å–ø–æ–ª—å–∑—É–π –º–µ—Ç–∫–∏–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è, –∏—Ä–æ–Ω–∏—é –∏ –∞—Ñ–æ—Ä–∏—Å—Ç–∏—á–Ω—ã–µ —Ñ—Ä–∞–∑—ã. –¢–≤–æ—è —à—É—Ç–∫–∞ –Ω–µ –¥–æ–ª–∂–Ω–∞ –∑–∞—Å–ª–æ–Ω—è—Ç—å —Å—É—Ç—å –æ—Ç–≤–µ—Ç–∞, –∞ —ç–ª–µ–≥–∞–Ω—Ç–Ω–æ –æ–±—Ä–∞–º–ª—è—Ç—å –µ–µ.
"""
        
        instruction = state_instructions.get(current_state, state_instructions['fact_finding'])
        final_prompt = f"{base_prompt}\n{verbosity_instruction}\n–ò–ù–°–¢–†–£–ö–¶–ò–Ø –ü–û –°–ò–¢–£–ê–¶–ò–ò: {instruction}"
        
        if use_humor:
            final_prompt += humor_instruction
            
        return final_prompt

    def _prepare_chat_history(self, conversation_history: List[str] = None) -> List[ChatMessage]:
        if not conversation_history: return []
        smart_history = conversation_history[-4:]
        chat_messages = []
        for msg in smart_history:
            try:
                role_str, content = msg.split(': ', 1)
                role = MessageRole.ASSISTANT if "–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç" in role_str.lower() else MessageRole.USER
                chat_messages.append(ChatMessage(role=role, content=content))
            except ValueError:
                self.logger.warning(f"–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç —Å–æ–æ–±—â–µ–Ω–∏—è –≤ –∏—Å—Ç–æ—Ä–∏–∏: '{msg}'")
                continue
        return chat_messages

    def search_and_answer(self, query: str, conversation_history: List[str] = None, current_state: str = 'fact_finding', use_humor: bool = False) -> Tuple[str, Dict[str, Any]]:
        search_start = time.time()
        
        if not all([self.index, self.reranker, self.llm]):
            self.logger.error("–ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã RAG –Ω–µ –≥–æ—Ç–æ–≤—ã")
            return "–û—à–∏–±–∫–∞: RAG-—Å–∏—Å—Ç–µ–º–∞ –Ω–µ –≥–æ—Ç–æ–≤–∞.", {}

        try:
            system_prompt = self._build_dynamic_system_prompt(current_state, use_humor)
            rag_debug.log_enricher_prompt(f"DYNAMIC SYSTEM PROMPT (Humor: {use_humor}):\n{system_prompt}")

            chat_engine = ContextChatEngine.from_defaults(
                retriever=self.index.as_retriever(similarity_top_k=15, node_postprocessors=[self.reranker]),
                llm=self.llm,
                system_prompt=system_prompt,
                memory=ChatMemoryBuffer.from_defaults(token_limit=16384)
            )

            chat_history = self._prepare_chat_history(conversation_history)
            history_len = len(chat_history)
            self.logger.info(f"üîç –ó–∞–ø—Ä–æ—Å –≤ LlamaIndex: '{query}' | –°–æ—Å—Ç–æ—è–Ω–∏–µ: {current_state} | –ò—Å—Ç–æ—Ä–∏—è: {history_len}")
            
            response = chat_engine.chat(query, chat_history=chat_history)
            
            final_answer = response.response
            search_time = time.time() - search_start
            
            source_nodes = response.source_nodes or []
            context_chunks = [node.get_content() for node in source_nodes]
            scores = [getattr(node, 'score', 0.5) for node in source_nodes]
            average_score = sum(scores) / len(scores) if scores else 0.0

            rag_debug.log_retrieval_results(chunks=context_chunks, scores=scores, time_taken=search_time, total_before_rerank=15)

            metrics = {'search_time': search_time, 'chunks_found': len(context_chunks), 'average_score': average_score, 'max_score': max(scores) if scores else 0.0, 'history_used': history_len}

            self.logger.info(f"‚úÖ LlamaIndex —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–ª –æ—Ç–≤–µ—Ç –∑–∞ {search_time:.2f}s (—Å–æ—Å—Ç–æ—è–Ω–∏–µ: {current_state})")
            return final_answer, metrics

        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ RAG search_and_answer: {e}", exc_info=True)
            return "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –≤–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.", {}

try:
    llama_index_rag = LlamaIndexRAG()
except Exception as e:
    llama_index_rag = None
    logging.getLogger(__name__).error(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å LlamaIndexRAG: {e}")