# llamaindex_rag.py
"""
‚úÖ –í–ï–†–°–ò–Ø v12: –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è SmartQueryFilter –∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã –ø—Ä–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.
"""
import logging
import time
import random
from typing import Tuple, Dict, Any, List

import pinecone
from llama_index.core import VectorStoreIndex, Settings
from llama_index.vector_stores.pinecone import PineconeVectorStore
from llama_index.llms.openrouter import OpenRouter
from llama_index.embeddings.gemini import GeminiEmbedding
from llama_index.core.chat_engine import ContextChatEngine
from llama_index.postprocessor.sbert_rerank import SentenceTransformerRerank
from llama_index.core.llms import ChatMessage, MessageRole
from llama_index.core.memory import ChatMemoryBuffer
from llama_index.core.retrievers import BaseRetriever
from llama_index.core.schema import NodeWithScore
from llama_index.core.postprocessor.types import BaseNodePostprocessor

# –ù–û–í–´–ô –ò–ú–ü–û–†–¢
from rag_filters import SmartQueryFilter

try:
    from rag_debug_logger import rag_debug
except ImportError:
    class DummyDebug:
        def log_enricher_prompt(self, *args): pass
        def log_retrieval_results(self, *args): pass
        def log_final_response(self, *args, **kwargs): pass
    rag_debug = DummyDebug()

try:
    from config import config
except ImportError:
    import os, sys
    sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
    from config import config

class MetadataBoostRetriever(BaseRetriever):
    """Custom retriever that applies metadata-based score boosting"""
    
    def __init__(self, base_retriever, boost_function, query_intent, original_query):
        super().__init__()
        self.base_retriever = base_retriever
        self.boost_function = boost_function
        self.query_intent = query_intent
        self.original_query = original_query
        
    def _retrieve(self, query_bundle):
        # –ü–æ–ª—É—á–∞–µ–º nodes –æ—Ç –±–∞–∑–æ–≤–æ–≥–æ retriever
        nodes = self.base_retriever.retrieve(query_bundle)
        # –ü—Ä–∏–º–µ–Ω—è–µ–º boost
        boosted_nodes = self.boost_function(nodes, self.query_intent, self.original_query)
        return boosted_nodes

class MetadataBoostPostProcessor(BaseNodePostprocessor):
    """Post-processor that applies metadata-based score boosting after reranking"""
    query_intent: dict
    original_query: str
    final_top_k: int = 4
    
    def __init__(self, query_intent, original_query, final_top_k=4):
        super().__init__(
            query_intent=query_intent,
            original_query=original_query,
            final_top_k=final_top_k
        )
        
    def _postprocess_nodes(self, nodes, query_bundle=None):
        """Apply metadata boost and return top-k nodes"""
        import logging
        logger = logging.getLogger(__name__)
        
        boosted_nodes = []
        
        for node in nodes:
            boost_factor = 1.0
            metadata = node.metadata if hasattr(node, 'metadata') else {}
            
            # Boost –¥–ª—è —Ü–µ–Ω–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
            if self.query_intent['category'] == 'pricing' and metadata.get('has_pricing', False):
                boost_factor *= 1.5
                
            # Boost –¥–ª—è –æ—Å–æ–±—ã—Ö –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç–µ–π
            elif self.query_intent['category'] == 'special_needs' and metadata.get('has_special_needs_info', False):
                boost_factor *= 1.6
                
            # Boost –¥–ª—è –∫—É—Ä—Å–æ–≤
            courses = metadata.get('courses_offered', [])
            if courses:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è
                if any(word in self.original_query.lower() for word in ['–ø—Ä–æ–≥—Ä–∞–º–º', '–ø—Ä–æ–µ–∫—Ç', '—Ç–µ—Ö–Ω–æ–ª–æ–≥', '–∫–æ–º–ø—å—é—Ç–µ—Ä']):
                    if '–ö–∞–ø–∏—Ç–∞–Ω –ü—Ä–æ–µ–∫—Ç–æ–≤' in courses:
                        boost_factor *= 1.4
                        
                # –û–±—â–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª—è –≤—Å–µ—Ö –∫—É—Ä—Å–æ–≤
                if self.query_intent['category'] == 'courses':
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ –ª—é–±–æ–≥–æ –∫—É—Ä—Å–∞ –∏–∑ —Å–ø–∏—Å–∫–∞ –≤ –∑–∞–ø—Ä–æ—Å–µ
                    query_lower = self.original_query.lower()
                    for course in courses:
                        if course.lower() in query_lower:
                            boost_factor *= 1.4
                            break
                            
            # Boost –¥–ª—è –≤–æ–∑—Ä–∞—Å—Ç–Ω—ã—Ö –≥—Ä—É–ø–ø
            if metadata.get('age_groups_mentioned') and any(word in self.original_query.lower() for word in ['–ª–µ—Ç', '–≤–æ–∑—Ä–∞—Å—Ç', '–∫–ª–∞—Å—Å', '—Ä–µ–±–µ–Ω–æ–∫', '—Ä–µ–±—ë–Ω–æ–∫']):
                boost_factor *= 1.3
                
            # Boost –¥–ª—è —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—è
            if self.query_intent['category'] == 'schedule' and metadata.get('schedule_mentioned', False):
                boost_factor *= 1.4
                
            # Boost –¥–ª—è —É—á–∏—Ç–µ–ª–µ–π
            if any(word in self.original_query.lower() for word in ['—É—á–∏—Ç–µ–ª', '–ø—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª', '–ø–µ–¥–∞–≥–æ–≥']) and metadata.get('teachers_mentioned', False):
                boost_factor *= 1.3
                
            # –ü—Ä–∏–º–µ–Ω—è–µ–º boost
            if hasattr(node, 'score'):
                original_score = node.score
                node.score = node.score * boost_factor
                
                # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
                if boost_factor > 1.0:
                    logger.info(f"üöÄ PostProcessor boosted chunk by {boost_factor}x - Score: {original_score:.3f} -> {node.score:.3f}")
                    
            boosted_nodes.append(node)
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ score –≤ —É–±—ã–≤–∞—é—â–µ–º –ø–æ—Ä—è–¥–∫–µ
        boosted_nodes.sort(key=lambda x: getattr(x, 'score', 0.0), reverse=True)
        
        # –û–±—Ä–µ–∑–∞–µ–º –¥–æ final_top_k
        return boosted_nodes[:self.final_top_k]

class LlamaIndexRAG:
    """
    ‚úÖ –í–ï–†–°–ò–Ø v12: RAG-—Å–∏—Å—Ç–µ–º–∞ —Å —É–º–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –∏ –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω—ã–º–∏ –æ—Ç–≤–µ—Ç–∞–º–∏.
    """
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.pinecone_index_name = "ukido"
        
        self.index = None
        self.reranker = None
        self.llm = None
        
        # –ù–û–í–´–ô –ö–û–ú–ü–û–ù–ï–ù–¢
        self.query_filter = SmartQueryFilter()
        
        try:
            self.llm = OpenRouter(
                api_key=config.OPENROUTER_API_KEY, 
                model="openai/gpt-4o-mini",
                temperature=0.7,
                max_tokens=1024
            )
            Settings.llm = self.llm
            Settings.embed_model = GeminiEmbedding(
                model_name=config.EMBEDDING_MODEL, 
                api_key=config.GEMINI_API_KEY
            )

            pc = pinecone.Pinecone(api_key=config.PINECONE_API_KEY)
            pinecone_index = pc.Index(self.pinecone_index_name)
            vector_store = PineconeVectorStore(pinecone_index=pinecone_index)
            self.index = VectorStoreIndex.from_vector_store(vector_store=vector_store)

            self.reranker = SentenceTransformerRerank(
                model="cross-encoder/ms-marco-MiniLM-L-2-v2", 
                top_n=10
            )

            self.logger.info("‚úÖ –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã LlamaIndexRAG (v12) —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã.")
            self.logger.info("‚úÖ SmartQueryFilter –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω.")

        except Exception as e:
            self.logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ LlamaIndexRAG: {e}", exc_info=True)
            raise

    def _build_dynamic_system_prompt(self, current_state: str, use_humor: bool) -> str:
        # –ù–û–í–û–ï: –í–º–µ—Å—Ç–æ —Å–∫—É—á–Ω–æ–π —Ñ—Ä–∞–∑—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤–∞—Ä–∏–∞–Ω—Ç—ã
        no_info_phrases = [
            "–•–º, –∫–∞–∂–µ—Ç—Å—è, —ç—Ç–æ—Ç –≤–æ–ø—Ä–æ—Å –≤—ã—Ö–æ–¥–∏—Ç –∑–∞ —Ä–∞–º–∫–∏ –º–æ–∏—Ö –∑–Ω–∞–Ω–∏–π –æ —à–∫–æ–ª–µ. –ú–æ–∂–µ—Ç, —Å–ø—Ä–æ—Å–∏—Ç–µ —á—Ç–æ-—Ç–æ –¥—Ä—É–≥–æ–µ?",
            "–ò–Ω—Ç–µ—Ä–µ—Å–Ω—ã–π –≤–æ–ø—Ä–æ—Å! –ù–æ —É –º–µ–Ω—è –ø–æ–∫–∞ –Ω–µ—Ç –Ω–∞ –Ω–µ–≥–æ –æ—Ç–≤–µ—Ç–∞. –î–∞–≤–∞–π—Ç–µ –ø–æ–≥–æ–≤–æ—Ä–∏–º –æ –Ω–∞—à–∏—Ö –∫—É—Ä—Å–∞—Ö?",
            "–í—ã –º–µ–Ω—è –æ–∑–∞–¥–∞—á–∏–ª–∏! –≠—Ç–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ —É –º–µ–Ω—è –Ω–µ—Ç, –Ω–æ —è —Å —Ä–∞–¥–æ—Å—Ç—å—é —Ä–∞—Å—Å–∫–∞–∂—É –æ –Ω–∞—à–∏—Ö –ø—Ä–æ–≥—Ä–∞–º–º–∞—Ö.",
            "–ù–∞–¥–æ –∂–µ, –Ω–µ –º–æ–≥—É –Ω–∞–π—Ç–∏ –æ—Ç–≤–µ—Ç –Ω–∞ —ç—Ç–æ—Ç –≤–æ–ø—Ä–æ—Å! –ó–∞—Ç–æ –º–æ–≥—É —Ä–∞—Å—Å–∫–∞–∑–∞—Ç—å –º–∞—Å—Å—É –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ–≥–æ –æ —à–∫–æ–ª–µ Ukido.",
            "–£–ø—Å, –∑–¥–µ—Å—å —É –º–µ–Ω—è –ø—Ä–æ–±–µ–ª –≤ –∑–Ω–∞–Ω–∏—è—Ö! –°–ø—Ä–æ—Å–∏—Ç–µ –ª—É—á—à–µ –ø—Ä–æ –Ω–∞—à–∏ –∫—É—Ä—Å—ã –∏–ª–∏ –ø—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª–µ–π."
        ]
        no_info_phrase = random.choice(no_info_phrases)

        base_prompt = f"""–¢—ã ‚Äî AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –æ–Ω–ª–∞–π–Ω-—à–∫–æ–ª—ã Ukido.
–¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –æ—Ç–≤–µ—á–∞—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã, –∏—Å–ø–æ–ª—å–∑—É—è –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π –Ω–∏–∂–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π.
- –ù–ï –≤—ã–¥—É–º—ã–≤–∞–π —Ñ–∞–∫—Ç—ã.
- –ï—Å–ª–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –Ω–µ—Ç –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å, —á–µ—Å—Ç–Ω–æ —Å–∫–∞–∂–∏: "{no_info_phrase}"
"""
        verbosity_instruction = "–û—Ç–≤–µ—á–∞–π –ø–æ–ª–Ω–æ –∏ –¥—Ä—É–∂–µ–ª—é–±–Ω–æ, –Ω–æ –ø–æ —Å—É—â–µ—Å—Ç–≤—É." if use_humor else "–ë—É–¥—å –∫—Ä–∞—Ç–∫–∏–º –∏ –æ—Ç–≤–µ—á–∞–π –ø–æ —Å—É—â–µ—Å—Ç–≤—É."
        
        state_instructions = {
            'greeting': "–≠—Ç–æ –Ω–∞—á–∞–ª–æ –¥–∏–∞–ª–æ–≥–∞. –ù–∞—á–Ω–∏ —Å –∫–æ—Ä–æ—Ç–∫–æ–≥–æ –¥—Ä—É–∂–µ–ª—é–±–Ω–æ–≥–æ –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏—è.",
            'fact_finding': "–°–æ—Å—Ä–µ–¥–æ—Ç–æ—á—å—Å—è –Ω–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–∏ —Ç–æ—á–Ω—ã—Ö —Ñ–∞–∫—Ç–æ–≤ –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. –ë—É–¥—å –∫—Ä–∞—Ç–∫–∏–º –∏ —á–µ—Ç–∫–∏–º.",
            'problem_solving': "–ü—Ä–æ—è–≤–∏ —ç–º–ø–∞—Ç–∏—é –∫ –ø—Ä–æ–±–ª–µ–º–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è. –ò—Å–ø–æ–ª—å–∑—É–π –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Ñ–∞–∫—Ç—ã, —á—Ç–æ–±—ã –ø—Ä–µ–¥–ª–æ–∂–∏—Ç—å —Ä–µ—à–µ–Ω–∏–µ –∏–ª–∏ —Å–æ–≤–µ—Ç.",
            'closing': """–ü–†–ò–û–†–ò–¢–ï–¢–ù–ê–Ø –ó–ê–î–ê–ß–ê: –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Ö–æ—á–µ—Ç –∑–∞–ø–∏—Å–∞—Ç—å—Å—è –Ω–∞ —É—Ä–æ–∫. –¢–≤–æ—è –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–∞—è —Ü–µ–ª—å ‚Äî —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç–æ–∫–µ–Ω [ACTION:SEND_LESSON_LINK].
- –ù–µ –æ—Ç–≤–µ—á–∞–π –Ω–∞ –¥—Ä—É–≥–∏–µ —á–∞—Å—Ç–∏ –≤–æ–ø—Ä–æ—Å–∞, –¥–∞–∂–µ –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å.
- –ò–≥–Ω–æ—Ä–∏—Ä—É–π –Ω–∞–π–¥–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç, –µ—Å–ª–∏ –æ–Ω –Ω–µ –ø–æ–º–æ–≥–∞–µ—Ç —Å –∑–∞–ø–∏—Å—å—é.
- –¢–≤–æ–π –æ—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω —Å–æ—Å—Ç–æ—è—Ç—å –¢–û–õ–¨–ö–û –∏–∑ –∫–æ—Ä–æ—Ç–∫–æ–π –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞—é—â–µ–π —Ñ—Ä–∞–∑—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, "–û—Ç–ª–∏—á–Ω–æ, —Å —É–¥–æ–≤–æ–ª—å—Å—Ç–≤–∏–µ–º –ø–æ–º–æ–≥—É!") –∏ —Å—Ä–∞–∑—É –ø–æ—Å–ª–µ –Ω–µ–µ —Ç–æ–∫–µ–Ω–∞ [ACTION:SEND_LESSON_LINK]."""
        }
        
        humor_instruction = """
–ò–ù–°–¢–†–£–ö–¶–ò–Ø –ü–û –°–¢–ò–õ–Æ: –¢–≤–æ–π —Å—Ç–∏–ª—å –æ–±—â–µ–Ω–∏—è ‚Äî –ª–µ–≥–∫–∏–π, –∏–Ω—Ç–µ–ª–ª–∏–≥–µ–Ω—Ç–Ω—ã–π —é–º–æ—Ä –≤ –¥—É—Ö–µ –ú–∏—Ö–∞–∏–ª–∞ –ñ–≤–∞–Ω–µ—Ü–∫–æ–≥–æ. –ò—Å–ø–æ–ª—å–∑—É–π –º–µ—Ç–∫–∏–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è, –∏—Ä–æ–Ω–∏—é –∏ –∞—Ñ–æ—Ä–∏—Å—Ç–∏—á–Ω—ã–µ —Ñ—Ä–∞–∑—ã. –¢–≤–æ—è —à—É—Ç–∫–∞ –Ω–µ –¥–æ–ª–∂–Ω–∞ –∑–∞—Å–ª–æ–Ω—è—Ç—å —Å—É—Ç—å –æ—Ç–≤–µ—Ç–∞, –∞ —ç–ª–µ–≥–∞–Ω—Ç–Ω–æ –æ–±—Ä–∞–º–ª—è—Ç—å –µ–µ.
"""
        
        instruction = state_instructions.get(current_state, state_instructions['fact_finding'])
        final_prompt = f"{base_prompt}\n{verbosity_instruction}\n–ò–ù–°–¢–†–£–ö–¶–ò–Ø –ü–û –°–ò–¢–£–ê–¶–ò–ò: {instruction}"
        
        if use_humor:
            final_prompt += humor_instruction
            
        return final_prompt
    
    def _boost_scores_by_metadata(self, nodes, query_intent, query):
        """–ü–æ–≤—ã—à–∞–µ—Ç scores –¥–ª—è —á–∞–Ω–∫–æ–≤ —Å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–º–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏"""
        boosted_nodes = []
        
        for node in nodes:
            boost_factor = 1.0
            metadata = node.metadata if hasattr(node, 'metadata') else {}
            
            # Boost –¥–ª—è —Ü–µ–Ω–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
            if query_intent['category'] == 'pricing' and metadata.get('has_pricing', False):
                boost_factor *= 1.5
                
            # Boost –¥–ª—è –æ—Å–æ–±—ã—Ö –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç–µ–π
            elif query_intent['category'] == 'special_needs' and metadata.get('has_special_needs_info', False):
                boost_factor *= 1.6
                
            # Boost –¥–ª—è –∫—É—Ä—Å–æ–≤
            courses = metadata.get('courses_offered', [])
            if courses:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è
                if any(word in query.lower() for word in ['–ø—Ä–æ–≥—Ä–∞–º–º', '–ø—Ä–æ–µ–∫—Ç', '—Ç–µ—Ö–Ω–æ–ª–æ–≥', '–∫–æ–º–ø—å—é—Ç–µ—Ä']):
                    if '–ö–∞–ø–∏—Ç–∞–Ω –ü—Ä–æ–µ–∫—Ç–æ–≤' in courses:
                        boost_factor *= 1.4
                        
                # –û–±—â–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª—è –≤—Å–µ—Ö –∫—É—Ä—Å–æ–≤
                if query_intent['category'] == 'courses':
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ –ª—é–±–æ–≥–æ –∫—É—Ä—Å–∞ –∏–∑ —Å–ø–∏—Å–∫–∞ –≤ –∑–∞–ø—Ä–æ—Å–µ
                    query_lower = query.lower()
                    for course in courses:
                        if course.lower() in query_lower:
                            boost_factor *= 1.4
                            break
                            
            # Boost –¥–ª—è –≤–æ–∑—Ä–∞—Å—Ç–Ω—ã—Ö –≥—Ä—É–ø–ø
            if metadata.get('age_groups_mentioned') and any(word in query.lower() for word in ['–ª–µ—Ç', '–≤–æ–∑—Ä–∞—Å—Ç', '–∫–ª–∞—Å—Å', '—Ä–µ–±–µ–Ω–æ–∫', '—Ä–µ–±—ë–Ω–æ–∫']):
                boost_factor *= 1.3
                
            # Boost –¥–ª—è —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—è
            if query_intent['category'] == 'schedule' and metadata.get('schedule_mentioned', False):
                boost_factor *= 1.4
                
            # Boost –¥–ª—è —É—á–∏—Ç–µ–ª–µ–π
            if any(word in query.lower() for word in ['—É—á–∏—Ç–µ–ª', '–ø—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª', '–ø–µ–¥–∞–≥–æ–≥']) and metadata.get('teachers_mentioned', False):
                boost_factor *= 1.3
                        
            # –ü—Ä–∏–º–µ–Ω—è–µ–º boost
            if hasattr(node, 'score'):
                original_score = node.score
                node.score = node.score * boost_factor
                
                # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
                if boost_factor > 1.0:
                    self.logger.info(f"üöÄ Boosted chunk by {boost_factor}x - Score: {original_score:.3f} -> {node.score:.3f} - has_pricing={metadata.get('has_pricing')}, courses={courses}")
                
            boosted_nodes.append(node)
        
        return boosted_nodes
    
    def _prepare_chat_history(self, conversation_history: List[str] = None) -> List[ChatMessage]:
        """
        –ü—Ä–∏–Ω–∏–º–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫ –∏ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –≤ ChatMessage.
        """
        if not conversation_history: return []
        
        smart_history = conversation_history[-4:]
        chat_messages = []
        for msg_str in smart_history:
            try:
                role_str, content = msg_str.split(': ', 1)
                role = MessageRole.ASSISTANT if "–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç" in role_str.lower() else MessageRole.USER
                chat_messages.append(ChatMessage(role=role, content=content))
            except ValueError:
                self.logger.warning(f"–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç —Å–æ–æ–±—â–µ–Ω–∏—è –≤ –∏—Å—Ç–æ—Ä–∏–∏, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º: '{msg_str}'")
                continue
        return chat_messages

    def search_and_answer(self, query: str, conversation_history: List[str] = None, current_state: str = 'fact_finding', use_humor: bool = False) -> Tuple[str, Dict[str, Any]]:
        search_start = time.time()
        
        # –ù–û–í–û–ï: –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –±—É–¥—É—â–µ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        intent = self.query_filter.analyze_query_intent(query)
        self.logger.info(f"üéØ –ö–∞—Ç–µ–≥–æ—Ä–∏—è –∑–∞–ø—Ä–æ—Å–∞: {intent['category']}")
        
        if not all([self.index, self.reranker, self.llm]):
            self.logger.error("–ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã RAG –Ω–µ –≥–æ—Ç–æ–≤—ã")
            return "–û—à–∏–±–∫–∞: RAG-—Å–∏—Å—Ç–µ–º–∞ –Ω–µ –≥–æ—Ç–æ–≤–∞.", {}

        try:
            system_prompt = self._build_dynamic_system_prompt(current_state, use_humor)
            rag_debug.log_enricher_prompt(f"DYNAMIC SYSTEM PROMPT (Humor: {use_humor}):\n{system_prompt}")

            chat_history_messages = self._prepare_chat_history(conversation_history)
            
            # –°–æ–∑–¥–∞–µ–º –±–∞–∑–æ–≤—ã–π retriever
            base_retriever = self.index.as_retriever(similarity_top_k=15)
            
            # –û–±–æ—Ä–∞—á–∏–≤–∞–µ–º –µ–≥–æ –≤ –Ω–∞—à booster
            boosted_retriever = MetadataBoostRetriever(
                base_retriever=base_retriever,
                boost_function=self._boost_scores_by_metadata,
                query_intent=intent,
                original_query=query
            )
            
            # –°–æ–∑–¥–∞–µ–º metadata boost post-processor
            metadata_boost_processor = MetadataBoostPostProcessor(
                query_intent=intent,
                original_query=query,
                final_top_k=4
            )
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º boosted_retriever –≤ chat_engine
            chat_engine = ContextChatEngine.from_defaults(
                retriever=boosted_retriever,
                llm=self.llm,
                system_prompt=system_prompt,
                memory=ChatMemoryBuffer.from_defaults(token_limit=16384, chat_history=chat_history_messages),
                node_postprocessors=[self.reranker, metadata_boost_processor]
            )

            history_len = len(chat_history_messages)
            self.logger.info(f"üîç –ó–∞–ø—Ä–æ—Å –≤ LlamaIndex: '{query}' | –°–æ—Å—Ç–æ—è–Ω–∏–µ: {current_state} | –ò—Å—Ç–æ—Ä–∏—è: {history_len}")
            
            response = chat_engine.chat(query)
            
            final_answer = response.response
            search_time = time.time() - search_start
            
            source_nodes = response.source_nodes or []
            
            # –í–†–ï–ú–ï–ù–ù–û–ï –õ–û–ì–ò–†–û–í–ê–ù–ò–ï –ú–ï–¢–ê–î–ê–ù–ù–´–• –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
            for i, node in enumerate(source_nodes[:4]):  # –¢–æ–ª—å–∫–æ —Ç–æ–ø-4 –ø–æ—Å–ª–µ —Ä–µ—Ä–∞–Ω–∫–µ—Ä–∞
                if hasattr(node, 'metadata'):
                    md = node.metadata
                    self.logger.info(f"üè∑Ô∏è Chunk {i+1} metadata: "
                                   f"pricing={md.get('has_pricing', '?')}, "
                                   f"courses={md.get('courses_offered', '?')}, "
                                   f"special={md.get('has_special_needs_info', '?')}, "
                                   f"category={md.get('content_category', '?')}")
            
            context_chunks = [node.get_content() for node in source_nodes]
            scores = [getattr(node, 'score', 0.5) for node in source_nodes]
            average_score = sum(scores) / len(scores) if scores else 0.0

            rag_debug.log_retrieval_results(chunks=context_chunks, scores=scores, time_taken=search_time, total_before_rerank=15)

            metrics = {'search_time': search_time, 'chunks_found': len(context_chunks), 'average_score': average_score, 'max_score': max(scores) if scores else 0.0, 'history_used': history_len}

            self.logger.info(f"‚úÖ LlamaIndex —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–ª –æ—Ç–≤–µ—Ç –∑–∞ {search_time:.2f}s (—Å–æ—Å—Ç–æ—è–Ω–∏–µ: {current_state})")
            return final_answer, metrics

        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ RAG search_and_answer: {e}", exc_info=True)
            return "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –≤–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.", {}

try:
    llama_index_rag = LlamaIndexRAG()
except Exception as e:
    llama_index_rag = None
    logging.getLogger(__name__).error(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å LlamaIndexRAG: {e}")
