"""
–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —É–ª—É—á—à–µ–Ω–∏–π –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö —Å CustomMetadataExtractor
–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —É–∫–∞–∑–∞–Ω–Ω—ã–π —Ñ–∞–π–ª –∏ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞.
"""

import logging
import os
import argparse
from collections import Counter
from llama_index.core import (
    SimpleDirectoryReader,
    Settings,
)
from llama_index.core.node_parser import (
    MarkdownNodeParser,
    SemanticSplitterNodeParser,
)
from llama_index.core.ingestion import IngestionPipeline
from llama_index.core.extractors import QuestionsAnsweredExtractor
from llama_index.embeddings.gemini import GeminiEmbedding
from llama_index.llms.openrouter import OpenRouter
from custom_metadata_extractor import CustomMetadataExtractor

try:
    from config import config
except ImportError:
    import sys
    sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
    from config import config

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
DEFAULT_TEST_FILE = "data_facts/faq.md"
QUESTIONS_FILE = "questions.txt"

def load_questions(filepath: str) -> str:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –≤–æ–ø—Ä–æ—Å—ã –∏–∑ —Ñ–∞–π–ª–∞."""
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            questions = [line.strip() for line in f if line.strip()]
        logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(questions)} –≤–æ–ø—Ä–æ—Å–æ–≤-–ø—Ä–∏–º–µ—Ä–æ–≤ –∏–∑ '{filepath}'.")
        return "\n".join(f"{i+1}. {q}" for i, q in enumerate(questions))
    except FileNotFoundError:
        logger.error(f"‚ùå –§–∞–π–ª —Å –≤–æ–ø—Ä–æ—Å–∞–º–∏ '{filepath}' –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        return "1. –ß—Ç–æ —ç—Ç–æ –∑–∞ –∫—É—Ä—Å?\n2. –°–∫–æ–ª—å–∫–æ —ç—Ç–æ —Å—Ç–æ–∏—Ç?\n3. –ö–∞–∫ –∑–∞–ø–∏—Å–∞—Ç—å—Å—è?"

def setup_models():
    """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –º–æ–¥–µ–ª–∏ LLM –∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞."""
    logger.info("‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –º–æ–¥–µ–ª–µ–π...")
    
    llm = OpenRouter(
        api_key=config.OPENROUTER_API_KEY, 
        model="mistralai/mistral-7b-instruct-v0.2"
    )
    embed_model = GeminiEmbedding(
        model_name=config.EMBEDDING_MODEL, 
        api_key=config.GEMINI_API_KEY
    )
    
    Settings.llm = llm
    Settings.embed_model = embed_model
    
    return llm, embed_model

def create_pipeline(llm, embed_model):
    """–°–æ–∑–¥–∞–µ—Ç pipeline –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤."""
    logger.info("üî™ –°–æ–∑–¥–∞–Ω–∏–µ pipeline...")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã –≤–æ–ø—Ä–æ—Å–æ–≤
    example_questions = load_questions(QUESTIONS_FILE)
    
    # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–æ–ø—Ä–æ—Å–æ–≤
    qa_generate_prompt_str = (
        "–ù–∏–∂–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω –∫–æ–Ω—Ç–µ–∫—Å—Ç:\n"
        "---------------------\n"
        "{context_str}\n"
        "---------------------\n"
        "–ò—Å–ø–æ–ª—å–∑—É—è —ç—Ç–æ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–µ –∑–Ω–∞–Ω–∏—è, "
        "—Å–≥–µ–Ω–µ—Ä–∏—Ä—É–π {num_questions} –≤–æ–ø—Ä–æ—Å–æ–≤, –Ω–∞ –∫–æ—Ç–æ—Ä—ã–µ –æ—Ç–≤–µ—á–∞–µ—Ç –¥–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç.\n"
        "–í–æ–ø—Ä–æ—Å—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –≤ —Å—Ç–∏–ª–µ –∏ –ø–æ —Ç–µ–º–∞—Ç–∏–∫–µ, –∫–∞–∫ –≤ —ç—Ç–∏—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö:\n"
        "--- –ü–†–ò–ú–ï–†–´ –í–û–ü–†–û–°–û–í ---\n"
        f"{example_questions}\n"
        "------------------------\n"
        "–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã:\n"
    )
    
    # –°–æ–∑–¥–∞–µ–º —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä –≤–æ–ø—Ä–æ—Å–æ–≤
    question_extractor = QuestionsAnsweredExtractor(
        questions=5,
        prompt_template=qa_generate_prompt_str,
        llm=llm,
    )
    
    # –°–æ–∑–¥–∞–µ–º pipeline
    pipeline = IngestionPipeline(
        transformations=[
            MarkdownNodeParser(include_metadata=True),
            SemanticSplitterNodeParser(
                buffer_size=1,
                breakpoint_percentile_threshold=95,
                embed_model=embed_model
            ),
            question_extractor,
            CustomMetadataExtractor(),  # –ù–ê–®–ê –ù–û–í–ê–Ø –§–£–ù–ö–¶–ò–Ø
            embed_model,
        ],
    )
    
    return pipeline

def display_chunk_details(chunk, chunk_num):
    """–û—Ç–æ–±—Ä–∞–∂–∞–µ—Ç –¥–µ—Ç–∞–ª–∏ —á–∞–Ω–∫–∞ –≤ –∫—Ä–∞—Å–∏–≤–æ–º —Ñ–æ—Ä–º–∞—Ç–µ."""
    print(f"\n{'='*50}")
    print(f"–ß–ê–ù–ö {chunk_num}")
    print(f"{'='*50}")
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 200 —Å–∏–º–≤–æ–ª–æ–≤ —Ç–µ–∫—Å—Ç–∞
    text = getattr(chunk, 'text', '')
    print(f"–¢–µ–∫—Å—Ç (–ø–µ—Ä–≤—ã–µ 200 —Å–∏–º–≤–æ–ª–æ–≤): {text[:200]}...")
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    metadata = getattr(chunk, 'metadata', {})
    print(f"\n–ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ:")
    
    # –°–ø–∏—Å–æ–∫ –≤–∞–∂–Ω—ã—Ö –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ–∫–∞–∑–∞
    important_metadata = [
        'questions_this_excerpt_can_answer',
        'content_type',
        'has_pricing',
        'course_mentioned',
        'is_teacher_info',
        'is_faq',
        'text_length',
        'has_courses'
    ]
    
    for key in important_metadata:
        if key in metadata:
            value = metadata[key]
            if isinstance(value, list) and len(value) > 3:
                # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω–Ω—ã–µ —Å–ø–∏—Å–∫–∏
                print(f"  ‚Ä¢ {key}: {value[:3]}... ({len(value)} –≤—Å–µ–≥–æ)")
            else:
                print(f"  ‚Ä¢ {key}: {value}")
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    other_metadata = {k: v for k, v in metadata.items() if k not in important_metadata}
    if other_metadata:
        print(f"\n–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ:")
        for key, value in other_metadata.items():
            if isinstance(value, str) and len(value) > 100:
                print(f"  ‚Ä¢ {key}: {value[:100]}...")
            else:
                print(f"  ‚Ä¢ {key}: {value}")

def display_statistics(chunks):
    """–û—Ç–æ–±—Ä–∞–∂–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–º —á–∞–Ω–∫–∞–º."""
    print(f"\n{'='*60}")
    print("–°–¢–ê–¢–ò–°–¢–ò–ö–ê")
    print(f"{'='*60}")
    
    total_chunks = len(chunks)
    print(f"üìä –í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {total_chunks}")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ has_pricing
    pricing_count = sum(1 for chunk in chunks 
                       if getattr(chunk, 'metadata', {}).get('has_pricing', False))
    print(f"üí∞ –ß–∞–Ω–∫–æ–≤ —Å —Ü–µ–Ω–æ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π (has_pricing=True): {pricing_count}")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ is_faq
    faq_count = sum(1 for chunk in chunks 
                   if getattr(chunk, 'metadata', {}).get('is_faq', False))
    print(f"‚ùì –ß–∞–Ω–∫–æ–≤ —Å FAQ (is_faq=True): {faq_count}")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ is_teacher_info
    teacher_count = sum(1 for chunk in chunks 
                       if getattr(chunk, 'metadata', {}).get('is_teacher_info', False))
    print(f"üë®‚Äçüè´ –ß–∞–Ω–∫–æ–≤ —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –ø—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª—è—Ö (is_teacher_info=True): {teacher_count}")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ content_type
    content_types = Counter()
    for chunk in chunks:
        content_type = getattr(chunk, 'metadata', {}).get('content_type', 'unknown')
        content_types[content_type] += 1
    
    print(f"\nüìã –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Ç–∏–ø–∞–º –∫–æ–Ω—Ç–µ–Ω—Ç–∞:")
    for content_type, count in content_types.most_common():
        print(f"  ‚Ä¢ {content_type}: {count} —á–∞–Ω–∫–æ–≤")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è–º –∫—É—Ä—Å–æ–≤
    all_courses = []
    for chunk in chunks:
        courses = getattr(chunk, 'metadata', {}).get('course_mentioned', [])
        all_courses.extend(courses)
    
    if all_courses:
        course_counts = Counter(all_courses)
        print(f"\nüéì –£–ø–æ–º–∏–Ω–∞–Ω–∏—è –∫—É—Ä—Å–æ–≤:")
        for course, count in course_counts.most_common():
            print(f"  ‚Ä¢ {course}: {count} —Ä–∞–∑")
    else:
        print(f"\nüéì –ö—É—Ä—Å—ã –Ω–µ —É–ø–æ–º–∏–Ω–∞—é—Ç—Å—è –≤ –¥–∞–Ω–Ω–æ–º —Ñ–∞–π–ª–µ")

def parse_arguments():
    """–ü–∞—Ä—Å–∏—Ç –∞—Ä–≥—É–º–µ–Ω—Ç—ã –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏."""
    parser = argparse.ArgumentParser(
        description="–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —É–ª—É—á—à–µ–Ω–∏–π –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö —Å CustomMetadataExtractor",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
  python test_new_metadata.py                           # –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Ñ–∞–π–ª –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: data_facts/faq.md
  python test_new_metadata.py --file data_facts/pricing.md
  python test_new_metadata.py --file data_facts/courses_detailed.md
  python test_new_metadata.py -f data_facts/methodology.md
        """
    )
    
    parser.add_argument(
        '--file', '-f',
        type=str,
        default=DEFAULT_TEST_FILE,
        help=f'–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: {DEFAULT_TEST_FILE})'
    )
    
    return parser.parse_args()

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è."""
    # –ü–∞—Ä—Å–∏–º –∞—Ä–≥—É–º–µ–Ω—Ç—ã –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏
    args = parse_arguments()
    test_file = args.file
    
    print("üß™ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –£–õ–£–ß–®–ï–ù–ò–ô –ú–ï–¢–ê–î–ê–ù–ù–´–•")
    print("=" * 60)
    print(f"üìÅ –¢–µ—Å—Ç–∏—Ä—É–µ–º—ã–π —Ñ–∞–π–ª: {test_file}")
    print("=" * 60)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ñ–∞–π–ª–∞
    if not os.path.exists(test_file):
        print(f"‚ùå –§–∞–π–ª '{test_file}' –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        print(f"\nüí° –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —Ñ–∞–π–ª —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ø–æ —É–∫–∞–∑–∞–Ω–Ω–æ–º—É –ø—É—Ç–∏.")
        print(f"üìã –î–æ—Å—Ç—É–ø–Ω—ã–µ —Ñ–∞–π–ª—ã –≤ data_facts/:")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ —Ñ–∞–π–ª—ã –≤ data_facts, –µ—Å–ª–∏ –ø–∞–ø–∫–∞ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        data_facts_dir = "data_facts"
        if os.path.exists(data_facts_dir):
            files = [f for f in os.listdir(data_facts_dir) if f.endswith('.md')]
            for file in sorted(files):
                print(f"  ‚Ä¢ {os.path.join(data_facts_dir, file)}")
        
        return
    
    try:
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –º–æ–¥–µ–ª–∏
        llm, embed_model = setup_models()
        
        # –°–æ–∑–¥–∞–µ–º pipeline
        pipeline = create_pipeline(llm, embed_model)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç
        logger.info(f"üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ '{test_file}'...")
        documents = SimpleDirectoryReader(
            input_files=[test_file]
        ).load_data()
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç
        logger.info("üß† –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞... –≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–∏–Ω—É—Ç.")
        chunks = pipeline.run(documents=documents, show_progress=True)
        
        logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤.")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 6 —á–∞–Ω–∫–æ–≤
        print(f"\nüîç –ê–ù–ê–õ–ò–ó –ü–ï–†–í–´–• 6 –ß–ê–ù–ö–û–í:")
        for i, chunk in enumerate(chunks[:6], 1):
            display_chunk_details(chunk, i)
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –æ–±—â—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        display_statistics(chunks)
        
        print(f"\nüéâ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!")
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
        print(f"\n‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {e}")

if __name__ == "__main__":
    main()